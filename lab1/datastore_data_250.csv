title,summary,authors,title_summary,keywords
The Meaning of Null in Databases and Programming Languages,"The meaning of null in relational databases is a major source of confusion
not only among database users but also among database textbook writers. The
purpose of this article is to examine what database nulls could mean and to
make some modest suggestions about how to reduce the confusion.",Kenneth Baclawski,"The Meaning of Null in Databases and Programming Languages. The meaning of null in relational databases is a major source of confusion
not only among database users but also among database textbook writers. The
purpose of this article is to examine what database nulls could mean and to
make some modest suggestions about how to reduce the confusion.","Null, Databases, Programming Languages"
"A collection of database industrial techniques and optimization
  approaches of database operations","Databases play an essential role in our society today. Databases are embedded
in sectors like corporations, institutions, and government organizations, among
others. These databases are used for our video and audio streaming platforms,
social gaming, finances, cloud storage, e-commerce, healthcare, economy, etc.
It is therefore imperative that we learn how to properly execute database
operations and efficiently implement methodologies so that we may optimize the
performance of databases.",Jasper Kyle Catapang,"A collection of database industrial techniques and optimization
  approaches of database operations. Databases play an essential role in our society today. Databases are embedded
in sectors like corporations, institutions, and government organizations, among
others. These databases are used for our video and audio streaming platforms,
social gaming, finances, cloud storage, e-commerce, healthcare, economy, etc.
It is therefore imperative that we learn how to properly execute database
operations and efficiently implement methodologies so that we may optimize the
performance of databases.","Database Industrial Techniques, Database Optimization"
Managing Variability in Relational Databases by VDBMS,"Variability inherently exists in databases in various contexts which creates
database variants. For example, variants of a database could have different
schemas/content (database evolution problem), variants of a database could root
from different sources (data integration problem), variants of a database could
be deployed differently for specific application domain (deploying a database
for different configurations of a software system), etc. Unfortunately, while
there are specific solutions to each of the problems arising in these contexts,
there is no general solution that accounts for variability in databases and
addresses managing variability within a database. In this paper, we formally
define variational databases (VDBs) and statically-typed variational relational
algebra (VRA) to query VDBs---both database and queries explicitly account for
variation. We also design and implement variational database management system
(VDBMS) to run variational queries over a VDB effectively and efficiently. To
assess this, we generate two VDBs from real-world databases in the context of
software development and database evolution with a set of experimental queries
for each.","Parisa Ataei, Qiaoran Li, Eric Walkingshaw, Arash Termehchy","Managing Variability in Relational Databases by VDBMS. Variability inherently exists in databases in various contexts which creates
database variants. For example, variants of a database could have different
schemas/content (database evolution problem), variants of a database could root
from different sources (data integration problem), variants of a database could
be deployed differently for specific application domain (deploying a database
for different configurations of a software system), etc. Unfortunately, while
there are specific solutions to each of the problems arising in these contexts,
there is no general solution that accounts for variability in databases and
addresses managing variability within a database. In this paper, we formally
define variational databases (VDBs) and statically-typed variational relational
algebra (VRA) to query VDBs---both database and queries explicitly account for
variation. We also design and implement variational database management system
(VDBMS) to run variational queries over a VDB effectively and efficiently. To
assess this, we generate two VDBs from real-world databases in the context of
software development and database evolution with a set of experimental queries
for each.","Variability, Relational Databases, VDBMS, VRA, Software Development, Database Evolution"
Firebird Database Backup by Serialized Database Table Dump,"This paper presents a simple data dump and load utility for Firebird
databases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load,
for dumping and loading respectively, retrieves each database table using
kinterbasdb and serializes the data using marshal module. This utility has two
advantages over the standard Firebird database backup utility, gbak. Firstly,
it is able to backup and restore single database tables which might help to
recover corrupted databases. Secondly, the output is in text-coded format (from
marshal module) making it more resilient than a compressed text backup, as in
the case of using gbak.",Maurice HT Ling,"Firebird Database Backup by Serialized Database Table Dump. This paper presents a simple data dump and load utility for Firebird
databases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load,
for dumping and loading respectively, retrieves each database table using
kinterbasdb and serializes the data using marshal module. This utility has two
advantages over the standard Firebird database backup utility, gbak. Firstly,
it is able to backup and restore single database tables which might help to
recover corrupted databases. Secondly, the output is in text-coded format (from
marshal module) making it more resilient than a compressed text backup, as in
the case of using gbak.","Firebird, Database Backup, Serialized Database Table Dump, Load Utility, Mysqldump"
Disjunctive Databases for Representing Repairs,"This paper addresses the problem of representing the set of repairs of a
possibly inconsistent database by means of a disjunctive database.
Specifically, the class of denial constraints is considered. We show that,
given a database and a set of denial constraints, there exists a (unique)
disjunctive database, called canonical, which represents the repairs of the
database w.r.t. the constraints and is contained in any other disjunctive
database with the same set of minimal models. We propose an algorithm for
computing the canonical disjunctive database. Finally, we study the size of the
canonical disjunctive database in the presence of functional dependencies for
both repairs and cardinality-based repairs.","Cristian Molinaro, Jan Chomicki, Jerzy Marcinkowski","Disjunctive Databases for Representing Repairs. This paper addresses the problem of representing the set of repairs of a
possibly inconsistent database by means of a disjunctive database.
Specifically, the class of denial constraints is considered. We show that,
given a database and a set of denial constraints, there exists a (unique)
disjunctive database, called canonical, which represents the repairs of the
database w.r.t. the constraints and is contained in any other disjunctive
database with the same set of minimal models. We propose an algorithm for
computing the canonical disjunctive database. Finally, we study the size of the
canonical disjunctive database in the presence of functional dependencies for
both repairs and cardinality-based repairs.","Disjunctive Databases, Denial Constraints, Functional Dependencies"
Database Aggregation,"Knowledge can be represented compactly in a multitude ways, from a set of
propositional formulas, to a Kripke model, to a database. In this paper we
study the aggregation of information coming from multiple sources, each source
submitting a database modelled as a first-order relational structure. In the
presence of an integrity constraint, we identify classes of aggregators that
respect it in the aggregated database, provided all individual databases
satisfy it. We also characterise languages for first-order queries on which the
answer to queries on the aggregated database coincides with the aggregation of
the answers to the query obtained on each individual database. This
contribution is meant to be a first step on the application of techniques from
rational choice theory to knowledge representation in databases.","Francesco Belardinelli, Umberto Grandi","Database Aggregation. Knowledge can be represented compactly in a multitude ways, from a set of
propositional formulas, to a Kripke model, to a database. In this paper we
study the aggregation of information coming from multiple sources, each source
submitting a database modelled as a first-order relational structure. In the
presence of an integrity constraint, we identify classes of aggregators that
respect it in the aggregated database, provided all individual databases
satisfy it. We also characterise languages for first-order queries on which the
answer to queries on the aggregated database coincides with the aggregation of
the answers to the query obtained on each individual database. This
contribution is meant to be a first step on the application of techniques from
rational choice theory to knowledge representation in databases.","Database Aggregation, Rational Choice Theory, Kripke Model, Integrity Constraint"
"Context-based Ontology Modelling for Database: Enabling ChatGPT for
  Semantic Database Management","This research paper explores the use of ChatGPT in database management.
ChatGPT, an AI-powered chatbot, has limitations in performing tasks related to
database management due to the lack of standardized vocabulary and grammar for
representing database semantics. To address this limitation, the paper proposes
a solution that involves developing a set of syntaxes that can represent
database semantics in natural language. The syntax is used to convert database
schemas into natural language formats, providing a new application of ChatGPT
in database management. The proposed solution is demonstrated through a case
study where ChatGPT is used to perform two tasks, semantic integration, and
tables joining. Results demonstrate that the use of semantic database
representations produces more precise outcomes and avoids common mistakes
compared to cases with no semantic representation. The proposed method has the
potential to speed up the database management process, reduce the level of
understanding required for database domain knowledge, and enable automatic
database operations without accessing the actual data, thus illuminating
privacy protection concerns when using AI. This paper provides a promising new
direction for research in the field of AI-based database management.","Wenjun Lin, Paul Babyn, Yan yan, Wenjun Zhang","Context-based Ontology Modelling for Database: Enabling ChatGPT for
  Semantic Database Management. This research paper explores the use of ChatGPT in database management.
ChatGPT, an AI-powered chatbot, has limitations in performing tasks related to
database management due to the lack of standardized vocabulary and grammar for
representing database semantics. To address this limitation, the paper proposes
a solution that involves developing a set of syntaxes that can represent
database semantics in natural language. The syntax is used to convert database
schemas into natural language formats, providing a new application of ChatGPT
in database management. The proposed solution is demonstrated through a case
study where ChatGPT is used to perform two tasks, semantic integration, and
tables joining. Results demonstrate that the use of semantic database
representations produces more precise outcomes and avoids common mistakes
compared to cases with no semantic representation. The proposed method has the
potential to speed up the database management process, reduce the level of
understanding required for database domain knowledge, and enable automatic
database operations without accessing the actual data, thus illuminating
privacy protection concerns when using AI. This paper provides a promising new
direction for research in the field of AI-based database management.","ChatGPT, AI, Database Management, Semantic Database, Natural Language Processing, NLP"
Mragyati : A System for Keyword-based Searching in Databases,"The web, through many search engine sites, has popularized the keyword-based
search paradigm, where a user can specify a string of keywords and expect to
retrieve relevant documents, possibly ranked by their relevance to the query.
Since a lot of information is stored in databases (and not as HTML documents),
it is important to provide a similar search paradigm for databases, where users
can query a database without knowing the database schema and database query
languages such as SQL. In this paper, we propose such a database search system,
which accepts a free-form query as a collection of keywords, translates it into
queries on the database using the database metadata, and presents query results
in a well-structured and browsable form. Th eysytem maps keywords onto the
database schema and uses inter-relationships (i.e., data semantics) among the
referred tables to generate meaningful query results. We also describe our
prototype for database search, called Mragyati. Th eapproach proposed here is
scalable, as it does not build an in-memory graph of the entire database for
searching for relationships among the objects selected by the user's query.","N. L. Sarda, Ankur Jain","Mragyati : A System for Keyword-based Searching in Databases. The web, through many search engine sites, has popularized the keyword-based
search paradigm, where a user can specify a string of keywords and expect to
retrieve relevant documents, possibly ranked by their relevance to the query.
Since a lot of information is stored in databases (and not as HTML documents),
it is important to provide a similar search paradigm for databases, where users
can query a database without knowing the database schema and database query
languages such as SQL. In this paper, we propose such a database search system,
which accepts a free-form query as a collection of keywords, translates it into
queries on the database using the database metadata, and presents query results
in a well-structured and browsable form. Th eysytem maps keywords onto the
database schema and uses inter-relationships (i.e., data semantics) among the
referred tables to generate meaningful query results. We also describe our
prototype for database search, called Mragyati. Th eapproach proposed here is
scalable, as it does not build an in-memory graph of the entire database for
searching for relationships among the objects selected by the user's query.","Mragyati, Keyword-based Searching, Databases, SQL"
Database Semantics,"This paper, the first step to connect relational databases with systems
consequence (Kent: ""System Consequence"" 2009), is concerned with the semantics
of relational databases. It aims to to study system consequence in the
logical/semantic system of relational databases. The paper, which was inspired
by and which extends a recent set of papers on the theory of relational
database systems (Spivak: ""Functorial Data Migration"" 2012), is linked with
work on the Information Flow Framework (IFF) [http://suo.ieee.org/IFF/]
connected with the ontology standards effort (SUO), since relational databases
naturally embed into first order logic. The database semantics discussed here
is concerned with the conceptual level of database architecture. We offer both
an intuitive and technical discussion. Corresponding to the notions of primary
and foreign keys, relational database semantics takes two forms: a
distinguished form where entities are distinguished from relations, and a
unified form where relations and entities coincide. The distinguished form
corresponds to the theory presented in (Spivak: ""Simplicial databases""
2009)[arXiv:0904.2012]. The unified form, a special case of the distinguished
form, corresponds to the theory presented in (Spivak: ""Functorial Data
Migration"" 2012). A later paper will discuss various formalisms of relational
databases, such as relational algebra and first order logic, and will complete
the description of the relational database logical environment.",Robert E. Kent,"Database Semantics. This paper, the first step to connect relational databases with systems
consequence (Kent: ""System Consequence"" 2009), is concerned with the semantics
of relational databases. It aims to to study system consequence in the
logical/semantic system of relational databases. The paper, which was inspired
by and which extends a recent set of papers on the theory of relational
database systems (Spivak: ""Functorial Data Migration"" 2012), is linked with
work on the Information Flow Framework (IFF) [http://suo.ieee.org/IFF/]
connected with the ontology standards effort (SUO), since relational databases
naturally embed into first order logic. The database semantics discussed here
is concerned with the conceptual level of database architecture. We offer both
an intuitive and technical discussion. Corresponding to the notions of primary
and foreign keys, relational database semantics takes two forms: a
distinguished form where entities are distinguished from relations, and a
unified form where relations and entities coincide. The distinguished form
corresponds to the theory presented in (Spivak: ""Simplicial databases""
2009)[arXiv:0904.2012]. The unified form, a special case of the distinguished
form, corresponds to the theory presented in (Spivak: ""Functorial Data
Migration"" 2012). A later paper will discuss various formalisms of relational
databases, such as relational algebra and first order logic, and will complete
the description of the relational database logical environment.","Database Semantics, System Consequence, Information Flow Framework, IFF, SUO, Relational Algebra, Foreign Keys"
EMIT: Micro-Invasive Database Configuration Tuning,"The process of database knob tuning has always been a challenging task.
Recently, database knob tuning methods has emerged as a promising solution to
mitigate these issues. However, these methods still face certain limitations.On
one hand, when applying knob tuning algorithms to optimize databases in
practice, it either requires frequent updates to the database or necessitates
acquiring database workload and optimizing through workload replay. The former
approach involves constant exploration and updating of database configurations,
inevitably leading to a decline in database performance during optimization.
The latter, on the other hand, requires the acquisition of workload data, which
could lead to data leakage issues. Moreover, the hyperparameter configuration
space for database knobs is vast, making it challenging for optimizers to
converge. These factors significantly hinder the practical implementation of
database tuning. To address these concerns, we proposes an efficient and
micro-invasive knob tuning method. This method relies on workload synthesis on
cloned databases to simulate the workload that needs tuning, thus minimizing
the intrusion on the database. And we utilizing a configuration replacement
strategy to filter configuration candidates that perform well under the
synthesized workload to find best configuration. And during the tuning process,
we employ a knowledge transfer method to extract a common high-performance
space, to boost the convergence of the optimizer.","Jian Geng, Hongzhi Wang, Yu Yan","EMIT: Micro-Invasive Database Configuration Tuning. The process of database knob tuning has always been a challenging task.
Recently, database knob tuning methods has emerged as a promising solution to
mitigate these issues. However, these methods still face certain limitations.On
one hand, when applying knob tuning algorithms to optimize databases in
practice, it either requires frequent updates to the database or necessitates
acquiring database workload and optimizing through workload replay. The former
approach involves constant exploration and updating of database configurations,
inevitably leading to a decline in database performance during optimization.
The latter, on the other hand, requires the acquisition of workload data, which
could lead to data leakage issues. Moreover, the hyperparameter configuration
space for database knobs is vast, making it challenging for optimizers to
converge. These factors significantly hinder the practical implementation of
database tuning. To address these concerns, we proposes an efficient and
micro-invasive knob tuning method. This method relies on workload synthesis on
cloned databases to simulate the workload that needs tuning, thus minimizing
the intrusion on the database. And we utilizing a configuration replacement
strategy to filter configuration candidates that perform well under the
synthesized workload to find best configuration. And during the tuning process,
we employ a knowledge transfer method to extract a common high-performance
space, to boost the convergence of the optimizer.","EMIT, Database Configuration Tuning, Database, Workload Synthesis, Configuration Replacement"
Table manipulation in simplicial databases,"In \cite{Spi}, we developed a category of databases in which the schema of a
database is represented as a simplicial set. Each simplex corresponds to a
table in the database. There, our main concern was to find a categorical
formulation of databases; the simplicial nature of the schemas was to some
degree unexpected and unexploited.
  In the present note, we show how to use this geometric formulation
effectively on a computer. If we think of each simplex as a polygonal tile, we
can imagine assembling custom databases by mixing and matching tiles. Queries
on this database can be performed by drawing paths through the resulting tile
formations, selecting records at the start-point of this path and retrieving
corresponding records at its end-point.",David I. Spivak,"Table manipulation in simplicial databases. In \cite{Spi}, we developed a category of databases in which the schema of a
database is represented as a simplicial set. Each simplex corresponds to a
table in the database. There, our main concern was to find a categorical
formulation of databases; the simplicial nature of the schemas was to some
degree unexpected and unexploited.
  In the present note, we show how to use this geometric formulation
effectively on a computer. If we think of each simplex as a polygonal tile, we
can imagine assembling custom databases by mixing and matching tiles. Queries
on this database can be performed by drawing paths through the resulting tile
formations, selecting records at the start-point of this path and retrieving
corresponding records at its end-point.","Table Manipulation, Database, Spi"
"Discovering new technique for mapping relational database based on
  semantic web technology","Most of data on the Web are still stored in relational databases. Therefore,
it is more important to make the correspondence between relational databases
(RDB) and ontologies for storing the Web data. In this paper, we present an new
approach to map the data stored in relational databases into the Semantic Web,
we exploit simple mappings based on some specifications of the database schema,
and we explain how relational databases can be used to define a mapping
mechanism between relational database and OWL ontology. A framework has been
developed, which migrates successfully RDB into OWL document. The experimental
results were very important, demonstrating that the proposed method is feasible
and efficient.","Noreddine Gherabi, Khaoula Addakiri, Mohamed Bahaj","Discovering new technique for mapping relational database based on
  semantic web technology. Most of data on the Web are still stored in relational databases. Therefore,
it is more important to make the correspondence between relational databases
(RDB) and ontologies for storing the Web data. In this paper, we present an new
approach to map the data stored in relational databases into the Semantic Web,
we exploit simple mappings based on some specifications of the database schema,
and we explain how relational databases can be used to define a mapping
mechanism between relational database and OWL ontology. A framework has been
developed, which migrates successfully RDB into OWL document. The experimental
results were very important, demonstrating that the proposed method is feasible
and efficient.","Relational Database, OWL, Semantic Web"
GRAD: On Graph Database Modeling,"Graph databases have emerged as the fundamental technology underpinning
trendy application domains where traditional databases are not well-equipped to
handle complex graph data. However, current graph databases support basic graph
structures and integrity constraints with no standard algebra. In this paper,
we introduce GRAD, a native and generic graph database model. GRAD goes beyond
traditional graph database models, which support simple graph structures and
constraints. Instead, GRAD presents a complete graph database model supporting
advanced graph structures, a set of well-defined constraints over these
structures and a powerful graph analysis-oriented algebra.","Amine Ghrab, Oscar Romero, Sabri Skhiri, Alejandro Vaisman, Esteban Zimányi","GRAD: On Graph Database Modeling. Graph databases have emerged as the fundamental technology underpinning
trendy application domains where traditional databases are not well-equipped to
handle complex graph data. However, current graph databases support basic graph
structures and integrity constraints with no standard algebra. In this paper,
we introduce GRAD, a native and generic graph database model. GRAD goes beyond
traditional graph database models, which support simple graph structures and
constraints. Instead, GRAD presents a complete graph database model supporting
advanced graph structures, a set of well-defined constraints over these
structures and a powerful graph analysis-oriented algebra.","GRAD, Graph Database Modeling, Graph Structures, Integrity Constraints"
"A Systematic Method for On-The-Fly Denormalization of Relational
  Databases","Normalized relational databases are a common method for storing data, but
pulling out usable denormalized data for consumption generally requires either
direct access to the source data or creation of an appropriate view or table by
a database administrator. End-users are thus limited in their ability to
explore and use data that is stored in this manner. Presented here is a method
for performing automated denormalization of relational databases at run-time,
without requiring access to source data or ongoing intervention by a database
administrator. Furthermore, this method does not require a restructure of the
database itself and so it can be flexibly applied as a layer on top of already
existing databases.",Sareen Shah,"A Systematic Method for On-The-Fly Denormalization of Relational
  Databases. Normalized relational databases are a common method for storing data, but
pulling out usable denormalized data for consumption generally requires either
direct access to the source data or creation of an appropriate view or table by
a database administrator. End-users are thus limited in their ability to
explore and use data that is stored in this manner. Presented here is a method
for performing automated denormalization of relational databases at run-time,
without requiring access to source data or ongoing intervention by a database
administrator. Furthermore, this method does not require a restructure of the
database itself and so it can be flexibly applied as a layer on top of already
existing databases.","Denormalization, Relational Databases, Automation"
An Overview on Cloud Distributed Databases for Business Environments,"Cloud-based distributed databases are a popular choice for many current
applications, especially those that run over the Internet. By incorporating
distributed database systems within cloud environments, it has enabled
businesses to scale operations to a global level, all while achieving desired
standards of system reliability, availability, and responsiveness. Cloud
providers offer infrastructure and management tools for distributed databases
as Database-as-a-Service (DBaaS), re-purposing the investment by businesses
towards database services. This paper reviews the functionality of these
services, by highlighting Amazon Relational Data Service (RDS), suited for
handling relational distributed databases.","Allan Vikiru, Mfadhili Muiruri, Ismail Ateya","An Overview on Cloud Distributed Databases for Business Environments. Cloud-based distributed databases are a popular choice for many current
applications, especially those that run over the Internet. By incorporating
distributed database systems within cloud environments, it has enabled
businesses to scale operations to a global level, all while achieving desired
standards of system reliability, availability, and responsiveness. Cloud
providers offer infrastructure and management tools for distributed databases
as Database-as-a-Service (DBaaS), re-purposing the investment by businesses
towards database services. This paper reviews the functionality of these
services, by highlighting Amazon Relational Data Service (RDS), suited for
handling relational distributed databases.","Cloud Distributed Databases, Amazon, Relational Data Service, DBaaS"
Trustworthy and Efficient LLMs Meet Databases,"In the rapidly evolving AI era with large language models (LLMs) at the core,
making LLMs more trustworthy and efficient, especially in output generation
(inference), has gained significant attention. This is to reduce plausible but
faulty LLM outputs (a.k.a hallucinations) and meet the highly increased
inference demands. This tutorial explores such efforts and makes them
transparent to the database community. Understanding these efforts is essential
in harnessing LLMs in database tasks and adapting database techniques to LLMs.
Furthermore, we delve into the synergy between LLMs and databases, highlighting
new opportunities and challenges in their intersection. This tutorial aims to
share with database researchers and practitioners essential concepts and
strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining
in the intersection between LLMs and databases.","Kyoungmin Kim, Anastasia Ailamaki","Trustworthy and Efficient LLMs Meet Databases. In the rapidly evolving AI era with large language models (LLMs) at the core,
making LLMs more trustworthy and efficient, especially in output generation
(inference), has gained significant attention. This is to reduce plausible but
faulty LLM outputs (a.k.a hallucinations) and meet the highly increased
inference demands. This tutorial explores such efforts and makes them
transparent to the database community. Understanding these efforts is essential
in harnessing LLMs in database tasks and adapting database techniques to LLMs.
Furthermore, we delve into the synergy between LLMs and databases, highlighting
new opportunities and challenges in their intersection. This tutorial aims to
share with database researchers and practitioners essential concepts and
strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining
in the intersection between LLMs and databases.","LLMs, Databases, Large Language Models, AI"
The Causality/Repair Connection in Databases: Causality-Programs,"In this work, answer-set programs that specify repairs of databases are used
as a basis for solving computational and reasoning problems about causes for
query answers from databases.",Leopoldo Bertossi,"The Causality/Repair Connection in Databases: Causality-Programs. In this work, answer-set programs that specify repairs of databases are used
as a basis for solving computational and reasoning problems about causes for
query answers from databases.","Causality, Repair, Databases"
"Incremental View Maintenance for Deductive Graph Databases Using
  Generalized Discrimination Networks","Nowadays, graph databases are employed when relationships between entities
are in the scope of database queries to avoid performance-critical join
operations of relational databases. Graph queries are used to query and modify
graphs stored in graph databases. Graph queries employ graph pattern matching
that is NP-complete for subgraph isomorphism. Graph database views can be
employed that keep ready answers in terms of precalculated graph pattern
matches for often stated and complex graph queries to increase query
performance. However, such graph database views must be kept consistent with
the graphs stored in the graph database.
  In this paper, we describe how to use incremental graph pattern matching as
technique for maintaining graph database views. We present an incremental
maintenance algorithm for graph database views, which works for imperatively
and declaratively specified graph queries. The evaluation shows that our
maintenance algorithm scales when the number of nodes and edges stored in the
graph database increases. Furthermore, our evaluation shows that our approach
can outperform existing approaches for the incremental maintenance of graph
query results.","Thomas Beyhl, Holger Giese","Incremental View Maintenance for Deductive Graph Databases Using
  Generalized Discrimination Networks. Nowadays, graph databases are employed when relationships between entities
are in the scope of database queries to avoid performance-critical join
operations of relational databases. Graph queries are used to query and modify
graphs stored in graph databases. Graph queries employ graph pattern matching
that is NP-complete for subgraph isomorphism. Graph database views can be
employed that keep ready answers in terms of precalculated graph pattern
matches for often stated and complex graph queries to increase query
performance. However, such graph database views must be kept consistent with
the graphs stored in the graph database.
  In this paper, we describe how to use incremental graph pattern matching as
technique for maintaining graph database views. We present an incremental
maintenance algorithm for graph database views, which works for imperatively
and declaratively specified graph queries. The evaluation shows that our
maintenance algorithm scales when the number of nodes and edges stored in the
graph database increases. Furthermore, our evaluation shows that our approach
can outperform existing approaches for the incremental maintenance of graph
query results.","Incremental View Maintenance, Deductive Graph Databases, Generalized Discrimination Networks, Graph Pattern Matching"
"Belnap-Dunn logic and query answering in inconsistent databases with
  null values","This paper concerns an expansion of first-order Belnap-Dunn logic, named
$\mathrm{BD}^{\supset,\mathsf{F}}$, and an application of this logic in the
area of relational database theory. The notion of a relational database, the
notion of a query applicable to a relational database, and several notions of
an answer to a query with respect to a relational database are considered from
the perspective of this logic, taking into account that a database may be an
inconsistent database or a database with null values. The chosen perspective
enables among other things the definition of a notion of a consistent answer to
a query with respect to a possibly inconsistent database without resort to
database repairs. For each of the notions of an answer considered, being an
answer to a query with respect to a database of the kind considered is
decidable.",C. A. Middelburg,"Belnap-Dunn logic and query answering in inconsistent databases with
  null values. This paper concerns an expansion of first-order Belnap-Dunn logic, named
$\mathrm{BD}^{\supset,\mathsf{F}}$, and an application of this logic in the
area of relational database theory. The notion of a relational database, the
notion of a query applicable to a relational database, and several notions of
an answer to a query with respect to a relational database are considered from
the perspective of this logic, taking into account that a database may be an
inconsistent database or a database with null values. The chosen perspective
enables among other things the definition of a notion of a consistent answer to
a query with respect to a possibly inconsistent database without resort to
database repairs. For each of the notions of an answer considered, being an
answer to a query with respect to a database of the kind considered is
decidable.","Belnap-Dunn Logic, Relational Database, Null Values"
Database Dependencies and Formal Concept Analysis,"This is an account of the characterization of database dependencies with
Formal Concept Analysis.",Jaume Baixeries,"Database Dependencies and Formal Concept Analysis. This is an account of the characterization of database dependencies with
Formal Concept Analysis.","Database Dependencies, Formal Concept Analysis"
"Extensible Database Simulator for Fast Prototyping In-Database
  Algorithms","With the rapid increasing of data scale, in-database analytics and learning
has become one of the most studied topics in data science community, because of
its significance on reducing the gap between the management and the analytics
of data. By extending the capability of database on analytics and learning,
data scientists can save much time on exchanging data between databases and
external analytic tools. For this goal, researchers are attempting to integrate
more data science algorithms into database. However, implementing the
algorithms in mainstream databases is super time-consuming, especially when it
is necessary to have a deep dive into the database kernels. Thus there are
demands for an easy-to-extend database simulator to help fast prototype and
verify the in-database algorithms before implementing them in real databases.
In this demo, we present such an extensible relational database simulator,
DBSim, to help data scientists prototype their in-database analytics and
learning algorithms and verify the effectiveness of their ideas with minimal
cost. DBSim simulates a real relational database by integrating all the major
components in mainstream RDBMS, including SQL parser, relational operators,
query optimizer, etc. In addition, DBSim provides various interfaces for users
to flexibly plug their custom extension modules into any of the major
components, without modifying the kernel. By those interfaces, DBSim supports
easy extensions on SQL syntax, relational operators, query optimizer rules and
cost models, and physical plan execution. Furthermore, DBSim provides utilities
to facilitate users' developing and debugging, like query plan visualizer and
interactive analyzer on optimization rules. We develop DBSim using pure Python
to support seamless implementation of most data science algorithms into it,
since many of them are written in Python.","Yifan Wang, Daisy Zhe Wang","Extensible Database Simulator for Fast Prototyping In-Database
  Algorithms. With the rapid increasing of data scale, in-database analytics and learning
has become one of the most studied topics in data science community, because of
its significance on reducing the gap between the management and the analytics
of data. By extending the capability of database on analytics and learning,
data scientists can save much time on exchanging data between databases and
external analytic tools. For this goal, researchers are attempting to integrate
more data science algorithms into database. However, implementing the
algorithms in mainstream databases is super time-consuming, especially when it
is necessary to have a deep dive into the database kernels. Thus there are
demands for an easy-to-extend database simulator to help fast prototype and
verify the in-database algorithms before implementing them in real databases.
In this demo, we present such an extensible relational database simulator,
DBSim, to help data scientists prototype their in-database analytics and
learning algorithms and verify the effectiveness of their ideas with minimal
cost. DBSim simulates a real relational database by integrating all the major
components in mainstream RDBMS, including SQL parser, relational operators,
query optimizer, etc. In addition, DBSim provides various interfaces for users
to flexibly plug their custom extension modules into any of the major
components, without modifying the kernel. By those interfaces, DBSim supports
easy extensions on SQL syntax, relational operators, query optimizer rules and
cost models, and physical plan execution. Furthermore, DBSim provides utilities
to facilitate users' developing and debugging, like query plan visualizer and
interactive analyzer on optimization rules. We develop DBSim using pure Python
to support seamless implementation of most data science algorithms into it,
since many of them are written in Python.","DBSim, Database Simulator, In-Database Analytics, Learning Algorithms, Python"
CASIA-Iris-Africa: A Large-scale African Iris Image Database,"Iris biometrics is a phenotypic biometric trait that has proven to be
agnostic to human natural physiological changes. Research on iris biometrics
has progressed tremendously, partly due to publicly available iris databases.
Various databases have been available to researchers that address pressing iris
biometric challenges such as constraint, mobile, multispectral, synthetics,
long-distance, contact lenses, liveness detection, etc. However, these
databases mostly contain subjects of Caucasian and Asian docents with very few
Africans. Despite many investigative studies on racial bias in face biometrics,
very few studies on iris biometrics have been published, mainly due to the lack
of racially diverse large-scale databases containing sufficient iris samples of
Africans in the public domain. Furthermore, most of these databases contain a
relatively small number of subjects and labelled images. This paper proposes a
large-scale African database named CASIA-Iris-Africa that can be used as a
complementary database for the iris recognition community to mediate the effect
of racial biases on Africans. The database contains 28,717 images of 1023
African subjects (2046 iris classes) with age, gender, and ethnicity attributes
that can be useful in demographically sensitive studies of Africans. Sets of
specific application protocols are incorporated with the database to ensure the
database's variability and scalability. Performance results of some open-source
SOTA algorithms on the database are presented, which will serve as baseline
performances. The relatively poor performances of the baseline algorithms on
the proposed database despite better performance on other databases prove that
racial biases exist in these iris recognition algorithms. The database will be
made available on our website: http://www.idealtest.org.","Jawad Muhammad, Yunlong Wang, Junxing Hu, Kunbo Zhang, Zhenan Sun","CASIA-Iris-Africa: A Large-scale African Iris Image Database. Iris biometrics is a phenotypic biometric trait that has proven to be
agnostic to human natural physiological changes. Research on iris biometrics
has progressed tremendously, partly due to publicly available iris databases.
Various databases have been available to researchers that address pressing iris
biometric challenges such as constraint, mobile, multispectral, synthetics,
long-distance, contact lenses, liveness detection, etc. However, these
databases mostly contain subjects of Caucasian and Asian docents with very few
Africans. Despite many investigative studies on racial bias in face biometrics,
very few studies on iris biometrics have been published, mainly due to the lack
of racially diverse large-scale databases containing sufficient iris samples of
Africans in the public domain. Furthermore, most of these databases contain a
relatively small number of subjects and labelled images. This paper proposes a
large-scale African database named CASIA-Iris-Africa that can be used as a
complementary database for the iris recognition community to mediate the effect
of racial biases on Africans. The database contains 28,717 images of 1023
African subjects (2046 iris classes) with age, gender, and ethnicity attributes
that can be useful in demographically sensitive studies of Africans. Sets of
specific application protocols are incorporated with the database to ensure the
database's variability and scalability. Performance results of some open-source
SOTA algorithms on the database are presented, which will serve as baseline
performances. The relatively poor performances of the baseline algorithms on
the proposed database despite better performance on other databases prove that
racial biases exist in these iris recognition algorithms. The database will be
made available on our website: http://www.idealtest.org.","CASIA-Iris-Africa, African Iris Image Database, Large-scale Database, Athealtest, Open Source"
"SPSW: Database Watermarking Based on Fake Tuples and Sparse Priority
  Strategy","Databases play a crucial role in storing and managing vast amounts of data in
various organizations and industries. Yet the risk of database leakage poses a
significant threat to data privacy and security. To trace the source of
database leakage, researchers have proposed many database watermarking schemes.
Among them, fake-tuples-based database watermarking shows great potential as it
does not modify the original data of the database, ensuring the seamless
usability of the watermarked database. However, the existing fake-tuple-based
database watermarking schemes need to insert a large number of fake tuples for
the embedding of each watermark bit, resulting in low watermark transparency.
Therefore, we propose a novel database watermarking scheme based on fake tuples
and sparse priority strategy, named SPSW, which achieves the same watermark
capacity with a lower number of inserted fake tuples compared to the existing
embedding strategy. Specifically, for a database about to be watermarked, we
prioritize embedding the sparsest watermark sequence, i.e., the sequence
containing the most `0' bits among the currently available watermark sequences.
For each bit in the sparse watermark sequence, when it is set to `1', SPSW will
embed the corresponding set of fake tuples into the database. Otherwise, no
modifications will be made to the database. Through theoretical analysis, the
proposed sparse priority strategy not only improves transparency but also
enhances the robustness of the watermark. The comparative experimental results
with other database watermarking schemes further validate the superior
performance of the proposed SPSW, aligning with the theoretical analysis.","Zhiwen Ren, Zehua Ma, Weiming Zhang, Nenghai Yu","SPSW: Database Watermarking Based on Fake Tuples and Sparse Priority
  Strategy. Databases play a crucial role in storing and managing vast amounts of data in
various organizations and industries. Yet the risk of database leakage poses a
significant threat to data privacy and security. To trace the source of
database leakage, researchers have proposed many database watermarking schemes.
Among them, fake-tuples-based database watermarking shows great potential as it
does not modify the original data of the database, ensuring the seamless
usability of the watermarked database. However, the existing fake-tuple-based
database watermarking schemes need to insert a large number of fake tuples for
the embedding of each watermark bit, resulting in low watermark transparency.
Therefore, we propose a novel database watermarking scheme based on fake tuples
and sparse priority strategy, named SPSW, which achieves the same watermark
capacity with a lower number of inserted fake tuples compared to the existing
embedding strategy. Specifically, for a database about to be watermarked, we
prioritize embedding the sparsest watermark sequence, i.e., the sequence
containing the most `0' bits among the currently available watermark sequences.
For each bit in the sparse watermark sequence, when it is set to `1', SPSW will
embed the corresponding set of fake tuples into the database. Otherwise, no
modifications will be made to the database. Through theoretical analysis, the
proposed sparse priority strategy not only improves transparency but also
enhances the robustness of the watermark. The comparative experimental results
with other database watermarking schemes further validate the superior
performance of the proposed SPSW, aligning with the theoretical analysis.","SPSW, Database Watermarking, Fake Tuples, Sparse Priority"
Scripting Relational Database Engine Using Transducer,"We allow database user to script a parallel relational database engine with a
procedural language. Procedural language code is executed as a user defined
relational query operator called transducer. Transducer is tightly integrated
with relation engine, including query optimizer, query executor and can be
executed in parallel like other query operators. With transducer, we can
efficiently execute queries that are very difficult to express in SQL. As
example, we show how to run time series and graph queries, etc, within a
parallel relational database.",Feng Tian,"Scripting Relational Database Engine Using Transducer. We allow database user to script a parallel relational database engine with a
procedural language. Procedural language code is executed as a user defined
relational query operator called transducer. Transducer is tightly integrated
with relation engine, including query optimizer, query executor and can be
executed in parallel like other query operators. With transducer, we can
efficiently execute queries that are very difficult to express in SQL. As
example, we show how to run time series and graph queries, etc, within a
parallel relational database.","Transducer, Relational Database Engine, Procedural Language, Query Optimizer, Query Executor"
"Materials databases: the need for open, interoperable databases with
  standardized data and rich metadata","Driven by the recent rapid increase in the number of materials databases
published (open and commercial), I discuss here some perspectives on the
growing need for standardized, interoperable, open databases. The field of
computational materials discovery is quickly expanding, and recent advances in
data mining, high throughput screening, and machine learning highlight the
potential of open databases.",François-Xavier Coudert,"Materials databases: the need for open, interoperable databases with
  standardized data and rich metadata. Driven by the recent rapid increase in the number of materials databases
published (open and commercial), I discuss here some perspectives on the
growing need for standardized, interoperable, open databases. The field of
computational materials discovery is quickly expanding, and recent advances in
data mining, high throughput screening, and machine learning highlight the
potential of open databases.","Materials Databases, Open, Interoperable, Metadata"
Database Theory + X: Database Visualization,"We draw a connection between data modeling and visualization, namely that a
visualization specification defines a mapping from database constraints to
visual representations of those constraints. Using this formalism, we show how
many visualization design decisions are, in fact, data modeling choices and
extend data visualization from single-dataset visualizations to database
visualization",Eugene Wu,"Database Theory + X: Database Visualization. We draw a connection between data modeling and visualization, namely that a
visualization specification defines a mapping from database constraints to
visual representations of those constraints. Using this formalism, we show how
many visualization design decisions are, in fact, data modeling choices and
extend data visualization from single-dataset visualizations to database
visualization","Database Theory, X, Database Visualization"
Database Security: A Historical Perspective,"The importance of security in database research has greatly increased over
the years as most of critical functionality of the business and military
enterprises became digitized. Database is an integral part of any information
system and they often hold sensitive data. The security of the data depends on
physical security, OS security and DBMS security. Database security can be
compromised by obtaining sensitive data, changing data or degrading
availability of the database. Over the last 30 years the information technology
environment have gone through many changes of evolution and the database
research community have tried to stay a step ahead of the upcoming threats to
the database security. The database research community has thoughts about these
issues long before they were address by the implementations. This paper will
examine the different topics pertaining to database security and see the
adaption of the research to the changing environment. Some short term database
research trends will be ascertained at the conclusion.",Paul Lesov,"Database Security: A Historical Perspective. The importance of security in database research has greatly increased over
the years as most of critical functionality of the business and military
enterprises became digitized. Database is an integral part of any information
system and they often hold sensitive data. The security of the data depends on
physical security, OS security and DBMS security. Database security can be
compromised by obtaining sensitive data, changing data or degrading
availability of the database. Over the last 30 years the information technology
environment have gone through many changes of evolution and the database
research community have tried to stay a step ahead of the upcoming threats to
the database security. The database research community has thoughts about these
issues long before they were address by the implementations. This paper will
examine the different topics pertaining to database security and see the
adaption of the research to the changing environment. Some short term database
research trends will be ascertained at the conclusion.","Database Security, Database, Information Technology, OS Security, DBMS"
Scalable Continual Top-k Keyword Search in Relational Databases,"Keyword search in relational databases has been widely studied in recent
years because it does not require users neither to master a certain structured
query language nor to know the complex underlying database schemas. Most of
existing methods focus on answering snapshot keyword queries in static
databases. In practice, however, databases are updated frequently, and users
may have long-term interests on specific topics. To deal with such a situation,
it is necessary to build effective and efficient facility in a database system
to support continual keyword queries.
  In this paper, we propose an efficient method for answering continual top-$k$
keyword queries over relational databases. The proposed method is built on an
existing scheme of keyword search on relational data streams, but incorporates
the ranking mechanisms into the query processing methods and makes two
improvements to support efficient top-$k$ keyword search in relational
databases. Compared to the existing methods, our method is more efficient both
in computing the top-$k$ results in a static database and in maintaining the
top-$k$ results when the database continually being updated. Experimental
results validate the effectiveness and efficiency of the proposed method.",Yanwei XU,"Scalable Continual Top-k Keyword Search in Relational Databases. Keyword search in relational databases has been widely studied in recent
years because it does not require users neither to master a certain structured
query language nor to know the complex underlying database schemas. Most of
existing methods focus on answering snapshot keyword queries in static
databases. In practice, however, databases are updated frequently, and users
may have long-term interests on specific topics. To deal with such a situation,
it is necessary to build effective and efficient facility in a database system
to support continual keyword queries.
  In this paper, we propose an efficient method for answering continual top-$k$
keyword queries over relational databases. The proposed method is built on an
existing scheme of keyword search on relational data streams, but incorporates
the ranking mechanisms into the query processing methods and makes two
improvements to support efficient top-$k$ keyword search in relational
databases. Compared to the existing methods, our method is more efficient both
in computing the top-$k$ results in a static database and in maintaining the
top-$k$ results when the database continually being updated. Experimental
results validate the effectiveness and efficiency of the proposed method.","Top-k Keyword Search, Relational Databases, Database System"
The View-Update Problem for Indefinite Databases,"This paper introduces and studies a declarative framework for updating views
over indefinite databases. An indefinite database is a database with null
values that are represented, following the standard database approach, by a
single null constant. The paper formalizes views over such databases as
indefinite deductive databases, and defines for them several classes of
database repairs that realize view-update requests. Most notable is the class
of constrained repairs. Constrained repairs change the database ""minimally"" and
avoid making arbitrary commitments. They narrow down the space of alternative
ways to fulfill the view-update request to those that are grounded, in a
certain strong sense, in the database, the view and the view-update request.","Luciano Caroprese, Irina Trubitsyna, Miroslaw Truszczynski, Ester Zumpano","The View-Update Problem for Indefinite Databases. This paper introduces and studies a declarative framework for updating views
over indefinite databases. An indefinite database is a database with null
values that are represented, following the standard database approach, by a
single null constant. The paper formalizes views over such databases as
indefinite deductive databases, and defines for them several classes of
database repairs that realize view-update requests. Most notable is the class
of constrained repairs. Constrained repairs change the database ""minimally"" and
avoid making arbitrary commitments. They narrow down the space of alternative
ways to fulfill the view-update request to those that are grounded, in a
certain strong sense, in the database, the view and the view-update request.","View-Update Problem, Indefinite Databases, Constrained Repairs"
A survey on data and transaction management in mobile databases,"The popularity of the Mobile Database is increasing day by day as people need
information even on the move in the fast changing world. This database
technology permits employees using mobile devices to connect to their corporate
networks, hoard the needed data, work in the disconnected mode and reconnect to
the network to synchronize with the corporate database. In this scenario, the
data is being moved closer to the applications in order to improve the
performance and autonomy. This leads to many interesting problems in mobile
database research and Mobile Database has become a fertile land for many
researchers. In this paper a survey is presented on data and Transaction
management in Mobile Databases from the year 2000 onwards. The survey focuses
on the complete study on the various types of Architectures used in Mobile
databases and Mobile Transaction Models. It also addresses the data management
issues namely Replication and Caching strategies and the transaction management
functionalities such as Concurrency Control and Commit protocols,
Synchronization, Query Processing, Recovery and Security. It also provides
Research Directions in Mobile databases.","D. Roselin Selvarani, T. N. Ravi","A survey on data and transaction management in mobile databases. The popularity of the Mobile Database is increasing day by day as people need
information even on the move in the fast changing world. This database
technology permits employees using mobile devices to connect to their corporate
networks, hoard the needed data, work in the disconnected mode and reconnect to
the network to synchronize with the corporate database. In this scenario, the
data is being moved closer to the applications in order to improve the
performance and autonomy. This leads to many interesting problems in mobile
database research and Mobile Database has become a fertile land for many
researchers. In this paper a survey is presented on data and Transaction
management in Mobile Databases from the year 2000 onwards. The survey focuses
on the complete study on the various types of Architectures used in Mobile
databases and Mobile Transaction Models. It also addresses the data management
issues namely Replication and Caching strategies and the transaction management
functionalities such as Concurrency Control and Commit protocols,
Synchronization, Query Processing, Recovery and Security. It also provides
Research Directions in Mobile databases.","Data Management, Transaction Management, Mobile Databases, Replication, Caching, Concurrency Control, Commit Protocol"
A new Watermarking Technique for Secure Database,"Digital multimedia watermarking technology was suggested in the last decade
to embed copyright information in digital objects such images, audio and video.
However, the increasing use of relational database systems in many real-life
applications created an ever increasing need for watermarking database systems.
As a result, watermarking relational database systems is now merging as a
research area that deals with the legal issue of copyright protection of
database systems. Approach: In this study, we proposed an efficient database
watermarking algorithm based on inserting binary image watermarks in
non-numeric mutli-word attributes of selected database tuples. Results: The
algorithm is robust as it resists attempts to remove or degrade the embedded
watermark and it is blind as it does not require the original database in order
to extract the embedded watermark. Conclusion: Experimental results
demonstrated blindness and the robustness of the algorithm against common
database attacks.","Jun Ziang Pinn, A. Fr. Zung","A new Watermarking Technique for Secure Database. Digital multimedia watermarking technology was suggested in the last decade
to embed copyright information in digital objects such images, audio and video.
However, the increasing use of relational database systems in many real-life
applications created an ever increasing need for watermarking database systems.
As a result, watermarking relational database systems is now merging as a
research area that deals with the legal issue of copyright protection of
database systems. Approach: In this study, we proposed an efficient database
watermarking algorithm based on inserting binary image watermarks in
non-numeric mutli-word attributes of selected database tuples. Results: The
algorithm is robust as it resists attempts to remove or degrade the embedded
watermark and it is blind as it does not require the original database in order
to extract the embedded watermark. Conclusion: Experimental results
demonstrated blindness and the robustness of the algorithm against common
database attacks.","Watermarking Technique, Secure Database, Relational Database, Digital Multimedia Watermarking, Copyright Protection"
"Automatic Conversion of Relational Databases into Ontologies: A
  Comparative Analysis of Protégé Plug-ins Performances","Constructing ontologies from relational databases is an active research topic
in the Semantic Web domain. While conceptual mapping rules/principles of
relational databases and ontology structures are being proposed, several
software modules or plug-ins are being developed to enable the automatic
conversion of relational databases into ontologies. However, the correlation
between the resulting ontologies built automatically with plug-ins from
relational databases and the database-to-ontology mapping principles has been
given little attention. This study reviews and applies two Prot\'eg\'e
plug-ins, namely, DataMaster and OntoBase to automatically construct ontologies
from a relational database. The resulting ontologies are further analysed to
match their structures against the database-to-ontology mapping principles. A
comparative analysis of the matching results reveals that OntoBase outperforms
DataMaster in applying the database-to-ontology mapping principles for
automatically converting relational databases into ontologies.","Kgotatso Desmond Mogotlane, Jean Vincent Fonou-Dombeu","Automatic Conversion of Relational Databases into Ontologies: A
  Comparative Analysis of Protégé Plug-ins Performances. Constructing ontologies from relational databases is an active research topic
in the Semantic Web domain. While conceptual mapping rules/principles of
relational databases and ontology structures are being proposed, several
software modules or plug-ins are being developed to enable the automatic
conversion of relational databases into ontologies. However, the correlation
between the resulting ontologies built automatically with plug-ins from
relational databases and the database-to-ontology mapping principles has been
given little attention. This study reviews and applies two Prot\'eg\'e
plug-ins, namely, DataMaster and OntoBase to automatically construct ontologies
from a relational database. The resulting ontologies are further analysed to
match their structures against the database-to-ontology mapping principles. A
comparative analysis of the matching results reveals that OntoBase outperforms
DataMaster in applying the database-to-ontology mapping principles for
automatically converting relational databases into ontologies.","Automatic Conversion, Relational Databases, Ontologies, DataMaster, OntoBase, Semantic Web"
"T-DB: Toward Fully Functional Transparent Encrypted Databases in DBaaS
  Framework","Individuals and organizations tend to migrate their data to clouds,
especially in a DataBase as a Service (DBaaS) pattern. The major obstacle is
the conflict between secrecy and utilization of the relational database to be
outsourced. We address this obstacle with a Transparent DataBase (T-DB) system
strictly following the unmodified DBaaS framework. A database owner outsources
an encrypted database to a cloud platform, needing only to store the secret
keys for encryption and an empty table header for the database; the database
users can make almost all types of queries on the encrypted database as usual;
and the cloud can process ciphertext queries as if the database were not
encrypted. Experimentations in realistic cloud environments demonstrate that
T-DB has perfect query answer precision and outstanding performance.","Xiaofei Wang, Qianhong Wu, Yuqing Zhang","T-DB: Toward Fully Functional Transparent Encrypted Databases in DBaaS
  Framework. Individuals and organizations tend to migrate their data to clouds,
especially in a DataBase as a Service (DBaaS) pattern. The major obstacle is
the conflict between secrecy and utilization of the relational database to be
outsourced. We address this obstacle with a Transparent DataBase (T-DB) system
strictly following the unmodified DBaaS framework. A database owner outsources
an encrypted database to a cloud platform, needing only to store the secret
keys for encryption and an empty table header for the database; the database
users can make almost all types of queries on the encrypted database as usual;
and the cloud can process ciphertext queries as if the database were not
encrypted. Experimentations in realistic cloud environments demonstrate that
T-DB has perfect query answer precision and outstanding performance.","T-DB, Transparent Encrypted Databases, DBaaS, Cloud, Ciphertext"
"Big Data, Big Decisions Choosing the Right Database","In the burgeoning era of big data, selecting the optimal database solution
has become a critical decision for organizations across every industry. Big
data demands a powerful database solution. Traditionally, SQL Database,
Database ruled, offering a structured approach familiar to many organizations.
However, big data's complexity and unstructured nature challenge SQL Database's
limitations. Enter NoSQL Database: flexible and scalable, making them ideal for
big data's ever-changing nature. We'll explore the key differences between SQL
and NoSQL Database. Performance-wise, SQL Database shines for structured
queries. Its standardized language (SQL) ensures data consistency and complex
analysis. But for big data's unstructured formats, this rigidity becomes a
hurdle. NoSQL offers a welcome contrast. Its flexible schema allows for diverse
data formats and evolving structures, perfect for undefined or frequently
changing data models. Additionally, NoSQL boasts superior horizontal
scalability, distributing data across multiple servers for cost-effective
growth. Understanding these key differentiators empowers organizations to
choose the optimal database for their big data needs.",Mohamed Hassan,"Big Data, Big Decisions Choosing the Right Database. In the burgeoning era of big data, selecting the optimal database solution
has become a critical decision for organizations across every industry. Big
data demands a powerful database solution. Traditionally, SQL Database,
Database ruled, offering a structured approach familiar to many organizations.
However, big data's complexity and unstructured nature challenge SQL Database's
limitations. Enter NoSQL Database: flexible and scalable, making them ideal for
big data's ever-changing nature. We'll explore the key differences between SQL
and NoSQL Database. Performance-wise, SQL Database shines for structured
queries. Its standardized language (SQL) ensures data consistency and complex
analysis. But for big data's unstructured formats, this rigidity becomes a
hurdle. NoSQL offers a welcome contrast. Its flexible schema allows for diverse
data formats and evolving structures, perfect for undefined or frequently
changing data models. Additionally, NoSQL boasts superior horizontal
scalability, distributing data across multiple servers for cost-effective
growth. Understanding these key differentiators empowers organizations to
choose the optimal database for their big data needs.","Big Data, Database, NoSQL"
Can we measure the impact of a database?,"In disseminating scientific and statistical data, on-line databases have
almost completely replaced traditional paper-based media such as journals and
reference works. Given this, can we measure the impact of a database in the
same way that we measure an author's or journal's impact? To do this, we need
somehow to represent a database as a set of publications, and databases
typically allow a large number of possible decompositions into parts, any of
which could be treated as a publication.
  We show that the definition of the h-index naturally extends to hierarchies,
so that if a database admits some kind of hierarchical interpretation we can
use this as one measure of the importance of a database; moreover, this can be
computed as efficiently as one can compute the normal h-index. This also gives
us a decomposition of the database that might be used for other purposes such
as giving credit to the curators or contributors to the database. We illustrate
the process by analyzing three widely used databases.","Peter Buneman, Dennis Dosso, Matteo Lissandrini, Gianmaria Silvello, He Sun","Can we measure the impact of a database?. In disseminating scientific and statistical data, on-line databases have
almost completely replaced traditional paper-based media such as journals and
reference works. Given this, can we measure the impact of a database in the
same way that we measure an author's or journal's impact? To do this, we need
somehow to represent a database as a set of publications, and databases
typically allow a large number of possible decompositions into parts, any of
which could be treated as a publication.
  We show that the definition of the h-index naturally extends to hierarchies,
so that if a database admits some kind of hierarchical interpretation we can
use this as one measure of the importance of a database; moreover, this can be
computed as efficiently as one can compute the normal h-index. This also gives
us a decomposition of the database that might be used for other purposes such
as giving credit to the curators or contributors to the database. We illustrate
the process by analyzing three widely used databases.","Database Impact, H-index, Hierarchical Interpretation"
"Generating EPICS IOC Databases from a Relational Database - a Different
  Approach","The EPICS based control system of the ISAC radioactive beam facility uses the
CapFast schematic editor to construct the IOC function-block databases. This
allows a self-documenting graphical representation of the IOC software using a
hierarchical, object-like software structure with reusable components. On the
other hand, the control system is quarterbacked by a relational database, which
contains all device information. Using database reports and Perl scripts, a
device instantiation method was developed which generates top-level schematic
files in CapFast format. This method exploits the advantages of device data
entry and reporting using a relational database system while maintaining the
graphical representation of the IOC database.",Rolf Keitel,"Generating EPICS IOC Databases from a Relational Database - a Different
  Approach. The EPICS based control system of the ISAC radioactive beam facility uses the
CapFast schematic editor to construct the IOC function-block databases. This
allows a self-documenting graphical representation of the IOC software using a
hierarchical, object-like software structure with reusable components. On the
other hand, the control system is quarterbacked by a relational database, which
contains all device information. Using database reports and Perl scripts, a
device instantiation method was developed which generates top-level schematic
files in CapFast format. This method exploits the advantages of device data
entry and reporting using a relational database system while maintaining the
graphical representation of the IOC database.","EPICS, IOC Databases, Relational Database, ISAC, CapFast"
Database Manipulation on Quantum Computers,"Manipulating a database system on a quantum computer is an essential aim to
benefit from the promising speed-up of quantum computers over classical
computers in areas that take a vast amount of storage and processing time such
as in databases. In this paper, the basic operations for manipulating the data
in a quantum database will be defined, e.g. INSERT, UPDATE, DELETE, SELECT,
backing up and restoring a database file. This gives the ability to perform the
data processing that usually takes a long processing time on a classical
database system, in a simultaneous way on a quantum computer. Defining a
quantum version of more advanced concepts used in database systems, e.g. the
referential integrity and the relational algebra, is a normal extension to this
work",Ahmed Younes,"Database Manipulation on Quantum Computers. Manipulating a database system on a quantum computer is an essential aim to
benefit from the promising speed-up of quantum computers over classical
computers in areas that take a vast amount of storage and processing time such
as in databases. In this paper, the basic operations for manipulating the data
in a quantum database will be defined, e.g. INSERT, UPDATE, DELETE, SELECT,
backing up and restoring a database file. This gives the ability to perform the
data processing that usually takes a long processing time on a classical
database system, in a simultaneous way on a quantum computer. Defining a
quantum version of more advanced concepts used in database systems, e.g. the
referential integrity and the relational algebra, is a normal extension to this
work","Database Manipulation, Quantum Computers, Relational Algorithms"
Empirical Probabilities in Monadic Deductive Databases,"We address the problem of supporting empirical probabilities in monadic logic
databases. Though the semantics of multivalued logic programs has been studied
extensively, the treatment of probabilities as results of statistical findings
has not been studied in logic programming/deductive databases. We develop a
model-theoretic characterization of logic databases that facilitates such a
treatment. We present an algorithm for checking consistency of such databases
and prove its total correctness. We develop a sound and complete query
processing procedure for handling queries to such databases.","Raymond T. Ng, V. S. Subrahmanian","Empirical Probabilities in Monadic Deductive Databases. We address the problem of supporting empirical probabilities in monadic logic
databases. Though the semantics of multivalued logic programs has been studied
extensively, the treatment of probabilities as results of statistical findings
has not been studied in logic programming/deductive databases. We develop a
model-theoretic characterization of logic databases that facilitates such a
treatment. We present an algorithm for checking consistency of such databases
and prove its total correctness. We develop a sound and complete query
processing procedure for handling queries to such databases.","Empirical Probabilities, Monadic, Deductive Databases, Multivalued Logic Programs"
Causality in Databases: The Diagnosis and Repair Connections,"In this work we establish and investigate the connections between causality
for query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. The vast body of research on database repairs can be applied
to the newer problem of determining actual causes for query answers. By
formulating a causality problem as a diagnosis problem, we manage to
characterize causes in terms of a system's diagnoses.","Babak Salimi, Leopoldo Bertossi","Causality in Databases: The Diagnosis and Repair Connections. In this work we establish and investigate the connections between causality
for query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. The vast body of research on database repairs can be applied
to the newer problem of determining actual causes for query answers. By
formulating a causality problem as a diagnosis problem, we manage to
characterize causes in terms of a system's diagnoses.","Causality, Databases, Database Repair, Denial Constraints, Consistency-based Diagnosis"
"The Capacity of Multi-round Private Information Retrieval from Byzantine
  Databases","In this work, we investigate the capacity of private information retrieval
(PIR) from $N$ replicated databases, where a subset of the databases are
untrustworthy (byzantine) in their answers to the query of the user. We allow
for multi-round queries and demonstrate that the identities of the byzantine
databases can be determined with a small additional download cost. As a result,
the capacity of the multi-round PIR with byzantine databases (BPIR) reaches
that of the robust PIR problem when the number of byzantine databases is less
than the number of trustworthy databases.","Xinyu Yao, Nan Liu, Wei Kang","The Capacity of Multi-round Private Information Retrieval from Byzantine
  Databases. In this work, we investigate the capacity of private information retrieval
(PIR) from $N$ replicated databases, where a subset of the databases are
untrustworthy (byzantine) in their answers to the query of the user. We allow
for multi-round queries and demonstrate that the identities of the byzantine
databases can be determined with a small additional download cost. As a result,
the capacity of the multi-round PIR with byzantine databases (BPIR) reaches
that of the robust PIR problem when the number of byzantine databases is less
than the number of trustworthy databases.","Private Information Retrieval, Byzantine Databases, Database Replication"
Inconsistency Measures for Relational Databases,"In this paper, building on work done on measuring inconsistency in knowledge
bases, we introduce inconsistency measures for databases. In particular,
focusing on databases with denial constraints, we first consider the natural
approach of virtually transforming a database into a propositional knowledge
base and then applying well-known measures. However, using this method, tuples
and constraints are equally considered in charge of inconsistencies. Then, we
introduce a version of inconsistency measures blaming database tuples only,
i.e., treating integrity constraints as irrefutable statements.
  We analyze the compliance of database inconsistency measures with standard
rationality postulates and find interesting relationships between measures.
Finally, we investigate the complexity of the inconsistency measurement problem
as well as of the problems of deciding whether the inconsistency is lower than,
greater than, or equal to a given threshold.","Francesco Parisi, John Grant","Inconsistency Measures for Relational Databases. In this paper, building on work done on measuring inconsistency in knowledge
bases, we introduce inconsistency measures for databases. In particular,
focusing on databases with denial constraints, we first consider the natural
approach of virtually transforming a database into a propositional knowledge
base and then applying well-known measures. However, using this method, tuples
and constraints are equally considered in charge of inconsistencies. Then, we
introduce a version of inconsistency measures blaming database tuples only,
i.e., treating integrity constraints as irrefutable statements.
  We analyze the compliance of database inconsistency measures with standard
rationality postulates and find interesting relationships between measures.
Finally, we investigate the complexity of the inconsistency measurement problem
as well as of the problems of deciding whether the inconsistency is lower than,
greater than, or equal to a given threshold.","Inconsistency Measures, Relational Databases, Denial Constraints"
Creating a dynamic database of finite groups,"A database of abstract groups has been added to the L-functions and Modular
Forms Database (LMFDB), available at https://www.lmfdb.org/Groups/Abstract/. We
discuss the functionality of the database and what makes it distinct from other
available databases of abstract groups. We describe solutions to mathematical
problems we encountered while creating the database, as well as connections
between the abstract groups database and other collections of objects in the
LMFDB.","Lewis Combes, John W. Jones, Jennifer Paulhus, David Roe, Manami Roy, Sam Schiavone","Creating a dynamic database of finite groups. A database of abstract groups has been added to the L-functions and Modular
Forms Database (LMFDB), available at https://www.lmfdb.org/Groups/Abstract/. We
discuss the functionality of the database and what makes it distinct from other
available databases of abstract groups. We describe solutions to mathematical
problems we encountered while creating the database, as well as connections
between the abstract groups database and other collections of objects in the
LMFDB.","Abstract Groups, L-functions, ModularForms Database, LMFDB"
Real-Time Outlier Connections Detection in Databases Network Traffic,"The article describes a practical method for detecting outlier database
connections in real-time. Outlier connections are detected with a specified
level of confidence. The method is based on generalized security rules and a
simple but effective real-time machine learning mechanism. The described method
is non-intrusive to the database and does not depend on the type of database.
The method is used to proactively control access even before database
connection is established, minimize false positives, and maintain the required
response speed to detected database connection outliers. The capabilities of
the system are demonstrated with several examples of outliers in real-world
scenarios.","Leonid Rodniansky, Tania Butovsky, Mikhail Shpak","Real-Time Outlier Connections Detection in Databases Network Traffic. The article describes a practical method for detecting outlier database
connections in real-time. Outlier connections are detected with a specified
level of confidence. The method is based on generalized security rules and a
simple but effective real-time machine learning mechanism. The described method
is non-intrusive to the database and does not depend on the type of database.
The method is used to proactively control access even before database
connection is established, minimize false positives, and maintain the required
response speed to detected database connection outliers. The capabilities of
the system are demonstrated with several examples of outliers in real-world
scenarios.","Real-Time Outlier Connections Detection, Databases, Network Traffic, Machine Learning"
The Evolution of the Computerized Database,"Databases, collections of related data, are as old as the written word. A
database can be anything from a homemaker's metal recipe file to a
sophisticated data warehouse. Yet today, when we think of a database we
invariably think of computerized data and their DBMSs (database management
systems). How did we go from organizing our data in a simple metal filing box
or cabinet to storing our data in a sophisticated computerized database? How
did the computerized database evolve?
  This paper defines what we mean by a database. It traces the evolution of the
database, from its start as a non-computerized set of related data, to the, now
standard, computerized RDBMS (relational database management system). Early
computerized storage methods are reviewed including both the ISAM (Indexed
Sequential Access Method) and VSAM (Virtual Storage Access Method) storage
methods. Early database models are explored including the network and
hierarchical database models. Eventually, the relational, object-relational and
object-oriented databases models are discussed. An appendix of diagrams,
including hierarchical occurrence tree, network schema, ER (entity
relationship) and UML (unified modeling language) diagrams, is included to
support the text.
  This paper concludes with an exploration of current and future trends in DBMS
development. It discusses the factors affecting these trends. It delves into
the relationship between DBMSs and the increasingly popular object-oriented
development methodologies. Finally, it speculates on the future of the DBMS.",Nancy Hartline Bercich,"The Evolution of the Computerized Database. Databases, collections of related data, are as old as the written word. A
database can be anything from a homemaker's metal recipe file to a
sophisticated data warehouse. Yet today, when we think of a database we
invariably think of computerized data and their DBMSs (database management
systems). How did we go from organizing our data in a simple metal filing box
or cabinet to storing our data in a sophisticated computerized database? How
did the computerized database evolve?
  This paper defines what we mean by a database. It traces the evolution of the
database, from its start as a non-computerized set of related data, to the, now
standard, computerized RDBMS (relational database management system). Early
computerized storage methods are reviewed including both the ISAM (Indexed
Sequential Access Method) and VSAM (Virtual Storage Access Method) storage
methods. Early database models are explored including the network and
hierarchical database models. Eventually, the relational, object-relational and
object-oriented databases models are discussed. An appendix of diagrams,
including hierarchical occurrence tree, network schema, ER (entity
relationship) and UML (unified modeling language) diagrams, is included to
support the text.
  This paper concludes with an exploration of current and future trends in DBMS
development. It discusses the factors affecting these trends. It delves into
the relationship between DBMSs and the increasingly popular object-oriented
development methodologies. Finally, it speculates on the future of the DBMS.","Database Evolution, Computerized Database, Database Management Systems, Relational, Object-Relational, UML, VSAM"
DB Category: Denotational Semantics for View-based Database Mappings,"We present a categorical denotational semantics for a database mapping, based
on views, in the most general framework of a database integration/exchange.
Developed database category DB, for databases (objects) and view-based mappings
(morphisms) between them, is different from Set category: the morphisms (based
on a set of complex query computations) are not functions, while the objects
are database instances (sets of relations). The logic based schema mappings
between databases, usually written in a highly expressive logical language (ex.
LAV, GAV, GLAV mappings, or tuple generating dependency) may be functorially
translated into this ""computation"" category DB. A new approach is adopted,
based on the behavioral point of view for databases, and behavioral
equivalences for databases and their mappings are established. By introduction
of view-based observations for databases, which are computations without
side-effects, we define a fundamental (Universal algebra) monad with a
power-view endofunctor T. The resulting 2-category DB is symmetric, so that any
mapping can be represented as an object (database instance) as well, where a
higher-level mapping between mappings is a 2-cell morphism. Database category
DB has the following properties: it is equal to its dual, complete and
cocomplete. Special attention is devoted to practical examples: a query
definition, a query rewriting in GAV Database-integration environment, and the
fixpoint solution of a canonical data integration model.",Zoran Majkic,"DB Category: Denotational Semantics for View-based Database Mappings. We present a categorical denotational semantics for a database mapping, based
on views, in the most general framework of a database integration/exchange.
Developed database category DB, for databases (objects) and view-based mappings
(morphisms) between them, is different from Set category: the morphisms (based
on a set of complex query computations) are not functions, while the objects
are database instances (sets of relations). The logic based schema mappings
between databases, usually written in a highly expressive logical language (ex.
LAV, GAV, GLAV mappings, or tuple generating dependency) may be functorially
translated into this ""computation"" category DB. A new approach is adopted,
based on the behavioral point of view for databases, and behavioral
equivalences for databases and their mappings are established. By introduction
of view-based observations for databases, which are computations without
side-effects, we define a fundamental (Universal algebra) monad with a
power-view endofunctor T. The resulting 2-category DB is symmetric, so that any
mapping can be represented as an object (database instance) as well, where a
higher-level mapping between mappings is a 2-cell morphism. Database category
DB has the following properties: it is equal to its dual, complete and
cocomplete. Special attention is devoted to practical examples: a query
definition, a query rewriting in GAV Database-integration environment, and the
fixpoint solution of a canonical data integration model.","DB Category, Denotational Semantics, View-based Database Mappings, Database Integration, GAV, GLAV"
A Call to Arms: Revisiting Database Design,"Good database design is crucial to obtain a sound, consistent database, and -
in turn - good database design methodologies are the best way to achieve the
right design. These methodologies are taught to most Computer Science
undergraduates, as part of any Introduction to Database class. They can be
considered part of the ""canon"", and indeed, the overall approach to database
design has been unchanged for years. Moreover, none of the major database
research assessments identify database design as a strategic research
direction.
  Should we conclude that database design is a solved problem?
  Our thesis is that database design remains a critical unsolved problem.
Hence, it should be the subject of more research. Our starting point is the
observation that traditional database design is not used in practice - and if
it were used it would result in designs that are not well adapted to current
environments. In short, database design has failed to keep up with the times.
In this paper, we put forth arguments to support our viewpoint, analyze the
root causes of this situation and suggest some avenues of research.","Antonio Badia, Daniel Lemire","A Call to Arms: Revisiting Database Design. Good database design is crucial to obtain a sound, consistent database, and -
in turn - good database design methodologies are the best way to achieve the
right design. These methodologies are taught to most Computer Science
undergraduates, as part of any Introduction to Database class. They can be
considered part of the ""canon"", and indeed, the overall approach to database
design has been unchanged for years. Moreover, none of the major database
research assessments identify database design as a strategic research
direction.
  Should we conclude that database design is a solved problem?
  Our thesis is that database design remains a critical unsolved problem.
Hence, it should be the subject of more research. Our starting point is the
observation that traditional database design is not used in practice - and if
it were used it would result in designs that are not well adapted to current
environments. In short, database design has failed to keep up with the times.
In this paper, we put forth arguments to support our viewpoint, analyze the
root causes of this situation and suggest some avenues of research.","Database Design, Computer Science"
"Integrated ERP System for Improving the Functional efficiency of the
  organization by Customized Architecture","An ERP is a kind of package which consist front end and backend as DBMS like
a collection of DBMSs. You can create DBMS to manage one aspect of your
business. For example, a publishing house has a database of books that keeps
information about books such as Author Name, Title, Translator Name, etc. But
this database app only helps enter books data and search them. It doesn't help
them, for example, sell books. They get or develop another DBMS database that
has all the Books data plus prices, discount formulas, names of common clients,
etc. Now they connect the Books database to Sales database and maybe also the
inventory database. Now its DBMS slowly turning into an ERP. They may add
payroll database and connect it to this ERP. They may develop sales staff and
commissions database and connect it to this ERP and so on. In the traditional
Database management system the different databases are used for the various
Campuses of the JSPM Group of Education like Wagholi Campus, Tathwade Campus,
Narhe Campus, Hadpsar Campuses, Bhavdhan Campus as well as Corporate office at
Katraj of same organization so it is not possible to keep different databases
for the same so in this paper proposed the use of Integrated Database for the
Entire organization using ERP system. The Proposed ERP system applied on the
existing Architecture of the JSPM Group; the marginal difference observed in
the Databases need to be accessed to generate the same number of Reports when
use the Traditional DBMS which end up with improvement in the Functional
efficiency of Organizational Architecture.","Suryakant B. Patil, Vijay S. Suryawanshi, Dipali V. Suryawanshi, Preeti Patil","Integrated ERP System for Improving the Functional efficiency of the
  organization by Customized Architecture. An ERP is a kind of package which consist front end and backend as DBMS like
a collection of DBMSs. You can create DBMS to manage one aspect of your
business. For example, a publishing house has a database of books that keeps
information about books such as Author Name, Title, Translator Name, etc. But
this database app only helps enter books data and search them. It doesn't help
them, for example, sell books. They get or develop another DBMS database that
has all the Books data plus prices, discount formulas, names of common clients,
etc. Now they connect the Books database to Sales database and maybe also the
inventory database. Now its DBMS slowly turning into an ERP. They may add
payroll database and connect it to this ERP. They may develop sales staff and
commissions database and connect it to this ERP and so on. In the traditional
Database management system the different databases are used for the various
Campuses of the JSPM Group of Education like Wagholi Campus, Tathwade Campus,
Narhe Campus, Hadpsar Campuses, Bhavdhan Campus as well as Corporate office at
Katraj of same organization so it is not possible to keep different databases
for the same so in this paper proposed the use of Integrated Database for the
Entire organization using ERP system. The Proposed ERP system applied on the
existing Architecture of the JSPM Group; the marginal difference observed in
the Databases need to be accessed to generate the same number of Reports when
use the Traditional DBMS which end up with improvement in the Functional
efficiency of Organizational Architecture.","Integrated ERP System, ERP, JSPM Group, Customized Architecture, Database Management System, Organizational Architecture"
"Graph or Relational Databases: A Speed Comparison for Process Mining
  Algorithm","Process-Aware Information System (PAIS) are IT systems that manages, supports
business processes and generate large event logs from execution of business
processes. An event log is represented as a tuple of the form CaseID,
TimeStamp, Activity and Actor. Process Mining is an emerging area of research
that deals with the study and analysis of business processes based on event
logs. Process Mining aims at analyzing event logs and discover business process
models, enhance them or check for conformance with an a priori model. The large
volume of event logs generated are stored in databases. Relational databases
perform well for certain class of applications. However, there are certain
class of applications for which relational databases are not able to scale. A
number of NoSQL databases have emerged to encounter the challenges of
scalability. Discovering social network from event logs is one of the most
challenging and important Process Mining task. Similar-Task and Sub-Contract
algorithms are some of the most widely used Organizational Mining techniques.
Our objective is to investigate which of the databases (Relational or Graph)
perform better for Organizational Mining under Process Mining. An intersection
of Process Mining and Graph Databases can be accomplished by modelling these
Organizational Mining metrics with graph databases. We implement Similar-Task
and Sub-Contract algorithms on relational and NoSQL (graph-oriented) databases
using only query language constructs. We conduct empirical analysis on a large
real world data set to compare the performance of row-oriented database and
NoSQL graph-oriented database. We benchmark performance factors like query
execution time, CPU usage and disk/memory space usage for NoSQL graph-oriented
database against row-oriented database.","Jeevan Joishi, Ashish Sureka","Graph or Relational Databases: A Speed Comparison for Process Mining
  Algorithm. Process-Aware Information System (PAIS) are IT systems that manages, supports
business processes and generate large event logs from execution of business
processes. An event log is represented as a tuple of the form CaseID,
TimeStamp, Activity and Actor. Process Mining is an emerging area of research
that deals with the study and analysis of business processes based on event
logs. Process Mining aims at analyzing event logs and discover business process
models, enhance them or check for conformance with an a priori model. The large
volume of event logs generated are stored in databases. Relational databases
perform well for certain class of applications. However, there are certain
class of applications for which relational databases are not able to scale. A
number of NoSQL databases have emerged to encounter the challenges of
scalability. Discovering social network from event logs is one of the most
challenging and important Process Mining task. Similar-Task and Sub-Contract
algorithms are some of the most widely used Organizational Mining techniques.
Our objective is to investigate which of the databases (Relational or Graph)
perform better for Organizational Mining under Process Mining. An intersection
of Process Mining and Graph Databases can be accomplished by modelling these
Organizational Mining metrics with graph databases. We implement Similar-Task
and Sub-Contract algorithms on relational and NoSQL (graph-oriented) databases
using only query language constructs. We conduct empirical analysis on a large
real world data set to compare the performance of row-oriented database and
NoSQL graph-oriented database. We benchmark performance factors like query
execution time, CPU usage and disk/memory space usage for NoSQL graph-oriented
database against row-oriented database.","Process Mining, Relational Databases, NoSQL, Similar-Task, Sub-Contract Algorithms"
Privacy-Preserving Database Fingerprinting,"When sharing sensitive relational databases with other parties, a database
owner aims to (i) have privacy guarantees for the database entries, (ii) have
liability guarantees (via fingerprinting) in case of unauthorized sharing of
its database by the recipients, and (iii) provide a high quality (utility)
database to the recipients. We observe that sharing a relational database with
privacy and liability guarantees are orthogonal objectives. The former can be
achieved by injecting noise into the database to prevent inference of the
original data values, whereas, the latter can be achieved by hiding unique
marks inside the database to trace malicious parties (data recipients) who
redistribute the data without the authorization. We achieve these two
objectives simultaneously by proposing a novel entry-level
differentially-private fingerprinting mechanism for relational databases.
  At a high level, the proposed mechanism fulfills the privacy and liability
requirements by leveraging the randomization nature that is intrinsic to
fingerprinting and achieves desired entry-level privacy guarantees. To be more
specific, we devise a bit-level random response scheme to achieve differential
privacy guarantee for arbitrary data entries when sharing the entire database,
and then, based on this, we develop an $\epsilon$-entry-level
differentially-private fingerprinting mechanism. Next, we theoretically analyze
the relationships between privacy guarantee, fingerprint robustness, and
database utility by deriving closed form expressions. The outcome of this
analysis allows us to bound the privacy leakage caused by attribute inference
attack and characterize the privacy-utility coupling and privacy-fingerprint
robustness coupling. Furthermore, we also propose a SVT-based solution to
control the cumulative privacy loss when fingerprinted copies of a database are
shared with multiple recipients.","Tianxi Ji, Erman Ayday, Emre Yilmaz, Pan Li","Privacy-Preserving Database Fingerprinting. When sharing sensitive relational databases with other parties, a database
owner aims to (i) have privacy guarantees for the database entries, (ii) have
liability guarantees (via fingerprinting) in case of unauthorized sharing of
its database by the recipients, and (iii) provide a high quality (utility)
database to the recipients. We observe that sharing a relational database with
privacy and liability guarantees are orthogonal objectives. The former can be
achieved by injecting noise into the database to prevent inference of the
original data values, whereas, the latter can be achieved by hiding unique
marks inside the database to trace malicious parties (data recipients) who
redistribute the data without the authorization. We achieve these two
objectives simultaneously by proposing a novel entry-level
differentially-private fingerprinting mechanism for relational databases.
  At a high level, the proposed mechanism fulfills the privacy and liability
requirements by leveraging the randomization nature that is intrinsic to
fingerprinting and achieves desired entry-level privacy guarantees. To be more
specific, we devise a bit-level random response scheme to achieve differential
privacy guarantee for arbitrary data entries when sharing the entire database,
and then, based on this, we develop an $\epsilon$-entry-level
differentially-private fingerprinting mechanism. Next, we theoretically analyze
the relationships between privacy guarantee, fingerprint robustness, and
database utility by deriving closed form expressions. The outcome of this
analysis allows us to bound the privacy leakage caused by attribute inference
attack and characterize the privacy-utility coupling and privacy-fingerprint
robustness coupling. Furthermore, we also propose a SVT-based solution to
control the cumulative privacy loss when fingerprinted copies of a database are
shared with multiple recipients.","Database Fingerprinting, Relational Databases, Authentication, Liability"
"An Empirical Study on the Characteristics of Database Access Bugs in
  Java Applications","Database-backed applications rely on the database access code to interact
with the underlying database management systems (DBMSs). Although many prior
studies aim at database access issues like SQL anti-patterns or SQL code
smells, there is a lack of study of database access bugs during the maintenance
of database-backed applications. In this paper, we empirically investigate 423
database access bugs collected from seven large-scale Java open source
applications that use relational database management systems (e.g., MySQL or
PostgreSQL). We study the characteristics (e.g., occurrence and root causes) of
the bugs by manually examining the bug reports and commit histories. We find
that the number of reported database and non-database access bugs share a
similar trend but their modified files in bug fixing commits are different.
Additionally, we generalize categories of the root causes of database access
bugs, containing five main categories (SQL queries, Schema, API, Configuration,
SQL query result) and 25 unique root causes. We find that the bugs pertaining
to SQL queries, Schema, and API cover 84.2% of database access bugs across all
studied applications. In particular, SQL queries bug (54%) and API bug (38.7%)
are the most frequent issues when using JDBC and Hibernate, respectively.
Finally, we provide a discussion on the implications of our findings for
developers and researchers.","Wei Liu, Shouvick Mondal, Tse-Hsun Chen","An Empirical Study on the Characteristics of Database Access Bugs in
  Java Applications. Database-backed applications rely on the database access code to interact
with the underlying database management systems (DBMSs). Although many prior
studies aim at database access issues like SQL anti-patterns or SQL code
smells, there is a lack of study of database access bugs during the maintenance
of database-backed applications. In this paper, we empirically investigate 423
database access bugs collected from seven large-scale Java open source
applications that use relational database management systems (e.g., MySQL or
PostgreSQL). We study the characteristics (e.g., occurrence and root causes) of
the bugs by manually examining the bug reports and commit histories. We find
that the number of reported database and non-database access bugs share a
similar trend but their modified files in bug fixing commits are different.
Additionally, we generalize categories of the root causes of database access
bugs, containing five main categories (SQL queries, Schema, API, Configuration,
SQL query result) and 25 unique root causes. We find that the bugs pertaining
to SQL queries, Schema, and API cover 84.2% of database access bugs across all
studied applications. In particular, SQL queries bug (54%) and API bug (38.7%)
are the most frequent issues when using JDBC and Hibernate, respectively.
Finally, we provide a discussion on the implications of our findings for
developers and researchers.","Java, Database Access Bugs, JDBC, Hibernate, MySQL, PostgreSQL"
Enhanced Differential Testing in Emerging Database Systems,"In recent years, a plethora of database management systems have surfaced to
meet the demands of various scenarios. Emerging database systems, such as
time-series and streaming database systems, are tailored to specific use cases
requiring enhanced functionality and performance. However, as they are
typically less mature, there can be bugs that either cause incorrect results or
errors impacting reliability. To tackle this, we propose enhanced differential
testing to uncover various bugs in emerging SQL-like database systems. The
challenge is how to deal with differences of these emerging databases. Our
insight is that many emerging database systems are conceptually extensions of
relational database systems, making it possible to reveal logic bugs leveraging
existing relational, known-reliable database systems. However, due to
inevitable syntax or semantics gaps, it remains challenging to scale
differential testing to various emerging database systems. We enhance
differential testing for emerging database systems with three steps: (i)
identifying shared clauses; (ii) extending shared clauses via mapping new
features back to existing clauses of relational database systems; and (iii)
generating differential inputs using extended shared clauses. We implemented
our approach in a tool called SQLxDiff and applied it to four popular emerging
database systems. In total, we found 57 unknown bugs, of which 17 were logic
bugs and 40 were internal errors. Overall, vendors fixed 50 bugs and confirmed
5. Our results demonstrate the practicality and effectiveness of SQLxDiff in
detecting bugs in emerging database systems, which has the potential to improve
the reliability of their applications.","Yuancheng Jiang, Jianing Wang, Chuqi Zhang, Roland Yap, Zhenkai Liang, Manuel Rigger","Enhanced Differential Testing in Emerging Database Systems. In recent years, a plethora of database management systems have surfaced to
meet the demands of various scenarios. Emerging database systems, such as
time-series and streaming database systems, are tailored to specific use cases
requiring enhanced functionality and performance. However, as they are
typically less mature, there can be bugs that either cause incorrect results or
errors impacting reliability. To tackle this, we propose enhanced differential
testing to uncover various bugs in emerging SQL-like database systems. The
challenge is how to deal with differences of these emerging databases. Our
insight is that many emerging database systems are conceptually extensions of
relational database systems, making it possible to reveal logic bugs leveraging
existing relational, known-reliable database systems. However, due to
inevitable syntax or semantics gaps, it remains challenging to scale
differential testing to various emerging database systems. We enhance
differential testing for emerging database systems with three steps: (i)
identifying shared clauses; (ii) extending shared clauses via mapping new
features back to existing clauses of relational database systems; and (iii)
generating differential inputs using extended shared clauses. We implemented
our approach in a tool called SQLxDiff and applied it to four popular emerging
database systems. In total, we found 57 unknown bugs, of which 17 were logic
bugs and 40 were internal errors. Overall, vendors fixed 50 bugs and confirmed
5. Our results demonstrate the practicality and effectiveness of SQLxDiff in
detecting bugs in emerging database systems, which has the potential to improve
the reliability of their applications.","Differential Testing, Database Systems, Time Series, Streaming Database, SQLxDiff"
Database Transposition for Constrained (Closed) Pattern Mining,"Recently, different works proposed a new way to mine patterns in databases
with pathological size. For example, experiments in genome biology usually
provide databases with thousands of attributes (genes) but only tens of objects
(experiments). In this case, mining the ""transposed"" database runs through a
smaller search space, and the Galois connection allows to infer the closed
patterns of the original database. We focus here on constrained pattern mining
for those unusual databases and give a theoretical framework for database and
constraint transposition. We discuss the properties of constraint transposition
and look into classical constraints. We then address the problem of generating
the closed patterns of the original database satisfying the constraint,
starting from those mined in the ""transposed"" database. Finally, we show how to
generate all the patterns satisfying the constraint from the closed ones.","Baptiste Jeudy, François Rioult","Database Transposition for Constrained (Closed) Pattern Mining. Recently, different works proposed a new way to mine patterns in databases
with pathological size. For example, experiments in genome biology usually
provide databases with thousands of attributes (genes) but only tens of objects
(experiments). In this case, mining the ""transposed"" database runs through a
smaller search space, and the Galois connection allows to infer the closed
patterns of the original database. We focus here on constrained pattern mining
for those unusual databases and give a theoretical framework for database and
constraint transposition. We discuss the properties of constraint transposition
and look into classical constraints. We then address the problem of generating
the closed patterns of the original database satisfying the constraint,
starting from those mined in the ""transposed"" database. Finally, we show how to
generate all the patterns satisfying the constraint from the closed ones.","Database Transposition, Constrained Pattern Mining, Galois Connection, Genome Biology"
LHC Databases on the Grid: Achievements and Open Issues,"To extract physics results from the recorded data, the LHC experiments are
using Grid computing infrastructure. The event data processing on the Grid
requires scalable access to non-event data (detector conditions, calibrations,
etc.) stored in relational databases. The database-resident data are critical
for the event data reconstruction processing steps and often required for
physics analysis. This paper reviews LHC experience with database technologies
for the Grid computing. List of topics includes: database integration with Grid
computing models of the LHC experiments; choice of database technologies;
examples of database interfaces; distributed database applications (data
complexity, update frequency, data volumes and access patterns); scalability of
database access in the Grid computing environment of the LHC experiments. The
review describes areas in which substantial progress was made and remaining
open issues.",A. V. Vaniachine,"LHC Databases on the Grid: Achievements and Open Issues. To extract physics results from the recorded data, the LHC experiments are
using Grid computing infrastructure. The event data processing on the Grid
requires scalable access to non-event data (detector conditions, calibrations,
etc.) stored in relational databases. The database-resident data are critical
for the event data reconstruction processing steps and often required for
physics analysis. This paper reviews LHC experience with database technologies
for the Grid computing. List of topics includes: database integration with Grid
computing models of the LHC experiments; choice of database technologies;
examples of database interfaces; distributed database applications (data
complexity, update frequency, data volumes and access patterns); scalability of
database access in the Grid computing environment of the LHC experiments. The
review describes areas in which substantial progress was made and remaining
open issues.","LHC, Databases, Grid Computing, Physics Analysis"
"Investigating the Detection of Adverse Drug Events in a UK General
  Practice Electronic Health-Care Database","Data-mining techniques have frequently been developed for Spontaneous
reporting databases. These techniques aim to find adverse drug events
accurately and efficiently. Spontaneous reporting databases are prone to
missing information, under reporting and incorrect entries. This often results
in a detection lag or prevents the detection of some adverse drug events. These
limitations do not occur in electronic health-care databases. In this paper,
existing methods developed for spontaneous reporting databases are implemented
on both a spontaneous reporting database and a general practice electronic
health-care database and compared. The results suggests that the application of
existing methods to the general practice database may help find signals that
have gone undetected when using the spontaneous reporting system database. In
addition the general practice database provides far more supplementary
information, that if incorporated in analysis could provide a wealth of
information for identifying adverse events more accurately.","Jenna Reps, Jan Feyereisl, Jonathan M. Garibaldi, Uwe Aickelin, Jack E. Gibson, Richard B. Hubbard","Investigating the Detection of Adverse Drug Events in a UK General
  Practice Electronic Health-Care Database. Data-mining techniques have frequently been developed for Spontaneous
reporting databases. These techniques aim to find adverse drug events
accurately and efficiently. Spontaneous reporting databases are prone to
missing information, under reporting and incorrect entries. This often results
in a detection lag or prevents the detection of some adverse drug events. These
limitations do not occur in electronic health-care databases. In this paper,
existing methods developed for spontaneous reporting databases are implemented
on both a spontaneous reporting database and a general practice electronic
health-care database and compared. The results suggests that the application of
existing methods to the general practice database may help find signals that
have gone undetected when using the spontaneous reporting system database. In
addition the general practice database provides far more supplementary
information, that if incorporated in analysis could provide a wealth of
information for identifying adverse events more accurately.","Spontaneous Reporting, Adverse Drug Events, UK General Practice, Electronic Health-Care Database, Data Mining"
A Novel approach as Multi-place Watermarking for Security in Database,"Digital multimedia watermarking technology had suggested in the last decade
to embed copyright information in digital objects such as images, audio and
video. However, the increasing use of relational database systems in many
real-life applications created an ever-increasing need for watermarking
database systems. As a result, watermarking relational database system is now
emerging as a research area that deals with the legal issue of copyright
protection of database systems. The main goal of database watermarking is to
generate robust and impersistent watermark for database. In this paper we
propose a method, based on image as watermark and this watermark is embedded
over the database at two different attribute of tuple, one in the numeric
attribute of tuple and another in the date attribute's time (seconds) field.
Our approach can be applied for numerical and categorical database.","Brijesh B. Mehta, Udai Pratap Rao","A Novel approach as Multi-place Watermarking for Security in Database. Digital multimedia watermarking technology had suggested in the last decade
to embed copyright information in digital objects such as images, audio and
video. However, the increasing use of relational database systems in many
real-life applications created an ever-increasing need for watermarking
database systems. As a result, watermarking relational database system is now
emerging as a research area that deals with the legal issue of copyright
protection of database systems. The main goal of database watermarking is to
generate robust and impersistent watermark for database. In this paper we
propose a method, based on image as watermark and this watermark is embedded
over the database at two different attribute of tuple, one in the numeric
attribute of tuple and another in the date attribute's time (seconds) field.
Our approach can be applied for numerical and categorical database.","Database Watermarking, Multi-place Watermark, Database, Copyright, Categorical Database"
Query Optimization Techniques In Graph Databases,"Graph databases (GDB) have recently been arisen to overcome the limits of
traditional databases for storing and managing data with graph-like structure.
Today, they represent a requirement for many applications that manage
graph-like data, like social networks. Most of the techniques, applied to
optimize queries in graph databases, have been used in traditional databases,
distribution systems... or they are inspired from graph theory. However, their
reuse in graph databases should take care of the main characteristics of graph
databases, such as dynamic structure, highly interconnected data, and ability
to efficiently access data relationships. In this paper, we survey the query
optimization techniques in graph databases. In particular, we focus on the
features they have introduced to improve querying graph-like data.",Ali Ben Ammar,"Query Optimization Techniques In Graph Databases. Graph databases (GDB) have recently been arisen to overcome the limits of
traditional databases for storing and managing data with graph-like structure.
Today, they represent a requirement for many applications that manage
graph-like data, like social networks. Most of the techniques, applied to
optimize queries in graph databases, have been used in traditional databases,
distribution systems... or they are inspired from graph theory. However, their
reuse in graph databases should take care of the main characteristics of graph
databases, such as dynamic structure, highly interconnected data, and ability
to efficiently access data relationships. In this paper, we survey the query
optimization techniques in graph databases. In particular, we focus on the
features they have introduced to improve querying graph-like data.","Query Optimization Techniques, Graph Databases, GDB, Graph Theory"
"A Comparative Analysis of XML Documents, XML Enabled Databases and
  Native XML Databases","With the increasing popularity of XML data and a great need for a database
management system able to store, retrieve and manipulate XML-based data in an
efficient manner, database research communities and software industries have
tried to respond to this requirement. XML-enabled database and native XML
database are two approaches that have been proposed to address this challenge.
These two approaches are a legacy database systems which are extended to store,
retrieve and manipulate XML-based data. The major objective of this paper is to
explore and compare between the two approaches and reach to some criteria to
have a suitable guideline to select the best approach in each circumstance. In
general, native XML database systems have more ability in comparison with
XML-enabled database system for managing XML-based data","Amir Mohammad Saba, Elham Shahab, Hadi Abdolrahimpour, Mahsa Hakimi, Akbar Moazzam","A Comparative Analysis of XML Documents, XML Enabled Databases and
  Native XML Databases. With the increasing popularity of XML data and a great need for a database
management system able to store, retrieve and manipulate XML-based data in an
efficient manner, database research communities and software industries have
tried to respond to this requirement. XML-enabled database and native XML
database are two approaches that have been proposed to address this challenge.
These two approaches are a legacy database systems which are extended to store,
retrieve and manipulate XML-based data. The major objective of this paper is to
explore and compare between the two approaches and reach to some criteria to
have a suitable guideline to select the best approach in each circumstance. In
general, native XML database systems have more ability in comparison with
XML-enabled database system for managing XML-based data","XML Documents, XML Enabled Databases, Native XML Databases"
"A Comparative Analysis of Materialized Views Selection and Concurrency
  Control Mechanisms in NoSQL Databases","Increasing resource demands require relational databases to scale. While
relational databases are well suited for vertical scaling, specialized hardware
can be expensive. Conversely, emerging NewSQL and NoSQL data stores are
designed to scale horizontally. NewSQL databases provide ACID transaction
support; however, joins are limited to the partition keys, resulting in
restricted query expressiveness. On the other hand, NoSQL databases are
designed to scale out linearly on commodity hardware; however, they are limited
by slow join performance. Hence, we consider if the NoSQL join performance can
be improved while ensuring ACID semantics and without drastically sacrificing
write performance, disk utilization and query expressiveness.
  This paper presents the Synergy system that leverages schema and workload
driven mechanism to identify materialized views and a specialized concurrency
control system on top of a NoSQL database to enable scalable data management
with familiar relational conventions. Synergy trades slight write performance
degradation and increased disk utilization for faster join performance
(compared to standard NoSQL databases) and improved query expressiveness
(compared to NewSQL databases). Experimental results using the TPC-W benchmark
show that, for a database populated with 1M customers, the Synergy system
exhibits a maximum performance improvement of 80.5% as compared to other
evaluated systems.","Ashish Tapdiya, Yuan Xue, Daniel Fabbri","A Comparative Analysis of Materialized Views Selection and Concurrency
  Control Mechanisms in NoSQL Databases. Increasing resource demands require relational databases to scale. While
relational databases are well suited for vertical scaling, specialized hardware
can be expensive. Conversely, emerging NewSQL and NoSQL data stores are
designed to scale horizontally. NewSQL databases provide ACID transaction
support; however, joins are limited to the partition keys, resulting in
restricted query expressiveness. On the other hand, NoSQL databases are
designed to scale out linearly on commodity hardware; however, they are limited
by slow join performance. Hence, we consider if the NoSQL join performance can
be improved while ensuring ACID semantics and without drastically sacrificing
write performance, disk utilization and query expressiveness.
  This paper presents the Synergy system that leverages schema and workload
driven mechanism to identify materialized views and a specialized concurrency
control system on top of a NoSQL database to enable scalable data management
with familiar relational conventions. Synergy trades slight write performance
degradation and increased disk utilization for faster join performance
(compared to standard NoSQL databases) and improved query expressiveness
(compared to NewSQL databases). Experimental results using the TPC-W benchmark
show that, for a database populated with 1M customers, the Synergy system
exhibits a maximum performance improvement of 80.5% as compared to other
evaluated systems.","Synergy, Materialized Views, Concurrency, NoSQL, TPC-W"
Differential Privacy for Growing Databases,"We study the design of differentially private algorithms for adaptive
analysis of dynamically growing databases, where a database accumulates new
data entries while the analysis is ongoing. We provide a collection of tools
for machine learning and other types of data analysis that guarantee
differential privacy and accuracy as the underlying databases grow arbitrarily
large. We give both a general technique and a specific algorithm for adaptive
analysis of dynamically growing databases. Our general technique is illustrated
by two algorithms that schedule black box access to some algorithm that
operates on a fixed database to generically transform private and accurate
algorithms for static databases into private and accurate algorithms for
dynamically growing databases. These results show that almost any private and
accurate algorithm can be rerun at appropriate points of data growth with
minimal loss of accuracy, even when data growth is unbounded. Our specific
algorithm directly adapts the private multiplicative weights algorithm to the
dynamic setting, maintaining the accuracy guarantee of the static setting
through unbounded data growth. Along the way, we develop extensions of several
other differentially private algorithms to the dynamic setting, which may be of
independent interest for future work on the design of differentially private
algorithms for growing databases.","Rachel Cummings, Sara Krehbiel, Kevin A. Lai, Uthaipon Tantipongpipat","Differential Privacy for Growing Databases. We study the design of differentially private algorithms for adaptive
analysis of dynamically growing databases, where a database accumulates new
data entries while the analysis is ongoing. We provide a collection of tools
for machine learning and other types of data analysis that guarantee
differential privacy and accuracy as the underlying databases grow arbitrarily
large. We give both a general technique and a specific algorithm for adaptive
analysis of dynamically growing databases. Our general technique is illustrated
by two algorithms that schedule black box access to some algorithm that
operates on a fixed database to generically transform private and accurate
algorithms for static databases into private and accurate algorithms for
dynamically growing databases. These results show that almost any private and
accurate algorithm can be rerun at appropriate points of data growth with
minimal loss of accuracy, even when data growth is unbounded. Our specific
algorithm directly adapts the private multiplicative weights algorithm to the
dynamic setting, maintaining the accuracy guarantee of the static setting
through unbounded data growth. Along the way, we develop extensions of several
other differentially private algorithms to the dynamic setting, which may be of
independent interest for future work on the design of differentially private
algorithms for growing databases.","Differentially Private Algorithms, Dynamic Databases, Machine Learning, Data Analysis"
"Cracking In-Memory Database Index A Case Study for Adaptive Radix Tree
  Index","Indexes provide a method to access data in databases quickly. It can improve
the response speed of subsequent queries by building a complete index in
advance. However, it also leads to a huge overhead of the continuous updating
during creating the index. An in-memory database usually has a higher query
processing performance than disk databases and is more suitable for real-time
query processing. Therefore, there is an urgent need to reduce the index
creation and update cost for in-memory databases. Database cracking technology
is currently recognized as an effective method to reduce the index
initialization time. However, conventional cracking algorithms are focused on
simple column data structure rather than those complex index structure for
in-memory databases. In order to show the feasibility of in-memory database
index cracking and promote to future more extensive research, this paper
conducted a case study on the Adaptive Radix Tree (ART), a popular tree index
structure of in-memory databases. On the basis of carefully examining the ART
index construction overhead, an algorithm using auxiliary data structures to
crack the ART index is proposed.","Gang Wu, Yidong Song, Guodong Zhao, Wei Sun, Donghong Han, Baiyou Qiao, Guoren Wang, Ye Yuan","Cracking In-Memory Database Index A Case Study for Adaptive Radix Tree
  Index. Indexes provide a method to access data in databases quickly. It can improve
the response speed of subsequent queries by building a complete index in
advance. However, it also leads to a huge overhead of the continuous updating
during creating the index. An in-memory database usually has a higher query
processing performance than disk databases and is more suitable for real-time
query processing. Therefore, there is an urgent need to reduce the index
creation and update cost for in-memory databases. Database cracking technology
is currently recognized as an effective method to reduce the index
initialization time. However, conventional cracking algorithms are focused on
simple column data structure rather than those complex index structure for
in-memory databases. In order to show the feasibility of in-memory database
index cracking and promote to future more extensive research, this paper
conducted a case study on the Adaptive Radix Tree (ART), a popular tree index
structure of in-memory databases. On the basis of carefully examining the ART
index construction overhead, an algorithm using auxiliary data structures to
crack the ART index is proposed.","In-Memory Database Index, Adaptive Radix Tree, Database Cracking, Algorithms"
Automated Database Indexing using Model-free Reinforcement Learning,"Configuring databases for efficient querying is a complex task, often carried
out by a database administrator. Solving the problem of building indexes that
truly optimize database access requires a substantial amount of database and
domain knowledge, the lack of which often results in wasted space and memory
for irrelevant indexes, possibly jeopardizing database performance for querying
and certainly degrading performance for updating. We develop an architecture to
solve the problem of automatically indexing a database by using reinforcement
learning to optimize queries by indexing data throughout the lifetime of a
database. In our experimental evaluation, our architecture shows superior
performance compared to related work on reinforcement learning and genetic
algorithms, maintaining near-optimal index configurations and efficiently
scaling to large databases.","Gabriel Paludo Licks, Felipe Meneguzzi","Automated Database Indexing using Model-free Reinforcement Learning. Configuring databases for efficient querying is a complex task, often carried
out by a database administrator. Solving the problem of building indexes that
truly optimize database access requires a substantial amount of database and
domain knowledge, the lack of which often results in wasted space and memory
for irrelevant indexes, possibly jeopardizing database performance for querying
and certainly degrading performance for updating. We develop an architecture to
solve the problem of automatically indexing a database by using reinforcement
learning to optimize queries by indexing data throughout the lifetime of a
database. In our experimental evaluation, our architecture shows superior
performance compared to related work on reinforcement learning and genetic
algorithms, maintaining near-optimal index configurations and efficiently
scaling to large databases.","Database Indexing, Model-free, Reinforcement Learning, Genetic Algorithms"
Symmetric Private Information Retrieval with User-Side Common Randomness,"We consider the problem of symmetric private information retrieval (SPIR)
with user-side common randomness. In SPIR, a user retrieves a message out of
$K$ messages from $N$ non-colluding and replicated databases in such a way that
no single database knows the retrieved message index (user privacy), and the
user gets to know nothing further than the retrieved message (database
privacy). SPIR has a capacity smaller than the PIR capacity which requires only
user privacy, is infeasible in the case of a single database, and requires
shared common randomness among the databases. We introduce a new variant of
SPIR where the user is provided with a random subset of the shared database
common randomness, which is unknown to the databases. We determine the exact
capacity region of the triple $(d, \rho_S, \rho_U)$, where $d$ is the download
cost, $\rho_S$ is the amount of shared database (server) common randomness, and
$\rho_U$ is the amount of available user-side common randomness. We show that
with a suitable amount of $\rho_U$, this new SPIR achieves the capacity of
conventional PIR. As a corollary, single-database SPIR becomes feasible.
Further, the presence of user-side $\rho_U$ reduces the amount of required
server-side $\rho_S$.","Zhusheng Wang, Sennur Ulukus","Symmetric Private Information Retrieval with User-Side Common Randomness. We consider the problem of symmetric private information retrieval (SPIR)
with user-side common randomness. In SPIR, a user retrieves a message out of
$K$ messages from $N$ non-colluding and replicated databases in such a way that
no single database knows the retrieved message index (user privacy), and the
user gets to know nothing further than the retrieved message (database
privacy). SPIR has a capacity smaller than the PIR capacity which requires only
user privacy, is infeasible in the case of a single database, and requires
shared common randomness among the databases. We introduce a new variant of
SPIR where the user is provided with a random subset of the shared database
common randomness, which is unknown to the databases. We determine the exact
capacity region of the triple $(d, \rho_S, \rho_U)$, where $d$ is the download
cost, $\rho_S$ is the amount of shared database (server) common randomness, and
$\rho_U$ is the amount of available user-side common randomness. We show that
with a suitable amount of $\rho_U$, this new SPIR achieves the capacity of
conventional PIR. As a corollary, single-database SPIR becomes feasible.
Further, the presence of user-side $\rho_U$ reduces the amount of required
server-side $\rho_S$.","Symmetric Private Information Retrieval, SPIR, User-Side Common Randomness, Database"
"Enabling On-Demand Database Computing with MIT SuperCloud Database
  Management System","The MIT SuperCloud database management system allows for rapid creation and
flexible execution of a variety of the latest scientific databases, including
Apache Accumulo and SciDB. It is designed to permit these databases to run on a
High Performance Computing Cluster (HPCC) platform as seamlessly as any other
HPCC job. It ensures the seamless migration of the databases to the resources
assigned by the HPCC scheduler and centralized storage of the database files
when not running. It also permits snapshotting of databases to allow
researchers to experiment and push the limits of the technology without
concerns for data or productivity loss if the database becomes unstable.","Andrew Prout, Jeremy Kepner, Peter Michaleas, William Arcand, David Bestor, Bill Bergeron, Chansup Byun, Lauren Edwards, Vijay Gadepally, Matthew Hubbell, Julie Mullen, Antonio Rosa, Charles Yee, Albert Reuther","Enabling On-Demand Database Computing with MIT SuperCloud Database
  Management System. The MIT SuperCloud database management system allows for rapid creation and
flexible execution of a variety of the latest scientific databases, including
Apache Accumulo and SciDB. It is designed to permit these databases to run on a
High Performance Computing Cluster (HPCC) platform as seamlessly as any other
HPCC job. It ensures the seamless migration of the databases to the resources
assigned by the HPCC scheduler and centralized storage of the database files
when not running. It also permits snapshotting of databases to allow
researchers to experiment and push the limits of the technology without
concerns for data or productivity loss if the database becomes unstable.","MIT SuperCloud, Database Computing, Apache Accumulo, SciDB, High Performance Computing, HPCC"
"DooML: A new Database & Object-Oriented Modeling Language for
  database-driven web application design and development","A database driven web application is a very common software solution to
rising business problems. Modeling the database and the software architecture
can be challenging, hence there not being one combined modeling language for
database and software architecture, specifically suited for web application
development. In this paper we present Database object-oriented Modeling
Language (DooML) and its primary Archetype Diagram: a notation for specifying
the design of a database schema and corresponding object-oriented software
architecture. It combines the syntax for drawing Entity Relationship Diagrams,
the Relational Model and Universal Modeling Language Class Diagrams as well to
create a mixed diagram, stating database design as well as software design
specifications. By default, DooML ensures that the approach of advanced web
application development is model-driven and both database-oriented as well as
object-oriented.",Thijs Otter,"DooML: A new Database & Object-Oriented Modeling Language for
  database-driven web application design and development. A database driven web application is a very common software solution to
rising business problems. Modeling the database and the software architecture
can be challenging, hence there not being one combined modeling language for
database and software architecture, specifically suited for web application
development. In this paper we present Database object-oriented Modeling
Language (DooML) and its primary Archetype Diagram: a notation for specifying
the design of a database schema and corresponding object-oriented software
architecture. It combines the syntax for drawing Entity Relationship Diagrams,
the Relational Model and Universal Modeling Language Class Diagrams as well to
create a mixed diagram, stating database design as well as software design
specifications. By default, DooML ensures that the approach of advanced web
application development is model-driven and both database-oriented as well as
object-oriented.","DooML, Database, Object-Oriented Modeling Language, Web Application Development"
Order-Preserving Database Encryption with Secret Sharing,"The order-preserving encryption (OPE) problem was initially formulated by the
database community in 2004 soon after the paradigm database-as-a-service (DaaS)
was coined in 2002. Over the past two decades, OPE has drawn tremendous
research interest from communities of databases, cryptography, and security; we
have witnessed significant advances in OPE schemes both theoretically and
systematically. All existing OPE schemes assume that the outsourced database is
modeled as a single semi-honest adversary who should learn nothing more than
the order information of plaintext messages up to a negligible probability.
This paper addresses the OPE problem from a new perspective: instead of
modeling the outsourced database as a single semi-honest adversary, we assume
the outsourced database \textit{service} compromises a cluster of non-colluding
servers, which is a practical assumption as all major cloud vendors support
multiple database instances deployed to exclusive sub-networks or even to
distinct data centers. This assumption allows us to design a new stateless OPE
protocol, namely order-preserving database encryption with secret sharing
(ODES), by employing secret-sharing schemes among those presumably
non-colluding servers. We will demonstrate that ODES guarantees the latest
security level, namely IND-FAOCPA, and outperforms the state-of-the-art scheme
by orders of magnitude.",Dongfang Zhao,"Order-Preserving Database Encryption with Secret Sharing. The order-preserving encryption (OPE) problem was initially formulated by the
database community in 2004 soon after the paradigm database-as-a-service (DaaS)
was coined in 2002. Over the past two decades, OPE has drawn tremendous
research interest from communities of databases, cryptography, and security; we
have witnessed significant advances in OPE schemes both theoretically and
systematically. All existing OPE schemes assume that the outsourced database is
modeled as a single semi-honest adversary who should learn nothing more than
the order information of plaintext messages up to a negligible probability.
This paper addresses the OPE problem from a new perspective: instead of
modeling the outsourced database as a single semi-honest adversary, we assume
the outsourced database \textit{service} compromises a cluster of non-colluding
servers, which is a practical assumption as all major cloud vendors support
multiple database instances deployed to exclusive sub-networks or even to
distinct data centers. This assumption allows us to design a new stateless OPE
protocol, namely order-preserving database encryption with secret sharing
(ODES), by employing secret-sharing schemes among those presumably
non-colluding servers. We will demonstrate that ODES guarantees the latest
security level, namely IND-FAOCPA, and outperforms the state-of-the-art scheme
by orders of magnitude.","Order-Preserving Database Encryption, Secret Sharing, IND-FAOCPA"
LLM As DBA,"Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
github.com/TsinghuaDatabaseGroup/DB-GPT.","Xuanhe Zhou, Guoliang Li, Zhiyuan Liu","LLM As DBA. Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
github.com/TsinghuaDatabaseGroup/DB-GPT.","LLM, D-Bot, Database Maintenance, Tree of Thought, Collaborative Diagnosis"
Hybrid Querying Over Relational Databases and Large Language Models,"Database queries traditionally operate under the closed-world assumption,
providing no answers to questions that require information beyond the data
stored in the database. Hybrid querying using SQL offers an alternative by
integrating relational databases with large language models (LLMs) to answer
beyond-database questions. In this paper, we present the first cross-domain
benchmark, SWAN, containing 120 beyond-database questions over four real-world
databases. To leverage state-of-the-art language models in addressing these
complex questions in SWAN, we present two solutions: one based on schema
expansion and the other based on user defined functions. We also discuss
optimization opportunities and potential future directions. Our evaluation
demonstrates that using GPT-4 Turbo with few-shot prompts, one can achieves up
to 40.0\% in execution accuracy and 48.2\% in data factuality. These results
highlights both the potential and challenges for hybrid querying. We believe
that our work will inspire further research in creating more efficient and
accurate data systems that seamlessly integrate relational databases and large
language models to address beyond-database questions.","Fuheng Zhao, Divyakant Agrawal, Amr El Abbadi","Hybrid Querying Over Relational Databases and Large Language Models. Database queries traditionally operate under the closed-world assumption,
providing no answers to questions that require information beyond the data
stored in the database. Hybrid querying using SQL offers an alternative by
integrating relational databases with large language models (LLMs) to answer
beyond-database questions. In this paper, we present the first cross-domain
benchmark, SWAN, containing 120 beyond-database questions over four real-world
databases. To leverage state-of-the-art language models in addressing these
complex questions in SWAN, we present two solutions: one based on schema
expansion and the other based on user defined functions. We also discuss
optimization opportunities and potential future directions. Our evaluation
demonstrates that using GPT-4 Turbo with few-shot prompts, one can achieves up
to 40.0\% in execution accuracy and 48.2\% in data factuality. These results
highlights both the potential and challenges for hybrid querying. We believe
that our work will inspire further research in creating more efficient and
accurate data systems that seamlessly integrate relational databases and large
language models to address beyond-database questions.","Hybrid Querying, Relational Databases, Large Language Models, GPT-4 Turbo, Scala"
"NeurDB: On the Design and Implementation of an AI-powered Autonomous
  Database","Databases are increasingly embracing AI to provide autonomous system
optimization and intelligent in-database analytics, aiming to relieve end-user
burdens across various industry sectors. Nonetheless, most existing approaches
fail to account for the dynamic nature of databases, which renders them
ineffective for real-world applications characterized by evolving data and
workloads. This paper introduces NeurDB, an AI-powered autonomous database that
deepens the fusion of AI and databases with adaptability to data and workload
drift. NeurDB establishes a new in-database AI ecosystem that seamlessly
integrates AI workflows within the database. This integration enables efficient
and effective in-database AI analytics and fast-adaptive learned system
components. Empirical evaluations demonstrate that NeurDB substantially
outperforms existing solutions in managing AI analytics tasks, with the
proposed learned components more effectively handling environmental dynamism
than state-of-the-art approaches.","Zhanhao Zhao, Shaofeng Cai, Haotian Gao, Hexiang Pan, Siqi Xiang, Naili Xing, Gang Chen, Beng Chin Ooi, Yanyan Shen, Yuncheng Wu, Meihui Zhang","NeurDB: On the Design and Implementation of an AI-powered Autonomous
  Database. Databases are increasingly embracing AI to provide autonomous system
optimization and intelligent in-database analytics, aiming to relieve end-user
burdens across various industry sectors. Nonetheless, most existing approaches
fail to account for the dynamic nature of databases, which renders them
ineffective for real-world applications characterized by evolving data and
workloads. This paper introduces NeurDB, an AI-powered autonomous database that
deepens the fusion of AI and databases with adaptability to data and workload
drift. NeurDB establishes a new in-database AI ecosystem that seamlessly
integrates AI workflows within the database. This integration enables efficient
and effective in-database AI analytics and fast-adaptive learned system
components. Empirical evaluations demonstrate that NeurDB substantially
outperforms existing solutions in managing AI analytics tasks, with the
proposed learned components more effectively handling environmental dynamism
than state-of-the-art approaches.","NeurDB, Autonomous Database, AI, Databases, In-Database Analytics, Environment Dynamism"
"On the effects of logical database design on database size, query
  complexity, query performance, and energy consumption","Database normalization theory is the basis for logical design of relational
databases. Normalization reduces data redundancy and consequently eliminates
potential data anomalies, while increasing the computational cost of read
operations. Despite decades worth of applications of normalization theory, it
still remains largely unclear to what extent normalization affects database
size and efficiency. In this study, we study the effects of database
normalization using the Internet Movie Database (IMDb) public dataset and
PostgreSQL. The results indicate, rather intuitively, that (i) database size on
disk is reduced through normalization from 1NF to 2NF by 10%, but not from 2NF
to 4NF, (ii) the number of tables and table rows in total increase
monotonically from 1NF to 2NF to 4NF, and that (iii) query complexity increases
with further normalization. Surprisingly, however, the results also indicate
that (iv) normalization from 1NF to 2NF increases throughput by a factor of 4,
and consequently, (v) energy consumption per transaction reduces by 74% with
normalization from 1NF to 2NF. The results imply that the gains of
normalization from 2NF to 4NF in terms of throughput and energy consumption are
minimal, yet increase the storage space requirements by approximately 7%. While
these results represent merely one specific case, they provide needed empirical
evaluation on the practical effects and magnitude of database normalization.",Toni Taipalus,"On the effects of logical database design on database size, query
  complexity, query performance, and energy consumption. Database normalization theory is the basis for logical design of relational
databases. Normalization reduces data redundancy and consequently eliminates
potential data anomalies, while increasing the computational cost of read
operations. Despite decades worth of applications of normalization theory, it
still remains largely unclear to what extent normalization affects database
size and efficiency. In this study, we study the effects of database
normalization using the Internet Movie Database (IMDb) public dataset and
PostgreSQL. The results indicate, rather intuitively, that (i) database size on
disk is reduced through normalization from 1NF to 2NF by 10%, but not from 2NF
to 4NF, (ii) the number of tables and table rows in total increase
monotonically from 1NF to 2NF to 4NF, and that (iii) query complexity increases
with further normalization. Surprisingly, however, the results also indicate
that (iv) normalization from 1NF to 2NF increases throughput by a factor of 4,
and consequently, (v) energy consumption per transaction reduces by 74% with
normalization from 1NF to 2NF. The results imply that the gains of
normalization from 2NF to 4NF in terms of throughput and energy consumption are
minimal, yet increase the storage space requirements by approximately 7%. While
these results represent merely one specific case, they provide needed empirical
evaluation on the practical effects and magnitude of database normalization.","Database Normalization, Query Performance, Energy Consumption, Internet Movie Database, IMDb, PostgreSQL"
Role of Databases in GenAI Applications,"Generative AI (GenAI) is transforming industries by enabling intelligent
content generation, automation, and decision-making. However, the effectiveness
of GenAI applications depends significantly on efficient data storage,
retrieval, and contextual augmentation. This paper explores the critical role
of databases in GenAI workflows, emphasizing the importance of choosing the
right database architecture to optimize performance, accuracy, and scalability.
It categorizes database roles into conversational context (key-value/document
databases), situational context (relational databases/data lakehouses), and
semantic context (vector databases) each serving a distinct function in
enriching AI-generated responses. Additionally, the paper highlights real-time
query processing, vector search for semantic retrieval, and the impact of
database selection on model efficiency and scalability. By leveraging a
multi-database approach, GenAI applications can achieve more context-aware,
personalized, and high-performing AI-driven solutions.",Santosh Bhupathi,"Role of Databases in GenAI Applications. Generative AI (GenAI) is transforming industries by enabling intelligent
content generation, automation, and decision-making. However, the effectiveness
of GenAI applications depends significantly on efficient data storage,
retrieval, and contextual augmentation. This paper explores the critical role
of databases in GenAI workflows, emphasizing the importance of choosing the
right database architecture to optimize performance, accuracy, and scalability.
It categorizes database roles into conversational context (key-value/document
databases), situational context (relational databases/data lakehouses), and
semantic context (vector databases) each serving a distinct function in
enriching AI-generated responses. Additionally, the paper highlights real-time
query processing, vector search for semantic retrieval, and the impact of
database selection on model efficiency and scalability. By leveraging a
multi-database approach, GenAI applications can achieve more context-aware,
personalized, and high-performing AI-driven solutions.","GenAI, Databases, Real-time Query Processing, Vector Search, Contextual Augmentation"
"Building Scalable AI-Powered Applications with Cloud Databases:
  Architectures, Best Practices and Performance Considerations","The rapid adoption of AI-powered applications demands high-performance,
scalable, and efficient cloud database solutions, as traditional architectures
often struggle with AI-driven workloads requiring real-time data access, vector
search, and low-latency queries. This paper explores how cloud-native databases
enable AI-driven applications by leveraging purpose-built technologies such as
vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores
(Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and
PostgreSQL). It presents architectural patterns for integrating AI workloads
with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with
LLMs, real-time data pipelines, AI-driven query optimization, and
embeddings-based search. Performance benchmarks, scalability considerations,
and cost-efficient strategies are evaluated to guide the design of AI-enabled
applications. Real-world case studies from industries such as healthcare,
finance, and customer experience illustrate how enterprises utilize cloud
databases to enhance AI capabilities while ensuring security, governance, and
compliance with enterprise and regulatory standards. By providing a
comprehensive analysis of AI and cloud database integration, this paper serves
as a practical guide for researchers, architects, and enterprises to build
next-generation AI applications that optimize performance, scalability, and
cost efficiency in cloud environments.",Santosh Bhupathi,"Building Scalable AI-Powered Applications with Cloud Databases:
  Architectures, Best Practices and Performance Considerations. The rapid adoption of AI-powered applications demands high-performance,
scalable, and efficient cloud database solutions, as traditional architectures
often struggle with AI-driven workloads requiring real-time data access, vector
search, and low-latency queries. This paper explores how cloud-native databases
enable AI-driven applications by leveraging purpose-built technologies such as
vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores
(Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and
PostgreSQL). It presents architectural patterns for integrating AI workloads
with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with
LLMs, real-time data pipelines, AI-driven query optimization, and
embeddings-based search. Performance benchmarks, scalability considerations,
and cost-efficient strategies are evaluated to guide the design of AI-enabled
applications. Real-world case studies from industries such as healthcare,
finance, and customer experience illustrate how enterprises utilize cloud
databases to enhance AI capabilities while ensuring security, governance, and
compliance with enterprise and regulatory standards. By providing a
comprehensive analysis of AI and cloud database integration, this paper serves
as a practical guide for researchers, architects, and enterprises to build
next-generation AI applications that optimize performance, scalability, and
cost efficiency in cloud environments.","AI-Powered Applications, Cloud Databases, Retrieval-Augmented Generation,RAG, Real-time Data Access, Vector Database, Graph Database, NoSQL"
"WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative
  Learning","Tabular data, ubiquitous and rich in informational value, is an increasing
focus for deep representation learning, yet progress is hindered by studies
centered on single tables or isolated databases, which limits model
capabilities due to data scale. While collaborative learning approaches such as
federated learning, transfer learning, split learning, and tabular foundation
models aim to learn from multiple correlated databases, they are challenged by
a scarcity of real-world interconnected tabular resources. Current data lakes
and corpora largely consist of isolated databases lacking defined
inter-database correlations. To overcome this, we introduce WikiDBGraph, a
large-scale graph of 100,000 real-world tabular databases from WikiData,
interconnected by 17 million edges and characterized by 13 node and 12 edge
properties derived from its database schema and data distribution.
WikiDBGraph's weighted edges identify both instance- and feature-overlapped
databases. Experiments on these newly identified databases confirm that
collaborative learning yields superior performance, thereby offering
considerable promise for structured foundation model training while also
exposing key challenges and future directions for learning from interconnected
tabular data.","Zhaomin Wu, Ziyang Wang, Bingsheng He","WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative
  Learning. Tabular data, ubiquitous and rich in informational value, is an increasing
focus for deep representation learning, yet progress is hindered by studies
centered on single tables or isolated databases, which limits model
capabilities due to data scale. While collaborative learning approaches such as
federated learning, transfer learning, split learning, and tabular foundation
models aim to learn from multiple correlated databases, they are challenged by
a scarcity of real-world interconnected tabular resources. Current data lakes
and corpora largely consist of isolated databases lacking defined
inter-database correlations. To overcome this, we introduce WikiDBGraph, a
large-scale graph of 100,000 real-world tabular databases from WikiData,
interconnected by 17 million edges and characterized by 13 node and 12 edge
properties derived from its database schema and data distribution.
WikiDBGraph's weighted edges identify both instance- and feature-overlapped
databases. Experiments on these newly identified databases confirm that
collaborative learning yields superior performance, thereby offering
considerable promise for structured foundation model training while also
exposing key challenges and future directions for learning from interconnected
tabular data.","WikiDBGraph, Large-Scale Database Graph, Wikidata, Collaborative Learning, Deep Representation Learning"
MINE GOLD to Deliver Green Cognitive Communications,"Geo-location database-assisted TV white space network reduces the need of
energy-intensive processes (such as spectrum sensing), hence can achieve green
cognitive communication effectively. The success of such a network relies on a
proper business model that provides incentives for all parties involved. In
this paper, we propose MINE GOLD (a Model of INformation markEt for
GeO-Location Database), which enables databases to sell the spectrum
information to unlicensed white space devices (WSDs) for profit. Specifically,
we focus on an oligopoly information market with multiple databases, and study
the interactions among databases and WSDs using a two-stage hierarchical model.
In Stage I, databases compete to sell information to WSDs by optimizing their
information prices. In Stage II, each WSD decides whether and from which
database to purchase the information, to maximize his benefit of using the TV
white space. We first characterize how the WSDs' purchasing behaviors
dynamically evolve, and what is the equilibrium point under fixed information
prices from the databases. We then analyze how the system parameters and the
databases' pricing decisions affect the market equilibrium, and what is the
equilibrium of the database price competition. Our numerical results show that,
perhaps counter-intuitively, the databases' aggregate revenue is not monotonic
with the number of databases. Moreover, numerical results show that a large
degree of positive network externality would improve the databases' revenues
and the system performance.","Yuan Luo, Lin Gao, Jianwei Huang","MINE GOLD to Deliver Green Cognitive Communications. Geo-location database-assisted TV white space network reduces the need of
energy-intensive processes (such as spectrum sensing), hence can achieve green
cognitive communication effectively. The success of such a network relies on a
proper business model that provides incentives for all parties involved. In
this paper, we propose MINE GOLD (a Model of INformation markEt for
GeO-Location Database), which enables databases to sell the spectrum
information to unlicensed white space devices (WSDs) for profit. Specifically,
we focus on an oligopoly information market with multiple databases, and study
the interactions among databases and WSDs using a two-stage hierarchical model.
In Stage I, databases compete to sell information to WSDs by optimizing their
information prices. In Stage II, each WSD decides whether and from which
database to purchase the information, to maximize his benefit of using the TV
white space. We first characterize how the WSDs' purchasing behaviors
dynamically evolve, and what is the equilibrium point under fixed information
prices from the databases. We then analyze how the system parameters and the
databases' pricing decisions affect the market equilibrium, and what is the
equilibrium of the database price competition. Our numerical results show that,
perhaps counter-intuitively, the databases' aggregate revenue is not monotonic
with the number of databases. Moreover, numerical results show that a large
degree of positive network externality would improve the databases' revenues
and the system performance.","MINE GOLD, Green Cognitive Communications, Geo-location Database, TV White Space Network, Spectrum Sensing"
Database Engines: Evolution of Greenness,"Context: Information Technology consumes up to 10\% of the world's
electricity generation, contributing to CO2 emissions and high energy costs.
Data centers, particularly databases, use up to 23% of this energy. Therefore,
building an energy-efficient (green) database engine could reduce energy
consumption and CO2 emissions.
  Goal: To understand the factors driving databases' energy consumption and
execution time throughout their evolution.
  Method: We conducted an empirical case study of energy consumption by two
MySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the
relationships of four software metrics to energy consumption and execution time
to determine which metrics reflect the greenness and performance of a database.
  Results: Our analysis shows that database engines' energy consumption and
execution time increase as databases evolve. Moreover, the Lines of Code metric
is correlated moderately to strongly with energy consumption and execution time
in 88% of cases.
  Conclusions: Our findings provide insights to both practitioners and
researchers. Database administrators may use them to select a fast, green
release of the MySQL database engine. MySQL database-engine developers may use
the software metric to assess products' greenness and performance. Researchers
may use our findings to further develop new hypotheses or build models to
predict greenness and performance of databases.","Andriy V. Miranskyy, Zainab Al-zanbouri, David Godwin, Ayse Basar Bener","Database Engines: Evolution of Greenness. Context: Information Technology consumes up to 10\% of the world's
electricity generation, contributing to CO2 emissions and high energy costs.
Data centers, particularly databases, use up to 23% of this energy. Therefore,
building an energy-efficient (green) database engine could reduce energy
consumption and CO2 emissions.
  Goal: To understand the factors driving databases' energy consumption and
execution time throughout their evolution.
  Method: We conducted an empirical case study of energy consumption by two
MySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the
relationships of four software metrics to energy consumption and execution time
to determine which metrics reflect the greenness and performance of a database.
  Results: Our analysis shows that database engines' energy consumption and
execution time increase as databases evolve. Moreover, the Lines of Code metric
is correlated moderately to strongly with energy consumption and execution time
in 88% of cases.
  Conclusions: Our findings provide insights to both practitioners and
researchers. Database administrators may use them to select a fast, green
release of the MySQL database engine. MySQL database-engine developers may use
the software metric to assess products' greenness and performance. Researchers
may use our findings to further develop new hypotheses or build models to
predict greenness and performance of databases.","Database Engines, Greenness, InnoDB, MyISAM, Lines of Code"
"Mining Top-k Sequential Patterns in Database Graphs:A New Challenging
  Problem and a Sampling-based Approach","In many real world networks, a vertex is usually associated with a
transaction database that comprehensively describes the behaviour of the
vertex. A typical example is the social network, where the behaviour of every
user is depicted by a transaction database that stores his daily posted
contents. A transaction database is a set of transactions, where a transaction
is a set of items. Every path of the network is a sequence of vertices that
induces multiple sequences of transactions. The sequences of transactions
induced by all of the paths in the network forms an extremely large sequence
database. Finding frequent sequential patterns from such sequence database
discovers interesting subsequences that frequently appear in many paths of the
network. However, it is a challenging task, since the sequence database induced
by a database graph is too large to be explicitly induced and stored. In this
paper, we propose the novel notion of database graph, which naturally models a
wide spectrum of real world networks by associating each vertex with a
transaction database. Our goal is to find the top-k frequent sequential
patterns in the sequence database induced from a database graph. We prove that
this problem is #P-hard. To tackle this problem, we propose an efficient
two-step sampling algorithm that approximates the top-k frequent sequential
patterns with provable quality guarantee. Extensive experimental results on
synthetic and real-world data sets demonstrate the effectiveness and efficiency
of our method.","Mingtao Lei, Lingyang Chu, Zhefeng Wang","Mining Top-k Sequential Patterns in Database Graphs:A New Challenging
  Problem and a Sampling-based Approach. In many real world networks, a vertex is usually associated with a
transaction database that comprehensively describes the behaviour of the
vertex. A typical example is the social network, where the behaviour of every
user is depicted by a transaction database that stores his daily posted
contents. A transaction database is a set of transactions, where a transaction
is a set of items. Every path of the network is a sequence of vertices that
induces multiple sequences of transactions. The sequences of transactions
induced by all of the paths in the network forms an extremely large sequence
database. Finding frequent sequential patterns from such sequence database
discovers interesting subsequences that frequently appear in many paths of the
network. However, it is a challenging task, since the sequence database induced
by a database graph is too large to be explicitly induced and stored. In this
paper, we propose the novel notion of database graph, which naturally models a
wide spectrum of real world networks by associating each vertex with a
transaction database. Our goal is to find the top-k frequent sequential
patterns in the sequence database induced from a database graph. We prove that
this problem is #P-hard. To tackle this problem, we propose an efficient
two-step sampling algorithm that approximates the top-k frequent sequential
patterns with provable quality guarantee. Extensive experimental results on
synthetic and real-world data sets demonstrate the effectiveness and efficiency
of our method.","Top-k Sequential Patterns, Database Graphs, Sampling Algorithm, Transactions Database"
"On-line database of the spectral properties of polycyclic aromatic
  hydrocarbons","We present an on-line database of computed molecular properties for a large
sample of polycyclic aromatic hydrocarbons (PAHs) in four charge states: -1, 0,
+1, and +2. At present our database includes 40 molecules ranging in size from
naphthalene and azulene (C10H8) up to circumovalene (C66H20). We performed our
calculations in the framework of the density functional theory (DFT) and the
time-dependent DFT to obtain the most relevant molecular parameters needed for
astrophysical applications. For each molecule in the sample, our database
presents in a uniform way the energetic, rotational, vibrational, and
electronic properties. It is freely accessible on the web at
http://astrochemistry.ca.astro.it/database/ and
http://www.cesr.fr/~joblin/database/.","G. Malloci, C. Joblin, G. Mulas","On-line database of the spectral properties of polycyclic aromatic
  hydrocarbons. We present an on-line database of computed molecular properties for a large
sample of polycyclic aromatic hydrocarbons (PAHs) in four charge states: -1, 0,
+1, and +2. At present our database includes 40 molecules ranging in size from
naphthalene and azulene (C10H8) up to circumovalene (C66H20). We performed our
calculations in the framework of the density functional theory (DFT) and the
time-dependent DFT to obtain the most relevant molecular parameters needed for
astrophysical applications. For each molecule in the sample, our database
presents in a uniform way the energetic, rotational, vibrational, and
electronic properties. It is freely accessible on the web at
http://astrochemistry.ca.astro.it/database/ and
http://www.cesr.fr/~joblin/database/.","Spectral Properties, Polycyclic Aromatic Hydrocarbons, DFT, Joblin Database"
The Algorithms of Updating Sequential Patterns,"Because the data being mined in the temporal database will evolve with time,
many researchers have focused on the incremental mining of frequent sequences
in temporal database. In this paper, we propose an algorithm called IUS, using
the frequent and negative border sequences in the original database for
incremental sequence mining. To deal with the case where some data need to be
updated from the original database, we present an algorithm called DUS to
maintain sequential patterns in the updated database. We also define the
negative border sequence threshold: Min_nbd_supp to control the number of
sequences in the negative border.","Qingguo Zheng, Ke Xu, Shilong Ma, Weifeng Lv","The Algorithms of Updating Sequential Patterns. Because the data being mined in the temporal database will evolve with time,
many researchers have focused on the incremental mining of frequent sequences
in temporal database. In this paper, we propose an algorithm called IUS, using
the frequent and negative border sequences in the original database for
incremental sequence mining. To deal with the case where some data need to be
updated from the original database, we present an algorithm called DUS to
maintain sequential patterns in the updated database. We also define the
negative border sequence threshold: Min_nbd_supp to control the number of
sequences in the negative border.","Sequential Patterns, IUS, DUS, Incremental Sequence Mining, Temporal Database"
Database Repairs and Analytic Tableaux,"In this article, we characterize in terms of analytic tableaux the repairs of
inconsistent relational databases, that is databases that do not satisfy a
given set of integrity constraints. For this purpose we provide closing and
opening criteria for branches in tableaux that are built for database instances
and their integrity constraints. We use the tableaux based characterization as
a basis for consistent query answering, that is for retrieving from the
database answers to queries that are consistent wrt the integrity constraints.","Leopoldo Bertossi, Camilla Schwind","Database Repairs and Analytic Tableaux. In this article, we characterize in terms of analytic tableaux the repairs of
inconsistent relational databases, that is databases that do not satisfy a
given set of integrity constraints. For this purpose we provide closing and
opening criteria for branches in tableaux that are built for database instances
and their integrity constraints. We use the tableaux based characterization as
a basis for consistent query answering, that is for retrieving from the
database answers to queries that are consistent wrt the integrity constraints.","Database Repairs, Analytic Tableaux, Relational Databases"
"Supporting Finite Element Analysis with a Relational Database Backend,
  Part II: Database Design and Access","This is Part II of a three article series on using databases for Finite
Element Analysis (FEA). It discusses (1) db design, (2) data loading, (3)
typical use cases during grid building, (4) typical use cases during simulation
(get and put), (5) typical use cases during analysis (also done in Part III)
and some performance measures of these cases. It argues that using a database
is simpler to implement than custom data schemas, has better performance
because it can use data parallelism, and better supports FEA modularity and
tool evolution because database schema evolution, data independence, and
self-defining data.","Gerd Heber, Jim Gray","Supporting Finite Element Analysis with a Relational Database Backend,
  Part II: Database Design and Access. This is Part II of a three article series on using databases for Finite
Element Analysis (FEA). It discusses (1) db design, (2) data loading, (3)
typical use cases during grid building, (4) typical use cases during simulation
(get and put), (5) typical use cases during analysis (also done in Part III)
and some performance measures of these cases. It argues that using a database
is simpler to implement than custom data schemas, has better performance
because it can use data parallelism, and better supports FEA modularity and
tool evolution because database schema evolution, data independence, and
self-defining data.","Finite Element Analysis, FEA, Relational Database, Database Design, Data Parallelism"
Performance Comparison of Persistence Frameworks,"One of the essential and most complex components in the software development
process is the database. The complexity increases when the ""orientation"" of the
interacting components differs. A persistence framework moves the program data
in its most natural form to and from a permanent data store, the database. Thus
a persistence framework manages the database and the mapping between the
database and the objects. This paper compares the performance of two
persistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking
database. The performance of both of these tools in single and multi-user
environments are evaluated.","Sabu M. Thampi, Ashwin a K","Performance Comparison of Persistence Frameworks. One of the essential and most complex components in the software development
process is the database. The complexity increases when the ""orientation"" of the
interacting components differs. A persistence framework moves the program data
in its most natural form to and from a permanent data store, the database. Thus
a persistence framework manages the database and the mapping between the
database and the objects. This paper compares the performance of two
persistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking
database. The performance of both of these tools in single and multi-user
environments are evaluated.","Persistence Frameworks, Hibernate, iBatis, SQLMaps, Database"
Database Reverse Engineering based on Association Rule Mining,"Maintaining a legacy database is a difficult task especially when system
documentation is poor written or even missing. Database reverse engineering is
an attempt to recover high-level conceptual design from the existing database
instances. In this paper, we propose a technique to discover conceptual schema
using the association mining technique. The discovered schema corresponds to
the normalization at the third normal form, which is a common practice in many
business organizations. Our algorithm also includes the rule filtering
heuristic to solve the problem of exponential growth of discovered rules
inherited with the association mining technique.","Nattapon Pannurat, Nittaya Kerdprasop, Kittisak Kerdprasop","Database Reverse Engineering based on Association Rule Mining. Maintaining a legacy database is a difficult task especially when system
documentation is poor written or even missing. Database reverse engineering is
an attempt to recover high-level conceptual design from the existing database
instances. In this paper, we propose a technique to discover conceptual schema
using the association mining technique. The discovered schema corresponds to
the normalization at the third normal form, which is a common practice in many
business organizations. Our algorithm also includes the rule filtering
heuristic to solve the problem of exponential growth of discovered rules
inherited with the association mining technique.","Database Reverse Engineering, Association Rule Mining, Rule Filtering"
"A Novel Watermarking Scheme for Detecting and Recovering Distortions in
  Database Tables","In this paper a novel fragile watermarking scheme is proposed to detect,
localize and recover malicious modifications in relational databases. In the
proposed scheme, all tuples in the database are first securely divided into
groups. Then watermarks are embedded and verified group-by-group independently.
By using the embedded watermark, we are able to detect and localize the
modification made to the database and even we recover the true data from the
database modified locations. Our experimental results show that this scheme is
so qualified; i.e. distortion detection and true data recovery both are
performed successfully.","Hamed khataeimaragheh, Hassan Rashidi","A Novel Watermarking Scheme for Detecting and Recovering Distortions in
  Database Tables. In this paper a novel fragile watermarking scheme is proposed to detect,
localize and recover malicious modifications in relational databases. In the
proposed scheme, all tuples in the database are first securely divided into
groups. Then watermarks are embedded and verified group-by-group independently.
By using the embedded watermark, we are able to detect and localize the
modification made to the database and even we recover the true data from the
database modified locations. Our experimental results show that this scheme is
so qualified; i.e. distortion detection and true data recovery both are
performed successfully.","Watermarking, Distortion Detection, Data Recovery, Relational Databases"
A Decade of Database Research Publications,"We analyze the database research publications of four major core database
technology conferences (SIGMOD, VLDB, ICDE, EDBT), two main theoretical
database conferences (PODS, ICDT) and three database journals (TODS, VLDB
Journal, TKDE) over a period of 10 years (2001 - 2010). Our analysis considers
only regular papers as we do not include short papers, demo papers, posters,
tutorials or panels into our statistics. We rank the research scholars
according to their number of publication in each conference/journal separately
and in combined. We also report about the growth in the number of research
publications and the size of the research community in the last decade.","Sherif Sakr, Mohammad Alomari","A Decade of Database Research Publications. We analyze the database research publications of four major core database
technology conferences (SIGMOD, VLDB, ICDE, EDBT), two main theoretical
database conferences (PODS, ICDT) and three database journals (TODS, VLDB
Journal, TKDE) over a period of 10 years (2001 - 2010). Our analysis considers
only regular papers as we do not include short papers, demo papers, posters,
tutorials or panels into our statistics. We rank the research scholars
according to their number of publication in each conference/journal separately
and in combined. We also report about the growth in the number of research
publications and the size of the research community in the last decade.","Database Research, SIGMOD, VLDB, ICDE, EDBT, PODS, ICDT"
Difference Sequence Compression of Multidimensional Databases,"The multidimensional databases often use compression techniques in order to
decrease the size of the database. This paper introduces a new method called
difference sequence compression. Under some conditions, this new technique is
able to create a smaller size multidimensional database than others like single
count header compression, logical position compression or base-offset
compression. Keywords: compression, multidimensional database, On-line
Analytical Processing, OLAP.",István Szépkúti,"Difference Sequence Compression of Multidimensional Databases. The multidimensional databases often use compression techniques in order to
decrease the size of the database. This paper introduces a new method called
difference sequence compression. Under some conditions, this new technique is
able to create a smaller size multidimensional database than others like single
count header compression, logical position compression or base-offset
compression. Keywords: compression, multidimensional database, On-line
Analytical Processing, OLAP.","Difference Sequence Compression, Multidimensional Databases, On-lineAnalytical Processing, OLAP"
Data Base Mappings and Theory of Sketches,"In this paper we will present the two basic operations for database schemas
used in database mapping systems (separation and Data Federation), and we will
explain why the functorial semantics for database mappings needed a new base
category instead of usual Set category. Successively, it is presented a
definition of the graph G for a schema database mapping system, and the
definition of its sketch category Sch(G). Based on this framework we presented
functorial semantics for database mapping systems with the new base category
DB.",Zoran Majkic,"Data Base Mappings and Theory of Sketches. In this paper we will present the two basic operations for database schemas
used in database mapping systems (separation and Data Federation), and we will
explain why the functorial semantics for database mappings needed a new base
category instead of usual Set category. Successively, it is presented a
definition of the graph G for a schema database mapping system, and the
definition of its sketch category Sch(G). Based on this framework we presented
functorial semantics for database mapping systems with the new base category
DB.","Data Base Mappings, Data Federation, Graph G, Sketch Category"
Performance of Short-Commit in Extreme Database Environment,"Atomic commit protocols are used where data integrity is more important than
data availability. Two-Phase commit (2PC) is a standard commit protocol for
commercial database management systems. To reduce certain drawbacks in 2PC
protocol people have suggested different variance of this protocol.
Short-Commit protocol is developed with an objective to achieve low cost
transaction commitment cost with non-blocking capability. In this paper we have
briefly explained short-commit protocol executing pattern. Experimental
analysis and results are presented to support the claim that short-commit can
work efficiently in extreme database environment.","Muhammad Tayyab Shahzad, Muhammad Rizwan","Performance of Short-Commit in Extreme Database Environment. Atomic commit protocols are used where data integrity is more important than
data availability. Two-Phase commit (2PC) is a standard commit protocol for
commercial database management systems. To reduce certain drawbacks in 2PC
protocol people have suggested different variance of this protocol.
Short-Commit protocol is developed with an objective to achieve low cost
transaction commitment cost with non-blocking capability. In this paper we have
briefly explained short-commit protocol executing pattern. Experimental
analysis and results are presented to support the claim that short-commit can
work efficiently in extreme database environment.","Short-Commit, Extreme Database Environment, Atomic Commit Protocol, 2PC"
"Proposing Cluster_Similarity Method in Order to Find as Much Better
  Similarities in Databases","Different ways of entering data into databases result in duplicate records
that cause increasing of databases' size. This is a fact that we cannot ignore
it easily. There are several methods that are used for this purpose. In this
paper, we have tried to increase the accuracy of operations by using cluster
similarity instead of direct similarity of fields. So that clustering is done
on fields of database and according to accomplished clustering on fields,
similarity degree of records is obtained. In this method by using present
information in database, more logical similarity is obtained for deficient
information that in general, the method of cluster similarity could improve
operations 24% compared with previous methods.","Mohammad-Reza Feizi-Derakhshi, Azade Roohany","Proposing Cluster_Similarity Method in Order to Find as Much Better
  Similarities in Databases. Different ways of entering data into databases result in duplicate records
that cause increasing of databases' size. This is a fact that we cannot ignore
it easily. There are several methods that are used for this purpose. In this
paper, we have tried to increase the accuracy of operations by using cluster
similarity instead of direct similarity of fields. So that clustering is done
on fields of database and according to accomplished clustering on fields,
similarity degree of records is obtained. In this method by using present
information in database, more logical similarity is obtained for deficient
information that in general, the method of cluster similarity could improve
operations 24% compared with previous methods.","Cluster_Similarity Method, Databases, Clustering"
Kleisli Database Instances,"We use monads to relax the atomicity requirement for data in a database.
Depending on the choice of monad, the database fields may contain generalized
values such as lists or sets of values, or they may contain exceptions such as
various types of nulls. The return operation for monads ensures that any
ordinary database instance will count as one of these generalized instances,
and the bind operation ensures that generalized values behave well under joins
of foreign key sequences. Different monads allow for vastly different types of
information to be stored in the database. For example, we show that classical
concepts like Markov chains, graphs, and finite state automata are each
perfectly captured by a different monad on the same schema.",David I. Spivak,"Kleisli Database Instances. We use monads to relax the atomicity requirement for data in a database.
Depending on the choice of monad, the database fields may contain generalized
values such as lists or sets of values, or they may contain exceptions such as
various types of nulls. The return operation for monads ensures that any
ordinary database instance will count as one of these generalized instances,
and the bind operation ensures that generalized values behave well under joins
of foreign key sequences. Different monads allow for vastly different types of
information to be stored in the database. For example, we show that classical
concepts like Markov chains, graphs, and finite state automata are each
perfectly captured by a different monad on the same schema.","Kleisli Database, Monads, Markov Chains, Graphs, Finite State Automata"
A Logical Formalization of a Secure XML Database,"In this paper, we first define a logical theory representing an XML database
supporting XPath as query language and XUpdate as modification language. We
then extend our theory with predicates allowing us to specify the security
policy protecting the database. The security policy includes rules addressing
the read and write privileges. We propose axioms to derive the database view
each user is permitted to see. We also propose axioms to derive the new
database content after an update.",Alban Gabillon,"A Logical Formalization of a Secure XML Database. In this paper, we first define a logical theory representing an XML database
supporting XPath as query language and XUpdate as modification language. We
then extend our theory with predicates allowing us to specify the security
policy protecting the database. The security policy includes rules addressing
the read and write privileges. We propose axioms to derive the database view
each user is permitted to see. We also propose axioms to derive the new
database content after an update.","XML Database, Logical Formalization, XPath, XUpdate, Security Policy"
Discover Aggregates Exceptions over Hidden Web Databases,"Nowadays, many web databases ""hidden"" behind their restrictive search
interfaces (e.g., Amazon, eBay) contain rich and valuable information that is
of significant interests to various third parties. Recent studies have
demonstrated the possibility of estimating/tracking certain aggregate queries
over dynamic hidden web databases. Nonetheless, tracking all possible aggregate
query answers to report interesting findings (i.e., exceptions), while still
adhering to the stringent query-count limitations enforced by many hidden web
databases providers, is very challenging. In this paper, we develop a novel
technique for tracking and discovering exceptions (in terms of sudden changes
of aggregates) over dynamic hidden web databases. Extensive real-world
experiments demonstrate the superiority of our proposed algorithms over
baseline solutions.","Saad Bin Suhaim, Weimo Liu, Nan Zhang","Discover Aggregates Exceptions over Hidden Web Databases. Nowadays, many web databases ""hidden"" behind their restrictive search
interfaces (e.g., Amazon, eBay) contain rich and valuable information that is
of significant interests to various third parties. Recent studies have
demonstrated the possibility of estimating/tracking certain aggregate queries
over dynamic hidden web databases. Nonetheless, tracking all possible aggregate
query answers to report interesting findings (i.e., exceptions), while still
adhering to the stringent query-count limitations enforced by many hidden web
databases providers, is very challenging. In this paper, we develop a novel
technique for tracking and discovering exceptions (in terms of sudden changes
of aggregates) over dynamic hidden web databases. Extensive real-world
experiments demonstrate the superiority of our proposed algorithms over
baseline solutions.","Aggregates, Exceptions, Hidden Web Databases, Algorithms"
"Speaker Identification in the Shouted Environment Using Suprasegmental
  Hidden Markov Models","In this paper, Suprasegmental Hidden Markov Models (SPHMMs) have been used to
enhance the recognition performance of text-dependent speaker identification in
the shouted environment. Our speech database consists of two databases: our
collected database and the Speech Under Simulated and Actual Stress (SUSAS)
database. Our results show that SPHMMs significantly enhance speaker
identification performance compared to Second-Order Circular Hidden Markov
Models (CHMM2s) in the shouted environment. Using our collected database,
speaker identification performance in this environment is 68% and 75% based on
CHMM2s and SPHMMs respectively. Using the SUSAS database, speaker
identification performance in the same environment is 71% and 79% based on
CHMM2s and SPHMMs respectively.",Ismail Shahin,"Speaker Identification in the Shouted Environment Using Suprasegmental
  Hidden Markov Models. In this paper, Suprasegmental Hidden Markov Models (SPHMMs) have been used to
enhance the recognition performance of text-dependent speaker identification in
the shouted environment. Our speech database consists of two databases: our
collected database and the Speech Under Simulated and Actual Stress (SUSAS)
database. Our results show that SPHMMs significantly enhance speaker
identification performance compared to Second-Order Circular Hidden Markov
Models (CHMM2s) in the shouted environment. Using our collected database,
speaker identification performance in this environment is 68% and 75% based on
CHMM2s and SPHMMs respectively. Using the SUSAS database, speaker
identification performance in the same environment is 71% and 79% based on
CHMM2s and SPHMMs respectively.","Speaker Identification, Shouted Environment, Suprasegmental Hidden Markov Models, CHMM2s, SUSAS"
Measuring and Computing Database Inconsistency via Repairs,"We propose a generic numerical measure of inconsistency of a database with
respect to a set of integrity constraints. It is based on an abstract repair
semantics. A particular inconsistency measure associated to cardinality-repairs
is investigated; and we show that it can be computed via answer-set programs.
  Keywords: Integrity constraints in databases, inconsistent databases,
database repairs, inconsistency measure.",Leopoldo Bertossi,"Measuring and Computing Database Inconsistency via Repairs. We propose a generic numerical measure of inconsistency of a database with
respect to a set of integrity constraints. It is based on an abstract repair
semantics. A particular inconsistency measure associated to cardinality-repairs
is investigated; and we show that it can be computed via answer-set programs.
  Keywords: Integrity constraints in databases, inconsistent databases,
database repairs, inconsistency measure.","Database Inconsistency, Integrity Constraints, Database Repairs"
QR2: A Third-party Query Reranking Service Over Web Databases,"The ranked retrieval model has rapidly become the de-facto way for search
query processing in web databases. Despite the extensive efforts on designing
better ranking mechanisms, in practice, many such databases fail to address the
diverse and sometimes contradicting preferences of users. In this paper, we
present QR2, a third-party service that uses nothing but the public search
interface of a web database and enables the on-the-fly processing of queries
with any user-specified ranking functions, no matter if the ranking function is
supported by the database or not.","Yeshwanth D. Gunasekaran, Abolfazl Asudeh, Sona Hasani, Nan Zhang, Ali Jaoua, Gautam Das","QR2: A Third-party Query Reranking Service Over Web Databases. The ranked retrieval model has rapidly become the de-facto way for search
query processing in web databases. Despite the extensive efforts on designing
better ranking mechanisms, in practice, many such databases fail to address the
diverse and sometimes contradicting preferences of users. In this paper, we
present QR2, a third-party service that uses nothing but the public search
interface of a web database and enables the on-the-fly processing of queries
with any user-specified ranking functions, no matter if the ranking function is
supported by the database or not.","QR2, Query Reranking, Web Databases"
Database Operations in D4M.jl,"Each step in the data analytics pipeline is important, including database
ingest and query. The D4M-Accumulo database connector has allowed analysts to
quickly and easily ingest to and query from Apache Accumulo using MATLAB(R)/GNU
Octave syntax. D4M.jl, a Julia implementation of D4M, provides much of the
functionality of the original D4M implementation to the Julia community. In
this work, we extend D4M.jl to include many of the same database capabilities
that the MATLAB(R)/GNU Octave implementation provides. Here we will describe
the D4M.jl database connector, demonstrate how it can be used, and show that it
has comparable or better performance to the original implementation in
MATLAB(R)/GNU Octave.","Lauren Milechin, Vijay Gadepally, Jeremy Kepner","Database Operations in D4M.jl. Each step in the data analytics pipeline is important, including database
ingest and query. The D4M-Accumulo database connector has allowed analysts to
quickly and easily ingest to and query from Apache Accumulo using MATLAB(R)/GNU
Octave syntax. D4M.jl, a Julia implementation of D4M, provides much of the
functionality of the original D4M implementation to the Julia community. In
this work, we extend D4M.jl to include many of the same database capabilities
that the MATLAB(R)/GNU Octave implementation provides. Here we will describe
the D4M.jl database connector, demonstrate how it can be used, and show that it
has comparable or better performance to the original implementation in
MATLAB(R)/GNU Octave.","Database Operations, D4M.jl, Julia, Apache Accumulo, MATLAB(R)/GNU Octave"
Weighing the techniques for data optimization in a database,"A set of preferred records can be obtained from a large database in a
multi-criteria setting using various computational methods which either depend
on the concept of dominance or on the concept of utility or scoring function
based on the attributes of the database record. A skyline approach relies on
the dominance relationship between different data points to discover
interesting data from a huge database. On the other hand, ranking queries make
use of specific scoring functions to rank tuples in a database. An experimental
evaluation of datasets can provides us with information on the effectiveness of
each of these methods.",Anagha Radhakrishnan,"Weighing the techniques for data optimization in a database. A set of preferred records can be obtained from a large database in a
multi-criteria setting using various computational methods which either depend
on the concept of dominance or on the concept of utility or scoring function
based on the attributes of the database record. A skyline approach relies on
the dominance relationship between different data points to discover
interesting data from a huge database. On the other hand, ranking queries make
use of specific scoring functions to rank tuples in a database. An experimental
evaluation of datasets can provides us with information on the effectiveness of
each of these methods.","Data Optimization, Database"
Facilitating Digital Agriculture with Simple Databases,"As an on-ramp to databases, we offer several well-structured private database
templates as open source resources for agriculturalists, particularly those
with modest spreadsheet skills. These farmer-oriented Air table databases use
simple data-validated forms, with the look and feel of a customized app, to
yield operational data that is tidy, machine- and human-readable, editable, and
exportable for analysis in other software. Such data can facilitate logistics,
provide contextual metadata, and improve enterprise analysis. A recorded
workshop explaining how to build a database for activity records is presented.
These resources may facilitate infusion of digital agriculture principles
through Extension and structured educational programming.","Dennis Buckmaster, Sami Basir, Hanae Sakata","Facilitating Digital Agriculture with Simple Databases. As an on-ramp to databases, we offer several well-structured private database
templates as open source resources for agriculturalists, particularly those
with modest spreadsheet skills. These farmer-oriented Air table databases use
simple data-validated forms, with the look and feel of a customized app, to
yield operational data that is tidy, machine- and human-readable, editable, and
exportable for analysis in other software. Such data can facilitate logistics,
provide contextual metadata, and improve enterprise analysis. A recorded
workshop explaining how to build a database for activity records is presented.
These resources may facilitate infusion of digital agriculture principles
through Extension and structured educational programming.","Digital Agriculture, Databases, Air Table, Extension, Enterprise Analysis"
ONDA: ONline Database Architect,"Database modeling is a key activity towards the fulfillment of storage
requirements. Despite the availability of several database modeling tools for
developers, these often come with associated costs, setup complexities,
usability challenges, or dependency on specific operating systems. In this
paper we present ONDA, a web-based tool developed at the University of Coimbra,
that allows the creation of Entity-Relationship diagrams, visualization of
physical models, and generation of SQL code for various database engines. ONDA
is freely available at https://onda.dei.uc.pt and was created with the
intention of supporting teaching activities at university-level database
courses. At the time of writing, the tool being used by more than three hundred
university students every academic year.","Nuno Laranjeiro, Alexandre Miguel Pinto","ONDA: ONline Database Architect. Database modeling is a key activity towards the fulfillment of storage
requirements. Despite the availability of several database modeling tools for
developers, these often come with associated costs, setup complexities,
usability challenges, or dependency on specific operating systems. In this
paper we present ONDA, a web-based tool developed at the University of Coimbra,
that allows the creation of Entity-Relationship diagrams, visualization of
physical models, and generation of SQL code for various database engines. ONDA
is freely available at https://onda.dei.uc.pt and was created with the
intention of supporting teaching activities at university-level database
courses. At the time of writing, the tool being used by more than three hundred
university students every academic year.","ONDA, Database Architect, University of Coimbra, Entity-Relationship Diagrams, Physical Models, SQL"
A Modification of Grover's Algorithm as a Fast Database Search,"A modification of Grover's algorithm is proposed, which can be used directly
as a fast database search. An explicit two q-bit example is displayed in
detail. We discuss the case where the database has multiple entries
corresponding to the same target value.",D. A. Ross,"A Modification of Grover's Algorithm as a Fast Database Search. A modification of Grover's algorithm is proposed, which can be used directly
as a fast database search. An explicit two q-bit example is displayed in
detail. We discuss the case where the database has multiple entries
corresponding to the same target value.","Grover's Algorithm, Database Search"
Believe It or Not: Adding Belief Annotations to Databases,"We propose a database model that allows users to annotate data with belief
statements. Our motivation comes from scientific database applications where a
community of users is working together to assemble, revise, and curate a shared
data repository. As the community accumulates knowledge and the database
content evolves over time, it may contain conflicting information and members
can disagree on the information it should store. For example, Alice may believe
that a tuple should be in the database, whereas Bob disagrees. He may also
insert the reason why he thinks Alice believes the tuple should be in the
database, and explain what he thinks the correct tuple should be instead.
  We propose a formal model for Belief Databases that interprets users'
annotations as belief statements. These annotations can refer both to the base
data and to other annotations. We give a formal semantics based on a fragment
of multi-agent epistemic logic and define a query language over belief
databases. We then prove a key technical result, stating that every belief
database can be encoded as a canonical Kripke structure. We use this structure
to describe a relational representation of belief databases, and give an
algorithm for translating queries over the belief database into standard
relational queries. Finally, we report early experimental results with our
prototype implementation on synthetic data.","Wolfgang Gatterbauer, Magdalena Balazinska, Nodira Khoussainova, Dan Suciu","Believe It or Not: Adding Belief Annotations to Databases. We propose a database model that allows users to annotate data with belief
statements. Our motivation comes from scientific database applications where a
community of users is working together to assemble, revise, and curate a shared
data repository. As the community accumulates knowledge and the database
content evolves over time, it may contain conflicting information and members
can disagree on the information it should store. For example, Alice may believe
that a tuple should be in the database, whereas Bob disagrees. He may also
insert the reason why he thinks Alice believes the tuple should be in the
database, and explain what he thinks the correct tuple should be instead.
  We propose a formal model for Belief Databases that interprets users'
annotations as belief statements. These annotations can refer both to the base
data and to other annotations. We give a formal semantics based on a fragment
of multi-agent epistemic logic and define a query language over belief
databases. We then prove a key technical result, stating that every belief
database can be encoded as a canonical Kripke structure. We use this structure
to describe a relational representation of belief databases, and give an
algorithm for translating queries over the belief database into standard
relational queries. Finally, we report early experimental results with our
prototype implementation on synthetic data.","Believe It or Not, Databases, Belief Annotations, Kripke, Synthetic Data"
NoSQL Databases,"In this document, I present the main notions of NoSQL databases and compare
four selected products (Riak, MongoDB, Cassandra, Neo4J) according to their
capabilities with respect to consistency, availability, and partition
tolerance, as well as performance. I also propose a few criteria for selecting
the right tool for the right situation.",Massimo Carro,"NoSQL Databases. In this document, I present the main notions of NoSQL databases and compare
four selected products (Riak, MongoDB, Cassandra, Neo4J) according to their
capabilities with respect to consistency, availability, and partition
tolerance, as well as performance. I also propose a few criteria for selecting
the right tool for the right situation.","NoSQL Databases, Rakio, MongoDB, Cassandra, Neo4J"
Suffix Arrays for Spaced-SNP Databases,"Single-nucleotide polymorphisms (SNPs) account for most variations between
human genomes. We show how, if the genomes in a database differ only by a
reasonable number of SNPs and the substrings between those SNPs are unique,
then we can store a fast compressed suffix array for that database.",Travis Gagie,"Suffix Arrays for Spaced-SNP Databases. Single-nucleotide polymorphisms (SNPs) account for most variations between
human genomes. We show how, if the genomes in a database differ only by a
reasonable number of SNPs and the substrings between those SNPs are unique,
then we can store a fast compressed suffix array for that database.","Suffix Arrays, Spaced-SNP Databases, Genomic Variables"
"The Journal Coverage of Web of Science, Scopus and Dimensions: A
  Comparative Analysis","Traditionally, Web of Science and Scopus have been the two most widely used
databases for bibliometric analyses. However, during the last few years some
new scholarly databases, such as Dimensions, have come up. Several previous
studies have compared different databases, either through a direct comparison
of article coverage or by comparing the citations across the databases. This
article attempts to compare the journal coverage of the three databases: Web of
Science, Scopus and Dimensions. The most recent master journal lists of the
three databases have been used for the purpose of identifying the overlapping
and unique journals covered in the databases. The results indicate that the
databases have significantly different journal coverage, with the Web of
Science being most selective and Dimensions being the most exhaustive. About
99.11% and 96.61% of the journals indexed in Web of Science are also indexed in
Scopus and Dimensions, respectively. Scopus has 96.42% of its indexed journals
also covered by Dimensions. Dimensions database has the most exhaustive
coverage, with 82.22% more journals covered as compared to Web of Science and
48.17% more journals covered as compared to Scopus. We also analysed the
research outputs for 20 highly productive countries for the 2010-2019 period,
as indexed in the three databases, and identified database-induced variations
in research output volume, rank and global share of different countries. In
addition to variations in overall coverage of research output from different
countries, the three databases appear to have differential coverage of
different disciplines.","Vivek Kumar Singh, Prashasti Singh, Mousumi Karmakar, Jacqueline Leta, Philipp Mayr","The Journal Coverage of Web of Science, Scopus and Dimensions: A
  Comparative Analysis. Traditionally, Web of Science and Scopus have been the two most widely used
databases for bibliometric analyses. However, during the last few years some
new scholarly databases, such as Dimensions, have come up. Several previous
studies have compared different databases, either through a direct comparison
of article coverage or by comparing the citations across the databases. This
article attempts to compare the journal coverage of the three databases: Web of
Science, Scopus and Dimensions. The most recent master journal lists of the
three databases have been used for the purpose of identifying the overlapping
and unique journals covered in the databases. The results indicate that the
databases have significantly different journal coverage, with the Web of
Science being most selective and Dimensions being the most exhaustive. About
99.11% and 96.61% of the journals indexed in Web of Science are also indexed in
Scopus and Dimensions, respectively. Scopus has 96.42% of its indexed journals
also covered by Dimensions. Dimensions database has the most exhaustive
coverage, with 82.22% more journals covered as compared to Web of Science and
48.17% more journals covered as compared to Scopus. We also analysed the
research outputs for 20 highly productive countries for the 2010-2019 period,
as indexed in the three databases, and identified database-induced variations
in research output volume, rank and global share of different countries. In
addition to variations in overall coverage of research output from different
countries, the three databases appear to have differential coverage of
different disciplines.","Java Coverage, Web of Science, Scopus, Dimensions, Bibliometric Analysis"
"High-concurrency Custom-build Relational Database System's design and
  SQL parser design based on Turing-complete automata","Database system is an indispensable part of software projects. It plays an
important role in data organization and storage. Its performance and efficiency
are directly related to the performance of software. Nowadays, we have many
general relational database systems that can be used in our projects, such as
SQL Server, MySQL, Oracle, etc. It is undeniable that in most cases, we can
easily use these database systems to complete our projects, but considering the
generality, the general database systems often can't play the ultimate speed
and fully adapt to our projects. In very few projects, we will need to design a
database system that fully adapt to our projects and have a high efficiency and
concurrency. Therefore, it is very important to consider a feasible solution of
designing a database system (We only consider the relational database system
here). Meanwhile, for a database system, SQL interpretation and execution
module is necessary. According to the theory of formal language and automata,
the realization of this module can be completed by automata. In our experiment,
we made the following contributions: 1) We designed a small relational
database, and used the database to complete a highly concurrent student course
selection system. 2) We design a general automaton module, which can complete
the operation from parsing to execution. The using of strategy model and event
driven design scheme is used and some improvement on general automata, for
example a memory like structure is added to automata to make it better to store
context. All these make the automata model can be used in a variety of
occasions, not only the parsing and execution of SQL statements.",WanHong Huang,"High-concurrency Custom-build Relational Database System's design and
  SQL parser design based on Turing-complete automata. Database system is an indispensable part of software projects. It plays an
important role in data organization and storage. Its performance and efficiency
are directly related to the performance of software. Nowadays, we have many
general relational database systems that can be used in our projects, such as
SQL Server, MySQL, Oracle, etc. It is undeniable that in most cases, we can
easily use these database systems to complete our projects, but considering the
generality, the general database systems often can't play the ultimate speed
and fully adapt to our projects. In very few projects, we will need to design a
database system that fully adapt to our projects and have a high efficiency and
concurrency. Therefore, it is very important to consider a feasible solution of
designing a database system (We only consider the relational database system
here). Meanwhile, for a database system, SQL interpretation and execution
module is necessary. According to the theory of formal language and automata,
the realization of this module can be completed by automata. In our experiment,
we made the following contributions: 1) We designed a small relational
database, and used the database to complete a highly concurrent student course
selection system. 2) We design a general automaton module, which can complete
the operation from parsing to execution. The using of strategy model and event
driven design scheme is used and some improvement on general automata, for
example a memory like structure is added to automata to make it better to store
context. All these make the automata model can be used in a variety of
occasions, not only the parsing and execution of SQL statements.","Relational Database System, Custom-build, Turing-complete Automata, SQL, SQL Server, MySQL, Oracle"
"KnobTree: Intelligent Database Parameter Configuration via Explainable
  Reinforcement Learning","Databases are fundamental to contemporary information systems, yet
traditional rule-based configuration methods struggle to manage the complexity
of real-world applications with hundreds of tunable parameters. Deep
reinforcement learning (DRL), which combines perception and decision-making,
presents a potential solution for intelligent database configuration tuning.
However, due to black-box property of RL-based method, the generated database
tuning strategies still face the urgent problem of lack explainability.
Besides, the redundant parameters in large scale database always make the
strategy learning become unstable. This paper proposes KnobTree, an
interpertable framework designed for the optimization of database parameter
configuration. In this framework, an interpertable database tuning algorithm
based on RL-based differentatial tree is proposed, which building a transparent
tree-based model to generate explainable database tuning strategies. To address
the problem of large-scale parameters, We also introduce a explainable method
for parameter importance assessment, by utilizing Shapley Values to identify
parameters that have significant impacts on database performance. Experiments
conducted on MySQL and Gbase8s databases have verified exceptional transparency
and interpretability of the KnobTree model. The good property makes generated
strategies can offer practical guidance to algorithm designers and database
administrators. Moreover, our approach also slightly outperforms the existing
RL-based tuning algorithms in aspects such as throughput, latency, and
processing time.","Jiahan Chen, Shuhan Qi, Yifan Li, Zeyu Dong, Mingfeng Ding, Yulin Wu, Xuan Wang","KnobTree: Intelligent Database Parameter Configuration via Explainable
  Reinforcement Learning. Databases are fundamental to contemporary information systems, yet
traditional rule-based configuration methods struggle to manage the complexity
of real-world applications with hundreds of tunable parameters. Deep
reinforcement learning (DRL), which combines perception and decision-making,
presents a potential solution for intelligent database configuration tuning.
However, due to black-box property of RL-based method, the generated database
tuning strategies still face the urgent problem of lack explainability.
Besides, the redundant parameters in large scale database always make the
strategy learning become unstable. This paper proposes KnobTree, an
interpertable framework designed for the optimization of database parameter
configuration. In this framework, an interpertable database tuning algorithm
based on RL-based differentatial tree is proposed, which building a transparent
tree-based model to generate explainable database tuning strategies. To address
the problem of large-scale parameters, We also introduce a explainable method
for parameter importance assessment, by utilizing Shapley Values to identify
parameters that have significant impacts on database performance. Experiments
conducted on MySQL and Gbase8s databases have verified exceptional transparency
and interpretability of the KnobTree model. The good property makes generated
strategies can offer practical guidance to algorithm designers and database
administrators. Moreover, our approach also slightly outperforms the existing
RL-based tuning algorithms in aspects such as throughput, latency, and
processing time.","KnobTree, Database Parameter Configuration, Reinforcement Learning, MySQL, Gbase8s, Shapley Values"
The Relational Database Aspects of Argonne's ATLAS Control System,"The Relational Database Aspects of Argonnes ATLAS Control System Argonnes
ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two
separate database concepts. The first is the distributed real-time database
structure provided by the commercial product Vsystem [1]. The second is a more
static relational database archiving system designed by ATLAS personnel using
Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS
facility has presented a unique opportunity to construct a control system
relational database that is capable of storing and retrieving complete archived
tune-up configurations for the entire accelerator. This capability has been a
major factor in allowing the facility to adhere to a rigorous operating
schedule. Most recently, a Web-based operator interface to the control systems
Oracle Rdb database has been installed. This paper explains the history of the
ATLAS database systems, how they interact with each other, the design of the
new Web-based operator interface, and future plans.","D. E. R. Quock, F. H. Munson, K. J. Eder, S. L. Dean","The Relational Database Aspects of Argonne's ATLAS Control System. The Relational Database Aspects of Argonnes ATLAS Control System Argonnes
ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two
separate database concepts. The first is the distributed real-time database
structure provided by the commercial product Vsystem [1]. The second is a more
static relational database archiving system designed by ATLAS personnel using
Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS
facility has presented a unique opportunity to construct a control system
relational database that is capable of storing and retrieving complete archived
tune-up configurations for the entire accelerator. This capability has been a
major factor in allowing the facility to adhere to a rigorous operating
schedule. Most recently, a Web-based operator interface to the control systems
Oracle Rdb database has been installed. This paper explains the history of the
ATLAS database systems, how they interact with each other, the design of the
new Web-based operator interface, and future plans.","Argonne, ATLAS, Relational Database, Argonne Tandem Linac Accelerator System, Vsystem, Paradox"
Querying Databases of Annotated Speech,"Annotated speech corpora are databases consisting of signal data along with
time-aligned symbolic `transcriptions'. Such databases are typically
multidimensional, heterogeneous and dynamic. These properties present a number
of tough challenges for representation and query. The temporal nature of the
data adds an additional layer of complexity. This paper presents and harmonises
two independent efforts to model annotated speech databases, one at Macquarie
University and one at the University of Pennsylvania. Various query languages
are described, along with illustrative applications to a variety of analytical
problems. The research reported here forms a part of several ongoing projects
to develop platform-independent open-source tools for creating, browsing,
searching, querying and transforming linguistic databases, and to disseminate
large linguistic databases over the internet.","Steve Cassidy, Steven Bird","Querying Databases of Annotated Speech. Annotated speech corpora are databases consisting of signal data along with
time-aligned symbolic `transcriptions'. Such databases are typically
multidimensional, heterogeneous and dynamic. These properties present a number
of tough challenges for representation and query. The temporal nature of the
data adds an additional layer of complexity. This paper presents and harmonises
two independent efforts to model annotated speech databases, one at Macquarie
University and one at the University of Pennsylvania. Various query languages
are described, along with illustrative applications to a variety of analytical
problems. The research reported here forms a part of several ongoing projects
to develop platform-independent open-source tools for creating, browsing,
searching, querying and transforming linguistic databases, and to disseminate
large linguistic databases over the internet.","Annotated Speech, Databases, Macquarie University, University of Pennsylvania, Open Source"
"An Integrated Approach for Extraction of Objects from XML and
  Transformation to Heterogeneous Object Oriented Databases","CERN's (European Organization for Nuclear Research) WISDOM project uses XML
for the replication of data between different data repositories in a
heterogeneous operating system environment. For exchanging data from
Web-resident databases, the data needs to be transformed into XML and back to
the database format. Many different approaches are employed to do this
transformation. This paper addresses issues that make this job more efficient
and robust than existing approaches. It incorporates the World Wide Web
Consortium (W3C) XML Schema specification in the database-XML relationship.
Incorporation of the XML Schema exhibits significant improvements in XML
content usage and reduces the limitations of DTD-based database XML services.
Secondly the paper explores the possibility of database independent
transformation of data between XML and different databases. It proposes a
standard XML format that every serialized object should follow. This makes it
possible to use objects of heterogeneous database seamlessly using XML.","Uzair Ahmad, Mohammad Waseem Hassan, Arshad Ali, Richard McClatchey, Ian Willers","An Integrated Approach for Extraction of Objects from XML and
  Transformation to Heterogeneous Object Oriented Databases. CERN's (European Organization for Nuclear Research) WISDOM project uses XML
for the replication of data between different data repositories in a
heterogeneous operating system environment. For exchanging data from
Web-resident databases, the data needs to be transformed into XML and back to
the database format. Many different approaches are employed to do this
transformation. This paper addresses issues that make this job more efficient
and robust than existing approaches. It incorporates the World Wide Web
Consortium (W3C) XML Schema specification in the database-XML relationship.
Incorporation of the XML Schema exhibits significant improvements in XML
content usage and reduces the limitations of DTD-based database XML services.
Secondly the paper explores the possibility of database independent
transformation of data between XML and different databases. It proposes a
standard XML format that every serialized object should follow. This makes it
possible to use objects of heterogeneous database seamlessly using XML.","XML, XML Schema, Heterogeneous Databases, CERN, WISDOM"
A Comparison of On-Line Computer Science Citation Databases,"This paper examines the difference and similarities between the two on-line
computer science citation databases DBLP and CiteSeer. The database entries in
DBLP are inserted manually while the CiteSeer entries are obtained autonomously
via a crawl of the Web and automatic processing of user submissions. CiteSeer's
autonomous citation database can be considered a form of self-selected on-line
survey. It is important to understand the limitations of such databases,
particularly when citation information is used to assess the performance of
authors, institutions and funding bodies.
  We show that the CiteSeer database contains considerably fewer single author
papers. This bias can be modeled by an exponential process with intuitive
explanation. The model permits us to predict that the DBLP database covers
approximately 24% of the entire literature of Computer Science. CiteSeer is
also biased against low-cited papers.
  Despite their difference, both databases exhibit similar and significantly
different citation distributions compared with previous analysis of the Physics
community. In both databases, we also observe that the number of authors per
paper has been increasing over time.","Vaclav Petricek, Ingemar J. Cox, Hui Han, Isaac G. Councill, C. Lee Giles","A Comparison of On-Line Computer Science Citation Databases. This paper examines the difference and similarities between the two on-line
computer science citation databases DBLP and CiteSeer. The database entries in
DBLP are inserted manually while the CiteSeer entries are obtained autonomously
via a crawl of the Web and automatic processing of user submissions. CiteSeer's
autonomous citation database can be considered a form of self-selected on-line
survey. It is important to understand the limitations of such databases,
particularly when citation information is used to assess the performance of
authors, institutions and funding bodies.
  We show that the CiteSeer database contains considerably fewer single author
papers. This bias can be modeled by an exponential process with intuitive
explanation. The model permits us to predict that the DBLP database covers
approximately 24% of the entire literature of Computer Science. CiteSeer is
also biased against low-cited papers.
  Despite their difference, both databases exhibit similar and significantly
different citation distributions compared with previous analysis of the Physics
community. In both databases, we also observe that the number of authors per
paper has been increasing over time.","Computer Science, Citation Databases, DBLP, CiteSeer"
Translating a first-order modal language to relational algebra,"This paper is about Kripke structures that are inside a relational database
and queried with a modal language. At first the modal language that is used is
introduced, followed by a definition of the database and relational algebra.
Based on these definitions two things are presented: a mapping from components
of the modal structure to a relational database schema and instance, and a
translation from queries in the modal language to relational algebra queries.",Yeb Havinga,"Translating a first-order modal language to relational algebra. This paper is about Kripke structures that are inside a relational database
and queried with a modal language. At first the modal language that is used is
introduced, followed by a definition of the database and relational algebra.
Based on these definitions two things are presented: a mapping from components
of the modal structure to a relational database schema and instance, and a
translation from queries in the modal language to relational algebra queries.","Modal Language, Relational Algebra, Kripke Structures"
Conditioning Probabilistic Databases,"Past research on probabilistic databases has studied the problem of answering
queries on a static database. Application scenarios of probabilistic databases
however often involve the conditioning of a database using additional
information in the form of new evidence. The conditioning problem is thus to
transform a probabilistic database of priors into a posterior probabilistic
database which is materialized for subsequent query processing or further
refinement. It turns out that the conditioning problem is closely related to
the problem of computing exact tuple confidence values.
  It is known that exact confidence computation is an NP-hard problem. This has
led researchers to consider approximation techniques for confidence
computation. However, neither conditioning nor exact confidence computation can
be solved using such techniques.
  In this paper we present efficient techniques for both problems. We study
several problem decomposition methods and heuristics that are based on the most
successful search techniques from constraint satisfaction, such as the
Davis-Putnam algorithm. We complement this with a thorough experimental
evaluation of the algorithms proposed. Our experiments show that our exact
algorithms scale well to realistic database sizes and can in some scenarios
compete with the most efficient previous approximation algorithms.","Christoph Koch, Dan Olteanu","Conditioning Probabilistic Databases. Past research on probabilistic databases has studied the problem of answering
queries on a static database. Application scenarios of probabilistic databases
however often involve the conditioning of a database using additional
information in the form of new evidence. The conditioning problem is thus to
transform a probabilistic database of priors into a posterior probabilistic
database which is materialized for subsequent query processing or further
refinement. It turns out that the conditioning problem is closely related to
the problem of computing exact tuple confidence values.
  It is known that exact confidence computation is an NP-hard problem. This has
led researchers to consider approximation techniques for confidence
computation. However, neither conditioning nor exact confidence computation can
be solved using such techniques.
  In this paper we present efficient techniques for both problems. We study
several problem decomposition methods and heuristics that are based on the most
successful search techniques from constraint satisfaction, such as the
Davis-Putnam algorithm. We complement this with a thorough experimental
evaluation of the algorithms proposed. Our experiments show that our exact
algorithms scale well to realistic database sizes and can in some scenarios
compete with the most efficient previous approximation algorithms.","Conditioning Probabilistic Databases, Confidence Computation, Davis-Putnam Algorithm, Problem Decomposition, Heuristics"
"A Compositional Query Algebra for Second-Order Logic and Uncertain
  Databases","World-set algebra is a variable-free query language for uncertain databases.
It constitutes the core of the query language implemented in MayBMS, an
uncertain database system. This paper shows that world-set algebra captures
exactly second-order logic over finite structures, or equivalently, the
polynomial hierarchy. The proofs also imply that world-set algebra is closed
under composition, a previously open problem.",Christoph Koch,"A Compositional Query Algebra for Second-Order Logic and Uncertain
  Databases. World-set algebra is a variable-free query language for uncertain databases.
It constitutes the core of the query language implemented in MayBMS, an
uncertain database system. This paper shows that world-set algebra captures
exactly second-order logic over finite structures, or equivalently, the
polynomial hierarchy. The proofs also imply that world-set algebra is closed
under composition, a previously open problem.","World-set Algebra, Second-Order Logic, Uncertain Databases, MayBMS"
Semantics and Evaluation of Top-k Queries in Probabilistic Databases,"We study here fundamental issues involved in top-k query evaluation in
probabilistic databases. We consider simple probabilistic databases in which
probabilities are associated with individual tuples, and general probabilistic
databases in which, additionally, exclusivity relationships between tuples can
be represented. In contrast to other recent research in this area, we do not
limit ourselves to injective scoring functions. We formulate three intuitive
postulates that the semantics of top-k queries in probabilistic databases
should satisfy, and introduce a new semantics, Global-Topk, that satisfies
those postulates to a large degree. We also show how to evaluate queries under
the Global-Topk semantics. For simple databases we design dynamic-programming
based algorithms, and for general databases we show polynomial-time reductions
to the simple cases. For example, we demonstrate that for a fixed k the time
complexity of top-k query evaluation is as low as linear, under the assumption
that probabilistic databases are simple and scoring functions are injective.","Xi Zhang, Jan Chomicki","Semantics and Evaluation of Top-k Queries in Probabilistic Databases. We study here fundamental issues involved in top-k query evaluation in
probabilistic databases. We consider simple probabilistic databases in which
probabilities are associated with individual tuples, and general probabilistic
databases in which, additionally, exclusivity relationships between tuples can
be represented. In contrast to other recent research in this area, we do not
limit ourselves to injective scoring functions. We formulate three intuitive
postulates that the semantics of top-k queries in probabilistic databases
should satisfy, and introduce a new semantics, Global-Topk, that satisfies
those postulates to a large degree. We also show how to evaluate queries under
the Global-Topk semantics. For simple databases we design dynamic-programming
based algorithms, and for general databases we show polynomial-time reductions
to the simple cases. For example, we demonstrate that for a fixed k the time
complexity of top-k query evaluation is as low as linear, under the assumption
that probabilistic databases are simple and scoring functions are injective.","Top-k Queries, Probabilistic Databases, Dynamic Programming, Global-Topk"
The WebContent XML Store,"In this article, we describe the XML storage system used in the WebContent
project. We begin by advocating the use of an XML database in order to store
WebContent documents, and we present two different ways of storing and querying
these documents : the use of a centralized XML database and the use of a P2P
XML database.","Benjamin Nguyen, Spyros Zoupanos","The WebContent XML Store. In this article, we describe the XML storage system used in the WebContent
project. We begin by advocating the use of an XML database in order to store
WebContent documents, and we present two different ways of storing and querying
these documents : the use of a centralized XML database and the use of a P2P
XML database.","XML, WebContent, Database, P2P"
Database optimization for empirical interatomic potential models,"Weighted least squares fitting to a database of quantum mechanical
calculations can determine the optimal parameters of empirical potential
models. While algorithms exist to provide optimal potential parameters for a
given fitting database of structures and their structure property functions,
and to estimate prediction errors using Bayesian sampling, defining an optimal
fitting database based on potential predictions remains elusive. A testing set
of structures and their structure property functions provides an empirical
measure of potential transferability. Here, we propose an objective function
for fitting databases based on testing set errors. The objective function
allows the optimization of the weights in a fitting database, the assessment of
the inclusion or removal of structures in the fitting database, or the
comparison of two different fitting databases. To showcase this technique, we
consider an example Lennard-Jones potential for Ti, where modeling multiple
complicated crystal structures is difficult for a radial pair potential. The
algorithm finds different optimal fitting databases, depending on the objective
function of potential prediction error for a testing set.","Pinchao Zhang, Dallas Trinkle","Database optimization for empirical interatomic potential models. Weighted least squares fitting to a database of quantum mechanical
calculations can determine the optimal parameters of empirical potential
models. While algorithms exist to provide optimal potential parameters for a
given fitting database of structures and their structure property functions,
and to estimate prediction errors using Bayesian sampling, defining an optimal
fitting database based on potential predictions remains elusive. A testing set
of structures and their structure property functions provides an empirical
measure of potential transferability. Here, we propose an objective function
for fitting databases based on testing set errors. The objective function
allows the optimization of the weights in a fitting database, the assessment of
the inclusion or removal of structures in the fitting database, or the
comparison of two different fitting databases. To showcase this technique, we
consider an example Lennard-Jones potential for Ti, where modeling multiple
complicated crystal structures is difficult for a radial pair potential. The
algorithm finds different optimal fitting databases, depending on the objective
function of potential prediction error for a testing set.","Database Optimization, Empirical Potential Models, Bayesian Sampling, Lennard-Jones Potential"
A Hybrid Pricing Framework for TV White Space Database,"According to the recent rulings of the Federal Communications Commission
(FCC), TV white spaces (TVWS) can now be accessed by secondary users (SUs)
after a list of vacant TV channels is obtained via a geo-location database.
Proper business models are therefore essential for database operators to manage
geo-location databases. Database access can be simultaneously priced under two
different schemes: the registration scheme and the service plan scheme. In the
registration scheme, the database reserves part of the TV bandwidth for
registered White Space Devices (WSDs). In the service plan scheme, the WSDs are
charged according to their queries. In this paper, we investigate the business
model for the TVWS database under a hybrid pricing scheme. We consider the
scenario where a database operator employs both the registration scheme and the
service plan scheme to serve the SUs. The SUs' choices of different pricing
schemes are modeled as a non-cooperative game and we derive distributed
algorithms to achieve Nash Equilibrium (NE). Considering the NE of the SUs, the
database operator optimally determines pricing parameters for both pricing
schemes in terms of bandwidth reservation, registration fee and query plans.","Xiaojun Feng, Qian Zhang, Jin Zhang","A Hybrid Pricing Framework for TV White Space Database. According to the recent rulings of the Federal Communications Commission
(FCC), TV white spaces (TVWS) can now be accessed by secondary users (SUs)
after a list of vacant TV channels is obtained via a geo-location database.
Proper business models are therefore essential for database operators to manage
geo-location databases. Database access can be simultaneously priced under two
different schemes: the registration scheme and the service plan scheme. In the
registration scheme, the database reserves part of the TV bandwidth for
registered White Space Devices (WSDs). In the service plan scheme, the WSDs are
charged according to their queries. In this paper, we investigate the business
model for the TVWS database under a hybrid pricing scheme. We consider the
scenario where a database operator employs both the registration scheme and the
service plan scheme to serve the SUs. The SUs' choices of different pricing
schemes are modeled as a non-cooperative game and we derive distributed
algorithms to achieve Nash Equilibrium (NE). Considering the NE of the SUs, the
database operator optimally determines pricing parameters for both pricing
schemes in terms of bandwidth reservation, registration fee and query plans.","Hybrid Pricing Framework, TV White Space Database, FCC, Geo-location Databases, Nash Equilibrium"
Large-scale Biological Meta-database Management,"Up-to-date meta-databases are vital for the analysis of biological data.
However,the current exponential increase in biological data leads to
exponentially increasing meta-database sizes. Large-scale meta-database
management is therefore an important challenge for production platforms
providing services for biological data analysis. In particular, there is often
a need either to run an analysis with a particular version of a meta-database,
or to rerun an analysis with an updated meta-database. We present our GeStore
approach for biological meta-database management. It provides efficient storage
and runtime generation of specific meta-database versions, and efficient
incremental updates for biological data analysis tools. The approach is
transparent to the tools, and we provide a framework that makes it easy to
integrate GeStore with biological data analysis frameworks. We present the
GeStore system, an evaluation of the performance characteristics of the system,
and an evaluation of the benefits for a biological data analysis workflow.","Edvard Pedersen, Lars Ailo Bongo","Large-scale Biological Meta-database Management. Up-to-date meta-databases are vital for the analysis of biological data.
However,the current exponential increase in biological data leads to
exponentially increasing meta-database sizes. Large-scale meta-database
management is therefore an important challenge for production platforms
providing services for biological data analysis. In particular, there is often
a need either to run an analysis with a particular version of a meta-database,
or to rerun an analysis with an updated meta-database. We present our GeStore
approach for biological meta-database management. It provides efficient storage
and runtime generation of specific meta-database versions, and efficient
incremental updates for biological data analysis tools. The approach is
transparent to the tools, and we provide a framework that makes it easy to
integrate GeStore with biological data analysis frameworks. We present the
GeStore system, an evaluation of the performance characteristics of the system,
and an evaluation of the benefits for a biological data analysis workflow.","Biological Meta-database, GeStore, Meta-databases, Production Platforms, Bioanalysis"
Getting Started with PATSTAT Register,"This paper provides a technical introduction to the PATSTAT Register
database, which contains bibliographical, procedural and legal status data on
patent applications handled by the European Patent Office. It presents eight
MySQL queries that cover some of the most relevant aspects of the database for
research purposes. It targets academic researchers and practitioners who are
familiar with the PATSTAT database and the MySQL language.","Gaetan de Rassenfosse, Martin Kracker, Gianluca Tarasconi","Getting Started with PATSTAT Register. This paper provides a technical introduction to the PATSTAT Register
database, which contains bibliographical, procedural and legal status data on
patent applications handled by the European Patent Office. It presents eight
MySQL queries that cover some of the most relevant aspects of the database for
research purposes. It targets academic researchers and practitioners who are
familiar with the PATSTAT database and the MySQL language.","PATSTAT Register, MySQL, European Patent Office"
"A Cloud-based Service for Real-Time Performance Evaluation of NoSQL
  Databases","We have created a cloud-based service that allows the end users to run tests
on multiple different databases to find which databases are most suitable for
their project. From our research, we could not find another application that
enables the user to test several databases to gauge the difference between
them. This application allows the user to choose which type of test to perform
and which databases to target. The application also displays the results of
different tests that were run by other users previously. There is also a map to
show the location where all the tests are run to give the user an estimate of
the location. Unlike the orthodox static tests and reports conducted to
evaluate NoSQL databases, we have created a web application to run and analyze
these tests in real time. This web application evaluates the performance of
several NoSQL databases. The databases covered are MongoDB, DynamoDB, CouchDB,
and Firebase. The web service is accessible from: nosqldb.nextproject.ca.","Omar Almootassem, Syed Hamza Husain, Denesh Parthipan, Qusay H. Mahmoud","A Cloud-based Service for Real-Time Performance Evaluation of NoSQL
  Databases. We have created a cloud-based service that allows the end users to run tests
on multiple different databases to find which databases are most suitable for
their project. From our research, we could not find another application that
enables the user to test several databases to gauge the difference between
them. This application allows the user to choose which type of test to perform
and which databases to target. The application also displays the results of
different tests that were run by other users previously. There is also a map to
show the location where all the tests are run to give the user an estimate of
the location. Unlike the orthodox static tests and reports conducted to
evaluate NoSQL databases, we have created a web application to run and analyze
these tests in real time. This web application evaluates the performance of
several NoSQL databases. The databases covered are MongoDB, DynamoDB, CouchDB,
and Firebase. The web service is accessible from: nosqldb.nextproject.ca.","NoSQL, Cloud-based Service, Performance Evaluation, MongoDB, DynamoDB, CouchDB, Firebase"
On Patterns and Re-Use in Bioinformatics Databases,"As the quantity of data being depositing into biological databases continues
to increase, it becomes ever more vital to develop methods that enable us to
understand this data and ensure that the knowledge is correct. It is
widely-held that data percolates between different databases, which causes
particular concerns for data correctness; if this percolation occurs, incorrect
data in one database may eventually affect many others while, conversely,
corrections in one database may fail to percolate to others.
  In this paper, we test this widely-held belief by directly looking for
sentence reuse both within and between databases. Further, we investigate
patterns of how sentences are reused over time. Finally, we consider the
limitations of this form of analysis and the implications that this may have
for bioinformatics database design.
  We show that reuse of annotation is common within many different databases,
and that also there is a detectable level of reuse between databases. In
addition, we show that there are patterns of reuse that have previously been
shown to be associated with percolation errors.","Michael J Bell, Phillip Lord","On Patterns and Re-Use in Bioinformatics Databases. As the quantity of data being depositing into biological databases continues
to increase, it becomes ever more vital to develop methods that enable us to
understand this data and ensure that the knowledge is correct. It is
widely-held that data percolates between different databases, which causes
particular concerns for data correctness; if this percolation occurs, incorrect
data in one database may eventually affect many others while, conversely,
corrections in one database may fail to percolate to others.
  In this paper, we test this widely-held belief by directly looking for
sentence reuse both within and between databases. Further, we investigate
patterns of how sentences are reused over time. Finally, we consider the
limitations of this form of analysis and the implications that this may have
for bioinformatics database design.
  We show that reuse of annotation is common within many different databases,
and that also there is a detectable level of reuse between databases. In
addition, we show that there are patterns of reuse that have previously been
shown to be associated with percolation errors.","Bioinformatics, Databases, Re-Use, Percolation Errors"
"Specifying and Computing Causes for Query Answers in Databases via
  Database Repairs and Repair Programs","A correspondence between database tuples as causes for query answers in
databases and tuple-based repairs of inconsistent databases with respect to
denial constraints has already been established. In this work, answer-set
programs that specify repairs of databases are used as a basis for solving
computational and reasoning problems about causes. Here, causes are also
introduced at the attribute level by appealing to a both null-based and
attribute-based repair semantics. The corresponding repair programs are
presented, and they are used as a basis for computation and reasoning about
attribute-level causes. They are extended to deal with the case of causality
under integrity constraints.",Leopoldo Bertossi,"Specifying and Computing Causes for Query Answers in Databases via
  Database Repairs and Repair Programs. A correspondence between database tuples as causes for query answers in
databases and tuple-based repairs of inconsistent databases with respect to
denial constraints has already been established. In this work, answer-set
programs that specify repairs of databases are used as a basis for solving
computational and reasoning problems about causes. Here, causes are also
introduced at the attribute level by appealing to a both null-based and
attribute-based repair semantics. The corresponding repair programs are
presented, and they are used as a basis for computation and reasoning about
attribute-level causes. They are extended to deal with the case of causality
under integrity constraints.","Database Repairs, Repair Programs, Query Answers, Causality, Integrity Constraints"
An introduction to Graph Data Management,"A graph database is a database where the data structures for the schema
and/or instances are modeled as a (labeled)(directed) graph or generalizations
of it, and where querying is expressed by graph-oriented operations and type
constructors. In this article we present the basic notions of graph databases,
give an historical overview of its main development, and study the main current
systems that implement them.","Renzo Angles, Claudio Gutierrez","An introduction to Graph Data Management. A graph database is a database where the data structures for the schema
and/or instances are modeled as a (labeled)(directed) graph or generalizations
of it, and where querying is expressed by graph-oriented operations and type
constructors. In this article we present the basic notions of graph databases,
give an historical overview of its main development, and study the main current
systems that implement them.","Graph Database, Data Management"
A Concentration of Measure Approach to Database De-anonymization,"In this paper, matching of correlated high-dimensional databases is
investigated. A stochastic database model is considered where the correlation
among the database entries is governed by an arbitrary joint distribution.
Concentration of measure theorems such as typicality and laws of large numbers
are used to develop a database matching scheme and derive necessary conditions
for successful matching. Furthermore, it is shown that these conditions are
tight through a converse result which characterizes a set of distributions on
the database entries for which reliable matching is not possible. The necessary
and sufficient conditions for reliable matching are evaluated in the cases when
the database entries are independent and identically distributed as well as
under Markovian database models.","Farhad Shirani, Siddharth Garg, Elza Erkip","A Concentration of Measure Approach to Database De-anonymization. In this paper, matching of correlated high-dimensional databases is
investigated. A stochastic database model is considered where the correlation
among the database entries is governed by an arbitrary joint distribution.
Concentration of measure theorems such as typicality and laws of large numbers
are used to develop a database matching scheme and derive necessary conditions
for successful matching. Furthermore, it is shown that these conditions are
tight through a converse result which characterizes a set of distributions on
the database entries for which reliable matching is not possible. The necessary
and sufficient conditions for reliable matching are evaluated in the cases when
the database entries are independent and identically distributed as well as
under Markovian database models.","Database De-anonymization, Stochastic Database, Markovian Database"
Long Live The Image: Container-Native Data Persistence in Production,"Containerization plays a crucial role in the de facto technology stack for
implementing microservices architecture (each microservice has its own database
in most cases). Nevertheless, there are still fierce debates on containerizing
production databases, mainly due to the data persistence issues and concerns.
Driven by a project of refactoring an Automated Machine Learning system, this
research proposes the container-native data persistence as a conditional
solution to running database containers in production. In essence, the proposed
solution distinguishes the stateless data access (i.e. reading) from the
stateful data processing (i.e. creating, updating, and deleting) in databases.
A master database handles the stateful data processing and dumps database
copies for building container images, while the database containers will keep
stateless at runtime, based on the preloaded dump in the image. Although there
are delays in the state/image update propagation, this solution is particularly
suitable for the read-only, the eventual consistency, and the asynchronous
processing scenarios. Moreover, with optimal tuning (e.g., disabling locking),
the portability and performance gains of a read-only database container would
outweigh the performance loss in accessing data across the underlying image
layers.",Zheng Li,"Long Live The Image: Container-Native Data Persistence in Production. Containerization plays a crucial role in the de facto technology stack for
implementing microservices architecture (each microservice has its own database
in most cases). Nevertheless, there are still fierce debates on containerizing
production databases, mainly due to the data persistence issues and concerns.
Driven by a project of refactoring an Automated Machine Learning system, this
research proposes the container-native data persistence as a conditional
solution to running database containers in production. In essence, the proposed
solution distinguishes the stateless data access (i.e. reading) from the
stateful data processing (i.e. creating, updating, and deleting) in databases.
A master database handles the stateful data processing and dumps database
copies for building container images, while the database containers will keep
stateless at runtime, based on the preloaded dump in the image. Although there
are delays in the state/image update propagation, this solution is particularly
suitable for the read-only, the eventual consistency, and the asynchronous
processing scenarios. Moreover, with optimal tuning (e.g., disabling locking),
the portability and performance gains of a read-only database container would
outweigh the performance loss in accessing data across the underlying image
layers.","Container-Native Data Persistence, Containerization, Automated Machine Learning, Asynchronous Processing"
One Model to Rule them All: Towards Zero-Shot Learning for Databases,"In this paper, we present our vision of so called zero-shot learning for
databases which is a new learning approach for database components. Zero-shot
learning for databases is inspired by recent advances in transfer learning of
models such as GPT-3 and can support a new database out-of-the box without the
need to train a new model. Furthermore, it can easily be extended to few-shot
learning by further retraining the model on the unseen database. As a first
concrete contribution in this paper, we show the feasibility of zero-shot
learning for the task of physical cost estimation and present very promising
initial results. Moreover, as a second contribution we discuss the core
challenges related to zero-shot learning for databases and present a roadmap to
extend zero-shot learning towards many other tasks beyond cost estimation or
even beyond classical database systems and workloads.","Benjamin Hilprecht, Carsten Binnig","One Model to Rule them All: Towards Zero-Shot Learning for Databases. In this paper, we present our vision of so called zero-shot learning for
databases which is a new learning approach for database components. Zero-shot
learning for databases is inspired by recent advances in transfer learning of
models such as GPT-3 and can support a new database out-of-the box without the
need to train a new model. Furthermore, it can easily be extended to few-shot
learning by further retraining the model on the unseen database. As a first
concrete contribution in this paper, we show the feasibility of zero-shot
learning for the task of physical cost estimation and present very promising
initial results. Moreover, as a second contribution we discuss the core
challenges related to zero-shot learning for databases and present a roadmap to
extend zero-shot learning towards many other tasks beyond cost estimation or
even beyond classical database systems and workloads.","Zero-Shot Learning, Databases, GPT-3, Transfer Learning, Few-shot"
"Bag-of-Features Image Indexing and Classification in Microsoft SQL
  Server Relational Database","This paper presents a novel relational database architecture aimed to visual
objects classification and retrieval. The framework is based on the
bag-of-features image representation model combined with the Support Vector
Machine classification and is integrated in a Microsoft SQL Server database.","Marcin Korytkowski, Rafal Scherer, Pawel Staszewski, Piotr Woldan","Bag-of-Features Image Indexing and Classification in Microsoft SQL
  Server Relational Database. This paper presents a novel relational database architecture aimed to visual
objects classification and retrieval. The framework is based on the
bag-of-features image representation model combined with the Support Vector
Machine classification and is integrated in a Microsoft SQL Server database.","Bag-of-Features, Image Indexing, Classification, Microsoft SQL Server, Support Vector Machine, Relational Database"
polyDB: A Database for Polytopes and Related Objects,"polyDB is a database for discrete geometric objects. The database is
accessible via web and an interface from the software package polymake. It
contains various datasets from the area of lattice polytopes, combinatorial
polytopes, matroids and tropical geometry. In this short note we introduce the
structure of the database and explain its use with a computation of the free
sums and certain skew bipyramids among the class of smooth Fano polytopes in
dimension up to 8.",Andreas Paffenholz,"polyDB: A Database for Polytopes and Related Objects. polyDB is a database for discrete geometric objects. The database is
accessible via web and an interface from the software package polymake. It
contains various datasets from the area of lattice polytopes, combinatorial
polytopes, matroids and tropical geometry. In this short note we introduce the
structure of the database and explain its use with a computation of the free
sums and certain skew bipyramids among the class of smooth Fano polytopes in
dimension up to 8.","polyDB, Database, Polytopes, Matroids, Tropical Geometry"
Fundamental Limits of Database Alignment,"We consider the problem of aligning a pair of databases with correlated
entries. We introduce a new measure of correlation in a joint distribution that
we call cycle mutual information. This measure has operational significance: it
determines whether exact recovery of the correspondence between database
entries is possible for any algorithm. Additionally, there is an efficient
algorithm for database alignment that achieves this information theoretic
threshold.","Daniel Cullina, Prateek Mittal, Negar Kiyavash","Fundamental Limits of Database Alignment. We consider the problem of aligning a pair of databases with correlated
entries. We introduce a new measure of correlation in a joint distribution that
we call cycle mutual information. This measure has operational significance: it
determines whether exact recovery of the correspondence between database
entries is possible for any algorithm. Additionally, there is an efficient
algorithm for database alignment that achieves this information theoretic
threshold.","Database Alignment, Information Theory, Cycle Mutual Information"
Social Choice Methods for Database Aggregation,"Knowledge can be represented compactly in multiple ways, from a set of
propositional formulas, to a Kripke model, to a database. In this paper we
study the aggregation of information coming from multiple sources, each source
submitting a database modelled as a first-order relational structure. In the
presence of integrity constraints, we identify classes of aggregators that
respect them in the aggregated database, provided these are satisfied in all
individual databases. We also characterise languages for first-order queries on
which the answer to a query on the aggregated database coincides with the
aggregation of the answers to the query obtained on each individual database.
This contribution is meant to be a first step on the application of techniques
from social choice theory to knowledge representation in databases.","Francesco Belardinelli, Umberto Grandi","Social Choice Methods for Database Aggregation. Knowledge can be represented compactly in multiple ways, from a set of
propositional formulas, to a Kripke model, to a database. In this paper we
study the aggregation of information coming from multiple sources, each source
submitting a database modelled as a first-order relational structure. In the
presence of integrity constraints, we identify classes of aggregators that
respect them in the aggregated database, provided these are satisfied in all
individual databases. We also characterise languages for first-order queries on
which the answer to a query on the aggregated database coincides with the
aggregation of the answers to the query obtained on each individual database.
This contribution is meant to be a first step on the application of techniques
from social choice theory to knowledge representation in databases.","Social Choice Methods, Database Aggregation, Kripke Model, Integrity Constraints"
Trends in Development of Databases and Blockchain,"This work is about the mutual influence between two technologies: Databases
and Blockchain. It addresses two questions: 1. How the database technology has
influenced the development of blockchain technology?, and 2. How blockchain
technology has influenced the introduction of new functionalities in some
modern databases? For the first question, we explain how database technology
contributes to blockchain technology by unlocking different features such as
ACID (Atomicity, Consistency, Isolation, and Durability) transactional
consistency, rich queries, real-time analytics, and low latency. We explain how
the CAP (Consistency, Availability, Partition tolerance) theorem known for
databases influenced the DCS (Decentralization, Consistency, Scalability)
theorem for the blockchain systems. By using an analogous relaxation approach
as it was used for the proof of the CAP theorem, we postulate a
""DCS-satisfiability conjecture."" For the second question, we review different
databases that are designed specifically for blockchain and provide most of the
blockchain functionality like immutability, privacy, censorship resistance,
along with database features.","Mayank Raikwar, Danilo Gligoroski, Goran Velinov","Trends in Development of Databases and Blockchain. This work is about the mutual influence between two technologies: Databases
and Blockchain. It addresses two questions: 1. How the database technology has
influenced the development of blockchain technology?, and 2. How blockchain
technology has influenced the introduction of new functionalities in some
modern databases? For the first question, we explain how database technology
contributes to blockchain technology by unlocking different features such as
ACID (Atomicity, Consistency, Isolation, and Durability) transactional
consistency, rich queries, real-time analytics, and low latency. We explain how
the CAP (Consistency, Availability, Partition tolerance) theorem known for
databases influenced the DCS (Decentralization, Consistency, Scalability)
theorem for the blockchain systems. By using an analogous relaxation approach
as it was used for the proof of the CAP theorem, we postulate a
""DCS-satisfiability conjecture."" For the second question, we review different
databases that are designed specifically for blockchain and provide most of the
blockchain functionality like immutability, privacy, censorship resistance,
along with database features.","Database Technology, Blockchain, DCS, Atomicity, Consistency, Availability, Partition Tolerance, Durability, Immutability, Privacy, Censorship"
"Hihooi: A Database Replication Middleware for Scaling Transactional
  Databases Consistently","With the advent of the Internet and Internet-connected devices, modern
business applications can experience rapid increases as well as variability in
transactional workloads. Database replication has been employed to scale
performance and improve availability of relational databases but past
approaches have suffered from various issues including limited scalability,
performance versus consistency tradeoffs, and requirements for database or
application modifications. This paper presents Hihooi, a replication-based
middleware system that is able to achieve workload scalability, strong
consistency guarantees, and elasticity for existing transactional databases at
a low cost. A novel replication algorithm enables Hihooi to propagate database
modifications asynchronously to all replicas at high speeds, while ensuring
that all replicas are consistent. At the same time, a fine-grained routing
algorithm is used to load balance incoming transactions to available replicas
in a consistent way. Our thorough experimental evaluation with several
well-established benchmarks shows how Hihooi is able to achieve almost linear
workload scalability for transactional databases.","Michael A. Georgiou, Aristodemos Paphitis, Michael Sirivianos, Herodotos Herodotou","Hihooi: A Database Replication Middleware for Scaling Transactional
  Databases Consistently. With the advent of the Internet and Internet-connected devices, modern
business applications can experience rapid increases as well as variability in
transactional workloads. Database replication has been employed to scale
performance and improve availability of relational databases but past
approaches have suffered from various issues including limited scalability,
performance versus consistency tradeoffs, and requirements for database or
application modifications. This paper presents Hihooi, a replication-based
middleware system that is able to achieve workload scalability, strong
consistency guarantees, and elasticity for existing transactional databases at
a low cost. A novel replication algorithm enables Hihooi to propagate database
modifications asynchronously to all replicas at high speeds, while ensuring
that all replicas are consistent. At the same time, a fine-grained routing
algorithm is used to load balance incoming transactions to available replicas
in a consistent way. Our thorough experimental evaluation with several
well-established benchmarks shows how Hihooi is able to achieve almost linear
workload scalability for transactional databases.","Hihooi, Database Replication, Relational Databases, Algorithms, Elasticity"
The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database,"The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database
extends the previous Wide Multi-Channel Attack database(WMCA), with more
channels including color, depth, thermal, infrared (spectra), and short-wave
infrared (spectra), and also a wide variety of attacks.","Zohreh Mostaani, Anjith George, Guillaume Heusch, David Geissbuhler, Sebastien Marcel","The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database. The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database
extends the previous Wide Multi-Channel Attack database(WMCA), with more
channels including color, depth, thermal, infrared (spectra), and short-wave
infrared (spectra), and also a wide variety of attacks.","HQ-WMCA, Wide Multi-Channel Attack, Database, Color, Depth, Thermal, Infrared"
"Translating synthetic natural language to database queries: a polyglot
  deep learning framework","The number of databases as well as their size and complexity is increasing.
This creates a barrier to use especially for non-experts, who have to come to
grips with the nature of the data, the way it has been represented in the
database, and the specific query languages or user interfaces by which data are
accessed. These difficulties worsen in research settings, where it is common to
work with many different databases. One approach to improving this situation is
to allow users to pose their queries in natural language.
  In this work we describe a machine learning framework, Polyglotter, that in a
general way supports the mapping of natural language searches to database
queries. Importantly, it does not require the creation of manually annotated
data for training and therefore can be applied easily to multiple domains. The
framework is polyglot in the sense that it supports multiple different database
engines that are accessed with a variety of query languages, including SQL and
Cypher. Furthermore Polyglotter also supports multi-class queries.
  Our results indicate that our framework performs well on both synthetic and
real databases, and may provide opportunities for database maintainers to
improve accessibility to their resources.","Adrián Bazaga, Nupur Gunwant, Gos Micklem","Translating synthetic natural language to database queries: a polyglot
  deep learning framework. The number of databases as well as their size and complexity is increasing.
This creates a barrier to use especially for non-experts, who have to come to
grips with the nature of the data, the way it has been represented in the
database, and the specific query languages or user interfaces by which data are
accessed. These difficulties worsen in research settings, where it is common to
work with many different databases. One approach to improving this situation is
to allow users to pose their queries in natural language.
  In this work we describe a machine learning framework, Polyglotter, that in a
general way supports the mapping of natural language searches to database
queries. Importantly, it does not require the creation of manually annotated
data for training and therefore can be applied easily to multiple domains. The
framework is polyglot in the sense that it supports multiple different database
engines that are accessed with a variety of query languages, including SQL and
Cypher. Furthermore Polyglotter also supports multi-class queries.
  Our results indicate that our framework performs well on both synthetic and
real databases, and may provide opportunities for database maintainers to
improve accessibility to their resources.","Polyglotter, Deep Learning, Synthetic Natural Language Processing, Database, SQL, Cypher"
NHtapDB: Native HTAP Databases,"Native database (1) provides a near-data machine learning framework to
facilitate generating real-time business insight, and predefined change
thresholds will trigger online training and deployment of new models, and (2)
offers a mixed-format store to guarantee the performance of HTAP workloads,
especially the hybrid workloads that consist of OLAP queries in-between online
transactions. We make rigorous test plans for native database with an enhanced
state-of-the-art HTAP benchmark.","Guoxin Kang, Lei Wang, Simin Chen, Jianfeng Zhan","NHtapDB: Native HTAP Databases. Native database (1) provides a near-data machine learning framework to
facilitate generating real-time business insight, and predefined change
thresholds will trigger online training and deployment of new models, and (2)
offers a mixed-format store to guarantee the performance of HTAP workloads,
especially the hybrid workloads that consist of OLAP queries in-between online
transactions. We make rigorous test plans for native database with an enhanced
state-of-the-art HTAP benchmark.","NHtapDB, Native HTAP Databases, Near-Data, Machine Learning, Mixed-format Store, OLAP"
Graph Data Models and Relational Database Technology,"Recent work on database application development platforms has sought to
include a declarative formulation of a conceptual data model in the application
code, using annotations or attributes. Some recent work has used metadata to
include the details of such formulations in the physical database, and this
approach brings significant advantages in that the model can be enforced across
a range of applications for a single database. In previous work, we have
discussed the advantages for enterprise integration of typed graph data models
(TGM), which can play a similar role in graphical databases, leveraging the
existing support for the unified modelling language UML. Ideally, the
integration of systems designed with different models, for example, graphical
and relational database, should also be supported. In this work, we implement
this approach, using metadata in a relational database management system
(DBMS).","Malcolm Crowe, Fritz Laux","Graph Data Models and Relational Database Technology. Recent work on database application development platforms has sought to
include a declarative formulation of a conceptual data model in the application
code, using annotations or attributes. Some recent work has used metadata to
include the details of such formulations in the physical database, and this
approach brings significant advantages in that the model can be enforced across
a range of applications for a single database. In previous work, we have
discussed the advantages for enterprise integration of typed graph data models
(TGM), which can play a similar role in graphical databases, leveraging the
existing support for the unified modelling language UML. Ideally, the
integration of systems designed with different models, for example, graphical
and relational database, should also be supported. In this work, we implement
this approach, using metadata in a relational database management system
(DBMS).","Graph Data Models, Relational Database, Database Management System, DBMS, TGM"
"DONUT -- Creation, Development, and Opportunities of a Database","DONUT is a database of papers about practical, real-world uses of Topological
Data Analysis (TDA). Its original seed was planted in a group chat formed
during the HIM Spring School on Applied and Computational Algebraic Topology in
April 2017. This document describes the creation, curation, and maintenance
process of the database.","Barbara Giunti, Jānis Lazovskis, Bastian Rieck","DONUT -- Creation, Development, and Opportunities of a Database. DONUT is a database of papers about practical, real-world uses of Topological
Data Analysis (TDA). Its original seed was planted in a group chat formed
during the HIM Spring School on Applied and Computational Algebraic Topology in
April 2017. This document describes the creation, curation, and maintenance
process of the database.","DONUT, TopologicalData Analysis, TDA, HIM Spring School, Algebraic Topology"
From Database Repairs to Causality in Databases and Beyond,"We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.",Leopoldo Bertossi,"From Database Repairs to Causality in Databases and Beyond. We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.","Database Repairs, Causality, Score-based Explanation, Counterfactual Reasoning"
"Relational Playground: Teaching the Duality of Relational Algebra and
  SQL","Students in introductory data management courses are often taught how to
write queries in SQL. This is a useful and practical skill, but it gives
limited insight into how queries are processed by relational database engines.
In contrast, relational algebra is a commonly used internal representation of
queries by database engines, but can be challenging for students to grasp. We
developed a tool we call Relational Playground for database students to explore
the connection between relational algebra and SQL.",Michael Mior,"Relational Playground: Teaching the Duality of Relational Algebra and
  SQL. Students in introductory data management courses are often taught how to
write queries in SQL. This is a useful and practical skill, but it gives
limited insight into how queries are processed by relational database engines.
In contrast, relational algebra is a commonly used internal representation of
queries by database engines, but can be challenging for students to grasp. We
developed a tool we call Relational Playground for database students to explore
the connection between relational algebra and SQL.","Relational Playground, Relational Algebra, SQL, Data Management"
Databases for comparative syntactic research,"Recent years have witnessed a steep increase in linguistic databases
capturing syntactic variation. We survey and describe 21 publicly available
morpho-syntactic databases, focusing on such properties as data structure, user
interface, documentation, formats, and overall user friendliness. We
demonstrate that all the surveyed databases can be fruitfully categorized along
two dimensions: units of description and the design principle. Units of
description refer to the type of the data the database represents (languages,
constructions, or expressions). The design principles capture the internal
logic of the database. We identify three primary design principles, which vary
in their descriptive power, granularity, and complexity: monocategorization,
multicategorization, and structural decomposition. We describe how these design
principles are implemented in concrete databases and discuss their advantages
and limitations. Finally, we outline essential desiderata for future modern
databases in linguistics.","Jessica K. Ivani, Balthasar Bickel","Databases for comparative syntactic research. Recent years have witnessed a steep increase in linguistic databases
capturing syntactic variation. We survey and describe 21 publicly available
morpho-syntactic databases, focusing on such properties as data structure, user
interface, documentation, formats, and overall user friendliness. We
demonstrate that all the surveyed databases can be fruitfully categorized along
two dimensions: units of description and the design principle. Units of
description refer to the type of the data the database represents (languages,
constructions, or expressions). The design principles capture the internal
logic of the database. We identify three primary design principles, which vary
in their descriptive power, granularity, and complexity: monocategorization,
multicategorization, and structural decomposition. We describe how these design
principles are implemented in concrete databases and discuss their advantages
and limitations. Finally, we outline essential desiderata for future modern
databases in linguistics.","Databases, Comparative Syntactic Research, Linguistics"
Effective Bug Detection in Graph Database Engines: An LLM-based Approach,"Graph database engines play a pivotal role in efficiently storing and
managing graph data across various domains, including bioinformatics, knowledge
graphs, and recommender systems. Ensuring data accuracy within graph database
engines is paramount, as inaccuracies can yield unreliable analytical outcomes.
Current bug-detection approaches are confined to specific graph query
languages, limiting their applicabilities when handling graph database engines
that use various graph query languages across various domains. Moreover, they
require extensive prior knowledge to generate queries for detecting bugs. To
address these challenges, we introduces DGDB, a novel paradigm harnessing large
language models(LLM), such as ChatGPT, for comprehensive bug detection in graph
database engines. DGDB leverages ChatGPT to generate high-quality queries for
different graph query languages. It subsequently employs differential testing
to identify bugs in graph database engines. We applied this paradigm to graph
database engines using the Gremlin query language and those using the Cypher
query language, generating approximately 4,000 queries each. In the latest
versions of Neo4j, Agensgraph, and JanusGraph databases, we detected 2, 5, and
3 wrong-result bugs, respectively.","Jiayi Wu, Zhengyu Wu, Ronghua Li, Hongchao Qin, Guoren Wang","Effective Bug Detection in Graph Database Engines: An LLM-based Approach. Graph database engines play a pivotal role in efficiently storing and
managing graph data across various domains, including bioinformatics, knowledge
graphs, and recommender systems. Ensuring data accuracy within graph database
engines is paramount, as inaccuracies can yield unreliable analytical outcomes.
Current bug-detection approaches are confined to specific graph query
languages, limiting their applicabilities when handling graph database engines
that use various graph query languages across various domains. Moreover, they
require extensive prior knowledge to generate queries for detecting bugs. To
address these challenges, we introduces DGDB, a novel paradigm harnessing large
language models(LLM), such as ChatGPT, for comprehensive bug detection in graph
database engines. DGDB leverages ChatGPT to generate high-quality queries for
different graph query languages. It subsequently employs differential testing
to identify bugs in graph database engines. We applied this paradigm to graph
database engines using the Gremlin query language and those using the Cypher
query language, generating approximately 4,000 queries each. In the latest
versions of Neo4j, Agensgraph, and JanusGraph databases, we detected 2, 5, and
3 wrong-result bugs, respectively.","DGDB, Bug Detection, LLM, Gremlin, Cypher, Neo4j, Agensgraph, JanusGraph"
"A Systematic Overview of Single-Cell Transcriptomics Databases, their
  Use cases, and Limitations","Rapid advancements in high-throughput single-cell RNA-seq (scRNA-seq)
technologies and experimental protocols have led to the generation of vast
amounts of genomic data that populates several online databases and
repositories. Here, we systematically examined large-scale scRNA-seq databases,
categorizing them based on their scope and purpose such as general,
tissue-specific databases, disease-specific databases, cancer-focused
databases, and cell type-focused databases. Next, we discuss the technical and
methodological challenges associated with curating large-scale scRNA-seq
databases, along with current computational solutions. We argue that
understanding scRNA-seq databases, including their limitations and assumptions,
is crucial for effectively utilizing this data to make robust discoveries and
identify novel biological insights. Furthermore, we propose that bridging the
gap between computational and wet lab scientists through user-friendly
web-based platforms is needed for democratizing access to single-cell data.
These platforms would facilitate interdisciplinary research, enabling
researchers from various disciplines to collaborate effectively. This review
underscores the importance of leveraging computational approaches to unravel
the complexities of single-cell data and offers a promising direction for
future research in the field.","Mahnoor N. Gondal, Saad Ur Rehman Shah, Arul M. Chinnaiyan, Marcin Cieslik","A Systematic Overview of Single-Cell Transcriptomics Databases, their
  Use cases, and Limitations. Rapid advancements in high-throughput single-cell RNA-seq (scRNA-seq)
technologies and experimental protocols have led to the generation of vast
amounts of genomic data that populates several online databases and
repositories. Here, we systematically examined large-scale scRNA-seq databases,
categorizing them based on their scope and purpose such as general,
tissue-specific databases, disease-specific databases, cancer-focused
databases, and cell type-focused databases. Next, we discuss the technical and
methodological challenges associated with curating large-scale scRNA-seq
databases, along with current computational solutions. We argue that
understanding scRNA-seq databases, including their limitations and assumptions,
is crucial for effectively utilizing this data to make robust discoveries and
identify novel biological insights. Furthermore, we propose that bridging the
gap between computational and wet lab scientists through user-friendly
web-based platforms is needed for democratizing access to single-cell data.
These platforms would facilitate interdisciplinary research, enabling
researchers from various disciplines to collaborate effectively. This review
underscores the importance of leveraging computational approaches to unravel
the complexities of single-cell data and offers a promising direction for
future research in the field.","Single-Cell Transcriptomics, ScRNA-seq, Large-Scale Databases, Computational Solutions"
"Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of
  MongoDB, Redis, Kudu and ArangoDB","In the era of big data, conventional RDBMS models have become impractical for
handling colossal workloads. Consequently, NoSQL databases have emerged as the
preferred storage solutions for executing processing-intensive Online
Analytical Processing (OLAP) tasks. Within the realm of NoSQL databases,
various classifications exist based on their data storage mechanisms, making it
challenging to select the most suitable one for a given OLAP workload. While
each NoSQL database boasts distinct advantages, inherent scalability,
adaptability to diverse data formats, and high data availability are
universally recognized benefits crucial for managing OLAP workloads
effectively. Existing research predominantly evaluates individual databases
within custom data pipeline setups, lacking a standardized approach for
comparative analysis across different databases to identify the optimal data
pipeline for OLAP workloads. In this paper, we present our experimental
insights into how various NoSQL databases handle OLAP workloads within a
standardized data processing pipeline. Our experimental pipeline comprises
Apache Spark for large-scale transformations, data cleansing, and schema
normalization, diverse NoSQL databases as data stores, and a Business
Intelligence tool for data analysis and visualization.","Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, Nisha Ramasubramanian","Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of
  MongoDB, Redis, Kudu and ArangoDB. In the era of big data, conventional RDBMS models have become impractical for
handling colossal workloads. Consequently, NoSQL databases have emerged as the
preferred storage solutions for executing processing-intensive Online
Analytical Processing (OLAP) tasks. Within the realm of NoSQL databases,
various classifications exist based on their data storage mechanisms, making it
challenging to select the most suitable one for a given OLAP workload. While
each NoSQL database boasts distinct advantages, inherent scalability,
adaptability to diverse data formats, and high data availability are
universally recognized benefits crucial for managing OLAP workloads
effectively. Existing research predominantly evaluates individual databases
within custom data pipeline setups, lacking a standardized approach for
comparative analysis across different databases to identify the optimal data
pipeline for OLAP workloads. In this paper, we present our experimental
insights into how various NoSQL databases handle OLAP workloads within a
standardized data processing pipeline. Our experimental pipeline comprises
Apache Spark for large-scale transformations, data cleansing, and schema
normalization, diverse NoSQL databases as data stores, and a Business
Intelligence tool for data analysis and visualization.","NoSQL Databases, OLAP, MongoDB, Redis, Kudu, ArangoDB, Apache Spark, Business Intelligence"
From Text to Databases: attribute grammar as database meta-model,"We present a general methodology for structuring textual data, represented as
syntax trees enriched with semantic information, guided by a meta-model G
defined as an attribute grammar. The method involves an evolution process where
both the instance and its grammar evolve, with instance transformations guided
by rewriting rules and a similarity measure. Each new instance generates a
corresponding grammar, culminating in a target grammar GT that satisfies G.
  This methodology is applied to build a database populated from textual data.
The process generates both a database schema and its instance, independent of
specific database models. We demonstrate the approach using clinical medical
cases, where trees represent database instances and grammars act as database
schemas. Key contributions include the proposal of a general attribute grammar
G, a formalization of grammar evolution, and a proof-of-concept implementation
for database structuring.","Jacques Chabin, Mirian Halfeld-Ferrari, Nicolas Hiot","From Text to Databases: attribute grammar as database meta-model. We present a general methodology for structuring textual data, represented as
syntax trees enriched with semantic information, guided by a meta-model G
defined as an attribute grammar. The method involves an evolution process where
both the instance and its grammar evolve, with instance transformations guided
by rewriting rules and a similarity measure. Each new instance generates a
corresponding grammar, culminating in a target grammar GT that satisfies G.
  This methodology is applied to build a database populated from textual data.
The process generates both a database schema and its instance, independent of
specific database models. We demonstrate the approach using clinical medical
cases, where trees represent database instances and grammars act as database
schemas. Key contributions include the proposal of a general attribute grammar
G, a formalization of grammar evolution, and a proof-of-concept implementation
for database structuring.","Attribute Grammar, Database, Meta-model, Grammar Evolution"
"Lua API and benchmark design using 3n+1 sequences: Comparing API
  elegance and raw speed in Redis and YottaDB databases","Elegance of a database API matters. Frequently, database APIs suit the
database designer, rather than the programmer's desire for elegance and
efficiency. This article pursues both: firstly, by comparing the Lua APIs for
two separate databases, Redis and YottaDB. Secondly, it looks under the API
covers at how object orientation can help to retain API efficiency. Finally, it
benchmarks both databases using each API to implement a 3n+1 sequence generator
(of Collatz Conjecture fame). It covers the eccentricities of the Lua APIs, the
databases, and the nifty choice of benchmark tool, presenting benchmark results
of each database's unique design.",Berwyn Hoyt,"Lua API and benchmark design using 3n+1 sequences: Comparing API
  elegance and raw speed in Redis and YottaDB databases. Elegance of a database API matters. Frequently, database APIs suit the
database designer, rather than the programmer's desire for elegance and
efficiency. This article pursues both: firstly, by comparing the Lua APIs for
two separate databases, Redis and YottaDB. Secondly, it looks under the API
covers at how object orientation can help to retain API efficiency. Finally, it
benchmarks both databases using each API to implement a 3n+1 sequence generator
(of Collatz Conjecture fame). It covers the eccentricities of the Lua APIs, the
databases, and the nifty choice of benchmark tool, presenting benchmark results
of each database's unique design.","Lua API, 3n+1 sequences, Redis, YottaDB, Collatz Conjecture"
"An Ehrenfeucht-Fraisse Game Approach to Collapse Results in Database
  Theory","We present a new Ehrenfeucht-Fraisse game approach to collapse results in
database theory and we show that, in principle, this approach suffices to prove
every natural generic collapse result. Following this approach we can deal with
certain infinite databases where previous, highly involved methods fail. We
prove the natural generic collapse for Z-embeddable databases over any linearly
ordered context structure with arbitrary monadic predicates, and for
N-embeddable databases over the context structure (R,<,+,Mon_Q,Groups). Here,
N, Z, R, denote the sets of natural numbers, integers, and real numbers,
respectively. Groups is the collection of all subgroups of (R,+) that contain
Z, and Mon_Q is the collection of all subsets of a particular infinite subset Q
of N. Restricting the complexity of the formulas that may be used to formulate
queries to Boolean combinations of purely existential first-order formulas, we
even obtain the collapse for N-embeddable databases over any linearly ordered
context structure with arbitrary predicates. Finally, we develop the notion of
N-representable databases, which is a natural generalization of the classical
notion of finitely representable databases. We show that natural generic
collapse results for N-embeddable databases can be lifted to the larger class
of N-representable databases. To obtain, in particular, the collapse result for
(N,<,+,Mon_Q), we explicitly construct a winning strategy for the duplicator in
the presence of the built-in addition relation +. This, as a side product, also
leads to an Ehrenfeucht-Fraisse game proof of the theorem of Ginsburg and
Spanier, stating that the spectra of FO(<,+)-sentences are semi-linear.",Nicole Schweikardt,"An Ehrenfeucht-Fraisse Game Approach to Collapse Results in Database
  Theory. We present a new Ehrenfeucht-Fraisse game approach to collapse results in
database theory and we show that, in principle, this approach suffices to prove
every natural generic collapse result. Following this approach we can deal with
certain infinite databases where previous, highly involved methods fail. We
prove the natural generic collapse for Z-embeddable databases over any linearly
ordered context structure with arbitrary monadic predicates, and for
N-embeddable databases over the context structure (R,<,+,Mon_Q,Groups). Here,
N, Z, R, denote the sets of natural numbers, integers, and real numbers,
respectively. Groups is the collection of all subgroups of (R,+) that contain
Z, and Mon_Q is the collection of all subsets of a particular infinite subset Q
of N. Restricting the complexity of the formulas that may be used to formulate
queries to Boolean combinations of purely existential first-order formulas, we
even obtain the collapse for N-embeddable databases over any linearly ordered
context structure with arbitrary predicates. Finally, we develop the notion of
N-representable databases, which is a natural generalization of the classical
notion of finitely representable databases. We show that natural generic
collapse results for N-embeddable databases can be lifted to the larger class
of N-representable databases. To obtain, in particular, the collapse result for
(N,<,+,Mon_Q), we explicitly construct a winning strategy for the duplicator in
the presence of the built-in addition relation +. This, as a side product, also
leads to an Ehrenfeucht-Fraisse game proof of the theorem of Ginsburg and
Spanier, stating that the spectra of FO(<,+)-sentences are semi-linear.","Database Theory, Ehrenfeucht-Fraisse Game, Natural Generic Collapse, N-embeddable Databases, Z Embeddable, Monadic, Groups"
"BB-Graph: A Subgraph Isomorphism Algorithm for Efficiently Querying Big
  Graph Databases","The big graph database model provides strong modeling for complex
applications and efficient querying. However, it is still a big challenge to
find all exact matches of a query graph in a big graph database, which is known
as the subgraph isomorphism problem. The current subgraph isomorphism
approaches are built on Ullmann's idea of focusing on the strategy of pruning
out the irrelevant candidates. Nevertheless, the existing pruning techniques
need much more improvement to efficiently handle complex queries. Moreover,
many of those existing algorithms need large indices requiring extra memory
consumption. Motivated by these, we introduce a new subgraph isomorphism
algorithm, named as BB-Graph, for querying big graph databases efficiently
without requiring a large data structure to be stored in main memory. We test
and compare our proposed BB-Graph algorithm with two popular existing
approaches, GraphQL and Cypher. Our experiments are done on three different
data sets; (1) a very big graph database of a real-life population database,
(2) a graph database of a simulated bank database, and (3) the publicly
available World Cup big graph database. We show that our solution performs
better than those algorithms mentioned here for most of the query types
experimented on these big databases.","Merve Asiler, Adnan Yazıcı","BB-Graph: A Subgraph Isomorphism Algorithm for Efficiently Querying Big
  Graph Databases. The big graph database model provides strong modeling for complex
applications and efficient querying. However, it is still a big challenge to
find all exact matches of a query graph in a big graph database, which is known
as the subgraph isomorphism problem. The current subgraph isomorphism
approaches are built on Ullmann's idea of focusing on the strategy of pruning
out the irrelevant candidates. Nevertheless, the existing pruning techniques
need much more improvement to efficiently handle complex queries. Moreover,
many of those existing algorithms need large indices requiring extra memory
consumption. Motivated by these, we introduce a new subgraph isomorphism
algorithm, named as BB-Graph, for querying big graph databases efficiently
without requiring a large data structure to be stored in main memory. We test
and compare our proposed BB-Graph algorithm with two popular existing
approaches, GraphQL and Cypher. Our experiments are done on three different
data sets; (1) a very big graph database of a real-life population database,
(2) a graph database of a simulated bank database, and (3) the publicly
available World Cup big graph database. We show that our solution performs
better than those algorithms mentioned here for most of the query types
experimented on these big databases.","BB-Graph, Subgraph Isomorphism Algorithm, Big Graph Databases, GraphQL, Cypher"
"Cognitive Database: A Step towards Endowing Relational Databases with
  Artificial Intelligence Capabilities","We propose Cognitive Databases, an approach for transparently enabling
Artificial Intelligence (AI) capabilities in relational databases. A novel
aspect of our design is to first view the structured data source as meaningful
unstructured text, and then use the text to build an unsupervised neural
network model using a Natural Language Processing (NLP) technique called word
embedding. This model captures the hidden inter-/intra-column relationships
between database tokens of different types. For each database token, the model
includes a vector that encodes contextual semantic relationships. We seamlessly
integrate the word embedding model into existing SQL query infrastructure and
use it to enable a new class of SQL-based analytics queries called cognitive
intelligence (CI) queries. CI queries use the model vectors to enable complex
queries such as semantic matching, inductive reasoning queries such as
analogies, predictive queries using entities not present in a database, and,
more generally, using knowledge from external sources. We demonstrate unique
capabilities of Cognitive Databases using an Apache Spark based prototype to
execute inductive reasoning CI queries over a multi-modal database containing
text and images. We believe our first-of-a-kind system exemplifies using AI
functionality to endow relational databases with capabilities that were
previously very hard to realize in practice.","Rajesh Bordawekar, Bortik Bandyopadhyay, Oded Shmueli","Cognitive Database: A Step towards Endowing Relational Databases with
  Artificial Intelligence Capabilities. We propose Cognitive Databases, an approach for transparently enabling
Artificial Intelligence (AI) capabilities in relational databases. A novel
aspect of our design is to first view the structured data source as meaningful
unstructured text, and then use the text to build an unsupervised neural
network model using a Natural Language Processing (NLP) technique called word
embedding. This model captures the hidden inter-/intra-column relationships
between database tokens of different types. For each database token, the model
includes a vector that encodes contextual semantic relationships. We seamlessly
integrate the word embedding model into existing SQL query infrastructure and
use it to enable a new class of SQL-based analytics queries called cognitive
intelligence (CI) queries. CI queries use the model vectors to enable complex
queries such as semantic matching, inductive reasoning queries such as
analogies, predictive queries using entities not present in a database, and,
more generally, using knowledge from external sources. We demonstrate unique
capabilities of Cognitive Databases using an Apache Spark based prototype to
execute inductive reasoning CI queries over a multi-modal database containing
text and images. We believe our first-of-a-kind system exemplifies using AI
functionality to endow relational databases with capabilities that were
previously very hard to realize in practice.","Cognitive Database, Relational Databases, Artificial Intelligence, NLP, Apache Spark"
Reconciling Inconsistent Molecular Structures from Biochemical Databases,"Information on the structure of molecules, retrieved via biochemical
databases, plays a pivotal role in various disciplines, such as metabolomics,
systems biology, and drug discovery. However, no such database can be complete,
and the chemical structure for a given compound is not necessarily consistent
between databases. This paper presents StructRecon, a novel tool for resolving
unique and correct molecular structures from database identifiers. StructRecon
traverses the cross-links between database entries in different databases to
construct what we call an identifier graph, which offers a more complete view
of the total information available on a particular compound across all the
databases. In order to reconcile discrepancies between databases, we first
present an extensible model for chemical structure which supports multiple
independent levels of detail, allowing standardisation of the structure to be
applied iteratively. In some cases, our standardisation approach results in
multiple structures for a given compound, in which case a random walk-based
algorithm is used to select the most likely structure among incompatible
alternates. We applied StructRecon to the EColiCore2 model, resolving a unique
chemical structure for 85.11 % of identifiers. StructRecon is open-source and
modular, which enables the potential support for more databases in the future.","Casper Asbjørn Eriksen, Jakob Lykke Andersen, Rolf Fagerberg, Daniel Merkle","Reconciling Inconsistent Molecular Structures from Biochemical Databases. Information on the structure of molecules, retrieved via biochemical
databases, plays a pivotal role in various disciplines, such as metabolomics,
systems biology, and drug discovery. However, no such database can be complete,
and the chemical structure for a given compound is not necessarily consistent
between databases. This paper presents StructRecon, a novel tool for resolving
unique and correct molecular structures from database identifiers. StructRecon
traverses the cross-links between database entries in different databases to
construct what we call an identifier graph, which offers a more complete view
of the total information available on a particular compound across all the
databases. In order to reconcile discrepancies between databases, we first
present an extensible model for chemical structure which supports multiple
independent levels of detail, allowing standardisation of the structure to be
applied iteratively. In some cases, our standardisation approach results in
multiple structures for a given compound, in which case a random walk-based
algorithm is used to select the most likely structure among incompatible
alternates. We applied StructRecon to the EColiCore2 model, resolving a unique
chemical structure for 85.11 % of identifiers. StructRecon is open-source and
modular, which enables the potential support for more databases in the future.","StructRecon, Molecular Structures, Biochemical Databases, EColiCore2, Metabolomics, Systems Biology, Drug Discovery"
Relational Database Augmented Large Language Model,"Large language models (LLMs) excel in many natural language processing (NLP)
tasks. However, since LLMs can only incorporate new knowledge through training
or supervised fine-tuning processes, they are unsuitable for applications that
demand precise, up-to-date, and private information not available in the
training corpora. This precise, up-to-date, and private information is
typically stored in relational databases. Thus, a promising solution is to
augment LLMs with the inclusion of relational databases as external memory.
This can ensure the timeliness, correctness, and consistency of data, and
assist LLMs in performing complex arithmetic operations beyond their inherent
capabilities. However, bridging the gap between LLMs and relational databases
is challenging. It requires the awareness of databases and data values stored
in databases to select correct databases and issue correct SQL queries.
Besides, it is necessary for the external memory to be independent of the LLM
to meet the needs of real-world applications. We introduce a novel LLM-agnostic
memory architecture comprising a database selection memory, a data value
memory, and relational databases. And we design an elegant pipeline to retrieve
information from it. Besides, we carefully design the prompts to instruct the
LLM to maximize the framework's potential. To evaluate our method, we compose a
new dataset with various types of questions. Experimental results show that our
framework enables LLMs to effectively answer database-related questions, which
is beyond their direct ability.","Zongyue Qin, Chen Luo, Zhengyang Wang, Haoming Jiang, Yizhou Sun","Relational Database Augmented Large Language Model. Large language models (LLMs) excel in many natural language processing (NLP)
tasks. However, since LLMs can only incorporate new knowledge through training
or supervised fine-tuning processes, they are unsuitable for applications that
demand precise, up-to-date, and private information not available in the
training corpora. This precise, up-to-date, and private information is
typically stored in relational databases. Thus, a promising solution is to
augment LLMs with the inclusion of relational databases as external memory.
This can ensure the timeliness, correctness, and consistency of data, and
assist LLMs in performing complex arithmetic operations beyond their inherent
capabilities. However, bridging the gap between LLMs and relational databases
is challenging. It requires the awareness of databases and data values stored
in databases to select correct databases and issue correct SQL queries.
Besides, it is necessary for the external memory to be independent of the LLM
to meet the needs of real-world applications. We introduce a novel LLM-agnostic
memory architecture comprising a database selection memory, a data value
memory, and relational databases. And we design an elegant pipeline to retrieve
information from it. Besides, we carefully design the prompts to instruct the
LLM to maximize the framework's potential. To evaluate our method, we compose a
new dataset with various types of questions. Experimental results show that our
framework enables LLMs to effectively answer database-related questions, which
is beyond their direct ability.","Large Language Models, LLMs, Relational Database, NLP"
"Revolutionizing Database Q&A with Large Language Models: Comprehensive
  Benchmark and Evaluation","The development of Large Language Models (LLMs) has revolutionized QA across
various industries, including the database domain. However, there is still a
lack of a comprehensive benchmark to evaluate the capabilities of different
LLMs and their modular components in database QA. To this end, we introduce
DQABench, the first comprehensive database QA benchmark for LLMs. DQABench
features an innovative LLM-based method to automate the generation, cleaning,
and rewriting of evaluation dataset, resulting in over 200,000 QA pairs in
English and Chinese, separately. These QA pairs cover a wide range of
database-related knowledge extracted from manuals, online communities, and
database instances. This inclusion allows for an additional assessment of LLMs'
Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG)
capabilities in the database QA task. Furthermore, we propose a comprehensive
LLM-based database QA testbed DQATestbed. This testbed is highly modular and
scalable, with basic and advanced components such as Question Classification
Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Moreover,
DQABench provides a comprehensive evaluation pipeline that computes various
metrics throughout a standardized evaluation process to ensure the accuracy and
fairness of the evaluation. We use DQABench to evaluate the database QA
capabilities under the proposed testbed comprehensively. The evaluation reveals
findings like (i) the strengths and limitations of nine LLM-based QA bots and
(ii) the performance impact and potential improvements of various service
components (e.g., QCR, RAG, TIG). Our benchmark and findings will guide the
future development of LLM-based database QA research.","Yihang Zheng, Bo Li, Zhenghao Lin, Yi Luo, Xuanhe Zhou, Chen Lin, Jinsong Su, Guoliang Li, Shifu Li","Revolutionizing Database Q&A with Large Language Models: Comprehensive
  Benchmark and Evaluation. The development of Large Language Models (LLMs) has revolutionized QA across
various industries, including the database domain. However, there is still a
lack of a comprehensive benchmark to evaluate the capabilities of different
LLMs and their modular components in database QA. To this end, we introduce
DQABench, the first comprehensive database QA benchmark for LLMs. DQABench
features an innovative LLM-based method to automate the generation, cleaning,
and rewriting of evaluation dataset, resulting in over 200,000 QA pairs in
English and Chinese, separately. These QA pairs cover a wide range of
database-related knowledge extracted from manuals, online communities, and
database instances. This inclusion allows for an additional assessment of LLMs'
Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG)
capabilities in the database QA task. Furthermore, we propose a comprehensive
LLM-based database QA testbed DQATestbed. This testbed is highly modular and
scalable, with basic and advanced components such as Question Classification
Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Moreover,
DQABench provides a comprehensive evaluation pipeline that computes various
metrics throughout a standardized evaluation process to ensure the accuracy and
fairness of the evaluation. We use DQABench to evaluate the database QA
capabilities under the proposed testbed comprehensively. The evaluation reveals
findings like (i) the strengths and limitations of nine LLM-based QA bots and
(ii) the performance impact and potential improvements of various service
components (e.g., QCR, RAG, TIG). Our benchmark and findings will guide the
future development of LLM-based database QA research.","Large Language Models, LLMs, Database QA, Benchmark, Evaluation"
SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark,"Electronic health records (EHRs) are stored in various database systems with
different database models on heterogeneous storage architectures, such as
relational databases, document stores, or graph databases. These different
database models have a big impact on query complexity and performance. While
this has been a known fact in database research, its implications for the
growing number of Text-to-Query systems have surprisingly not been investigated
so far. In this paper, we present SM3-Text-to-Query, the first multi-model
medical Text-to-Query benchmark based on synthetic patient data from Synthea,
following the SNOMED-CT taxonomy -- a widely used knowledge graph ontology
covering medical terminology. SM3-Text-to-Query provides data representations
for relational databases (PostgreSQL), document stores (MongoDB), and graph
databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four
popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically
and manually develop 408 template questions, which we augment to construct a
benchmark of 10K diverse natural language question/query pairs for these four
query languages (40K pairs overall). On our dataset, we evaluate several common
in-context-learning (ICL) approaches for a set of representative closed and
open-source LLMs. Our evaluation sheds light on the trade-offs between database
models and query languages for different ICL strategies and LLMs. Last,
SM3-Text-to-Query is easily extendable to additional query languages or real,
standard-based patient databases.","Sithursan Sivasubramaniam, Cedric Osei-Akoto, Yi Zhang, Kurt Stockinger, Jonathan Fuerst","SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark. Electronic health records (EHRs) are stored in various database systems with
different database models on heterogeneous storage architectures, such as
relational databases, document stores, or graph databases. These different
database models have a big impact on query complexity and performance. While
this has been a known fact in database research, its implications for the
growing number of Text-to-Query systems have surprisingly not been investigated
so far. In this paper, we present SM3-Text-to-Query, the first multi-model
medical Text-to-Query benchmark based on synthetic patient data from Synthea,
following the SNOMED-CT taxonomy -- a widely used knowledge graph ontology
covering medical terminology. SM3-Text-to-Query provides data representations
for relational databases (PostgreSQL), document stores (MongoDB), and graph
databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four
popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically
and manually develop 408 template questions, which we augment to construct a
benchmark of 10K diverse natural language question/query pairs for these four
query languages (40K pairs overall). On our dataset, we evaluate several common
in-context-learning (ICL) approaches for a set of representative closed and
open-source LLMs. Our evaluation sheds light on the trade-offs between database
models and query languages for different ICL strategies and LLMs. Last,
SM3-Text-to-Query is easily extendable to additional query languages or real,
standard-based patient databases.","SM3-Text-to-Query, Synthetic Multi-Model, Database, EHR, MongoDB, Neo4j, GraphDB, SPARQL, SNOMED-CT"
GaussMaster: An LLM-based Database Copilot System,"In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.","Wei Zhou, Ji Sun, Xuanhe Zhou, Guoliang Li, Luyang Liu, Hao Wu, Tianyuan Wang","GaussMaster: An LLM-based Database Copilot System. In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.","GaussMaster, LLM, Database Copilot System, Automation, NL2SQL, Anomaly Detection, Service Repair"
Lock Prediction for Zero-Downtime Database Encryption,"Modern enterprise database systems face significant challenges in balancing
data security and performance. Ensuring robust encryption for sensitive
information is critical for systems' compliance with security standards.
Although holistic database encryption provides strong protection, existing
database systems often require a complete backup and restore cycle, resulting
in prolonged downtime and increased storage usage. This makes it difficult to
implement online encryption techniques in high-throughput environments without
disrupting critical operations.
  To address this challenge, we envision a solution that enables online
database encryption aligned with system activity, eliminating the need for
downtime, storage overhead, or full-database reprocessing. Central to this
vision is the ability to predict which parts of the database will be accessed
next, allowing encryption to be applied online. As a step towards this
solution, this study proposes a predictive approach that leverages deep
learning models to forecast database lock sequences, using IBM Db2 as the
database system under study. In this study, we collected a specialized dataset
from TPC-C benchmark workloads, leveraging lock event logs for model training
and evaluation. We applied deep learning architectures, such as Transformer and
LSTM, to evaluate models for various table-level and page-level lock
predictions. We benchmark the accuracy of the trained models versus a Naive
Baseline across different prediction horizons and timelines.
  The study experiments demonstrate that the proposed deep learning-based
models achieve up to 49% average accuracy for table-level and 66% for
page-level predictions, outperforming a Naive Baseline. By anticipating which
tables and pages will be locked next, the proposed approach is a step toward
online encryption, offering a practical path toward secure, low-overhead
database systems.","Mohamed Sami Rakha, Adam Sorrenti, Greg Stager, Walid Rjaibi, Andriy Miranskyy","Lock Prediction for Zero-Downtime Database Encryption. Modern enterprise database systems face significant challenges in balancing
data security and performance. Ensuring robust encryption for sensitive
information is critical for systems' compliance with security standards.
Although holistic database encryption provides strong protection, existing
database systems often require a complete backup and restore cycle, resulting
in prolonged downtime and increased storage usage. This makes it difficult to
implement online encryption techniques in high-throughput environments without
disrupting critical operations.
  To address this challenge, we envision a solution that enables online
database encryption aligned with system activity, eliminating the need for
downtime, storage overhead, or full-database reprocessing. Central to this
vision is the ability to predict which parts of the database will be accessed
next, allowing encryption to be applied online. As a step towards this
solution, this study proposes a predictive approach that leverages deep
learning models to forecast database lock sequences, using IBM Db2 as the
database system under study. In this study, we collected a specialized dataset
from TPC-C benchmark workloads, leveraging lock event logs for model training
and evaluation. We applied deep learning architectures, such as Transformer and
LSTM, to evaluate models for various table-level and page-level lock
predictions. We benchmark the accuracy of the trained models versus a Naive
Baseline across different prediction horizons and timelines.
  The study experiments demonstrate that the proposed deep learning-based
models achieve up to 49% average accuracy for table-level and 66% for
page-level predictions, outperforming a Naive Baseline. By anticipating which
tables and pages will be locked next, the proposed approach is a step toward
online encryption, offering a practical path toward secure, low-overhead
database systems.","Lock Prediction, Zero-Downtime, Database Encryption, Deep Learning, IBM Db2, TPC-C, Transformer, LSTM"
Automatic Classification of Text Databases through Query Probing,"Many text databases on the web are ""hidden"" behind search interfaces, and
their documents are only accessible through querying. Search engines typically
ignore the contents of such search-only databases. Recently, Yahoo-like
directories have started to manually organize these databases into categories
that users can browse to find these valuable resources. We propose a novel
strategy to automate the classification of search-only text databases. Our
technique starts by training a rule-based document classifier, and then uses
the classifier's rules to generate probing queries. The queries are sent to the
text databases, which are then classified based on the number of matches that
they produce for each query. We report some initial exploratory experiments
that show that our approach is promising to automatically characterize the
contents of text databases accessible on the web.","Panagiotis Ipeirotis, Luis Gravano, Mehran Sahami","Automatic Classification of Text Databases through Query Probing. Many text databases on the web are ""hidden"" behind search interfaces, and
their documents are only accessible through querying. Search engines typically
ignore the contents of such search-only databases. Recently, Yahoo-like
directories have started to manually organize these databases into categories
that users can browse to find these valuable resources. We propose a novel
strategy to automate the classification of search-only text databases. Our
technique starts by training a rule-based document classifier, and then uses
the classifier's rules to generate probing queries. The queries are sent to the
text databases, which are then classified based on the number of matches that
they produce for each query. We report some initial exploratory experiments
that show that our approach is promising to automatically characterize the
contents of text databases accessible on the web.","Automatic Classification, Text Databases, Query Probing"
"Repairing Inconsistent Databases: A Model-Theoretic Approach and
  Abductive Reasoning","In this paper we consider two points of views to the problem of coherent
integration of distributed data. First we give a pure model-theoretic analysis
of the possible ways to `repair' a database. We do so by characterizing the
possibilities to `recover' consistent data from an inconsistent database in
terms of those models of the database that exhibit as minimal inconsistent
information as reasonably possible. Then we introduce an abductive application
to restore the consistency of a given database. This application is based on an
abductive solver (A-system) that implements an SLDNFA-resolution procedure, and
computes a list of data-facts that should be inserted to the database or
retracted from it in order to keep the database consistent. The two approaches
for coherent data integration are related by soundness and completeness
results.","Ofer Arieli, Marc Denecker, Bert Van Nuffelen, Maurice Bruynooghe","Repairing Inconsistent Databases: A Model-Theoretic Approach and
  Abductive Reasoning. In this paper we consider two points of views to the problem of coherent
integration of distributed data. First we give a pure model-theoretic analysis
of the possible ways to `repair' a database. We do so by characterizing the
possibilities to `recover' consistent data from an inconsistent database in
terms of those models of the database that exhibit as minimal inconsistent
information as reasonably possible. Then we introduce an abductive application
to restore the consistency of a given database. This application is based on an
abductive solver (A-system) that implements an SLDNFA-resolution procedure, and
computes a list of data-facts that should be inserted to the database or
retracted from it in order to keep the database consistent. The two approaches
for coherent data integration are related by soundness and completeness
results.","Model-Theoretic, Abductive Reasoning, Data Integration, SLDNFA"
"The Sloan Digital Sky Survey Science Archive: Migrating a Multi-Terabyte
  Astronomical Archive from Object to Relational DBMS","The Sloan Digital Sky Survey Science Archive is the first in a series of
multi-Terabyte digital archives in Astronomy and other data-intensive sciences.
To facilitate data mining in the SDSS archive, we adapted a commercial database
engine and built specialized tools on top of it. Originally we chose an
object-oriented database management system due to its data organization
capabilities, platform independence, query performance and conceptual fit to
the data. However, after using the object database for the first couple of
years of the project, it soon began to fall short in terms of its query support
and data mining performance. This was as much due to the inability of the
database vendor to respond our demands for features and bug fixes as it was due
to their failure to keep up with the rapid improvements in hardware
performance, particularly faster RAID disk systems. In the end, we were forced
to abandon the object database and migrate our data to a relational database.
We describe below the technical issues that we faced with the object database
and how and why we migrated to relational technology.","Aniruddha R. Thakar, Alexander S. Szalay, Peter Z. Kunszt, Jim Gray","The Sloan Digital Sky Survey Science Archive: Migrating a Multi-Terabyte
  Astronomical Archive from Object to Relational DBMS. The Sloan Digital Sky Survey Science Archive is the first in a series of
multi-Terabyte digital archives in Astronomy and other data-intensive sciences.
To facilitate data mining in the SDSS archive, we adapted a commercial database
engine and built specialized tools on top of it. Originally we chose an
object-oriented database management system due to its data organization
capabilities, platform independence, query performance and conceptual fit to
the data. However, after using the object database for the first couple of
years of the project, it soon began to fall short in terms of its query support
and data mining performance. This was as much due to the inability of the
database vendor to respond our demands for features and bug fixes as it was due
to their failure to keep up with the rapid improvements in hardware
performance, particularly faster RAID disk systems. In the end, we were forced
to abandon the object database and migrate our data to a relational database.
We describe below the technical issues that we faced with the object database
and how and why we migrated to relational technology.","SDSS, Relational DBMS, Data Mining"
"Complexity of Consistent Query Answering in Databases under
  Cardinality-Based and Incremental Repair Semantics","Consistent Query Answering (CQA) is the problem of computing from a database
the answers to a query that are consistent with respect to certain integrity
constraints that the database, as a whole, may fail to satisfy. Consistent
answers have been characterized as those that are invariant under certain
minimal forms of restoration of the database consistency. We investigate
algorithmic and complexity theoretic issues of CQA under database repairs that
minimally depart -wrt the cardinality of the symmetric difference- from the
original database. We obtain first tight complexity bounds.
  We also address the problem of incremental complexity of CQA, that naturally
occurs when an originally consistent database becomes inconsistent after the
execution of a sequence of update operations. Tight bounds on incremental
complexity are provided for various semantics under denial constraints. Fixed
parameter tractability is also investigated in this dynamic context, where the
size of the update sequence becomes the relevant parameter.","Andrei Lopatenko, Leopoldo Bertossi","Complexity of Consistent Query Answering in Databases under
  Cardinality-Based and Incremental Repair Semantics. Consistent Query Answering (CQA) is the problem of computing from a database
the answers to a query that are consistent with respect to certain integrity
constraints that the database, as a whole, may fail to satisfy. Consistent
answers have been characterized as those that are invariant under certain
minimal forms of restoration of the database consistency. We investigate
algorithmic and complexity theoretic issues of CQA under database repairs that
minimally depart -wrt the cardinality of the symmetric difference- from the
original database. We obtain first tight complexity bounds.
  We also address the problem of incremental complexity of CQA, that naturally
occurs when an originally consistent database becomes inconsistent after the
execution of a sequence of update operations. Tight bounds on incremental
complexity are provided for various semantics under denial constraints. Fixed
parameter tractability is also investigated in this dynamic context, where the
size of the update sequence becomes the relevant parameter.","Consistent Query Answering, CQA, Databases, Cardinality-Based, Incremental Repair, Denial Constraints"
Semantically Correct Query Answers in the Presence of Null Values,"For several reasons a database may not satisfy a given set of integrity
constraints(ICs), but most likely most of the information in it is still
consistent with those ICs; and could be retrieved when queries are answered.
Consistent answers to queries wrt a set of ICs have been characterized as
answers that can be obtained from every possible minimally repaired consistent
version of the original database. In this paper we consider databases that
contain null values and are also repaired, if necessary, using null values. For
this purpose, we propose first a precise semantics for IC satisfaction in a
database with null values that is compatible with the way null values are
treated in commercial database management systems. Next, a precise notion of
repair is introduced that privileges the introduction of null values when
repairing foreign key constraints, in such a way that these new values do not
create an infinite cycle of new inconsistencies. Finally, we analyze how to
specify this kind of repairs of a database that contains null values using
disjunctive logic programs with stable model semantics.","Loreto Bravo, Leopoldo Bertossi","Semantically Correct Query Answers in the Presence of Null Values. For several reasons a database may not satisfy a given set of integrity
constraints(ICs), but most likely most of the information in it is still
consistent with those ICs; and could be retrieved when queries are answered.
Consistent answers to queries wrt a set of ICs have been characterized as
answers that can be obtained from every possible minimally repaired consistent
version of the original database. In this paper we consider databases that
contain null values and are also repaired, if necessary, using null values. For
this purpose, we propose first a precise semantics for IC satisfaction in a
database with null values that is compatible with the way null values are
treated in commercial database management systems. Next, a precise notion of
repair is introduced that privileges the introduction of null values when
repairing foreign key constraints, in such a way that these new values do not
create an infinite cycle of new inconsistencies. Finally, we analyze how to
specify this kind of repairs of a database that contains null values using
disjunctive logic programs with stable model semantics.","Null Values, Database Integrity, Foreign Key Constraints, Disjunctive Logic"
Inductive Logic Programming in Databases: from Datalog to DL+log,"In this paper we address an issue that has been brought to the attention of
the database community with the advent of the Semantic Web, i.e. the issue of
how ontologies (and semantics conveyed by them) can help solving typical
database problems, through a better understanding of KR aspects related to
databases. In particular, we investigate this issue from the ILP perspective by
considering two database problems, (i) the definition of views and (ii) the
definition of constraints, for a database whose schema is represented also by
means of an ontology. Both can be reformulated as ILP problems and can benefit
from the expressive and deductive power of the KR framework DL+log. We
illustrate the application scenarios by means of examples. Keywords: Inductive
Logic Programming, Relational Databases, Ontologies, Description Logics, Hybrid
Knowledge Representation and Reasoning Systems. Note: To appear in Theory and
Practice of Logic Programming (TPLP).",Francesca A. Lisi,"Inductive Logic Programming in Databases: from Datalog to DL+log. In this paper we address an issue that has been brought to the attention of
the database community with the advent of the Semantic Web, i.e. the issue of
how ontologies (and semantics conveyed by them) can help solving typical
database problems, through a better understanding of KR aspects related to
databases. In particular, we investigate this issue from the ILP perspective by
considering two database problems, (i) the definition of views and (ii) the
definition of constraints, for a database whose schema is represented also by
means of an ontology. Both can be reformulated as ILP problems and can benefit
from the expressive and deductive power of the KR framework DL+log. We
illustrate the application scenarios by means of examples. Keywords: Inductive
Logic Programming, Relational Databases, Ontologies, Description Logics, Hybrid
Knowledge Representation and Reasoning Systems. Note: To appear in Theory and
Practice of Logic Programming (TPLP).","Inductive Logic Programming, Datalog, DL+log, Relational Databases, Ontologies, Description Logics, Hybrid Knowledge Representation, Reasoning Systems"
Efficient Continual Top-$k$ Keyword Search in Relational Databases,"Keyword search in relational databases has been widely studied in recent
years because it does not require users neither to master a certain structured
query language nor to know the complex underlying data schemas. Most of
existing methods focus on answering snapshot keyword queries in static
databases. In practice, however, databases are updated frequently, and users
may have long-term interests on specific topics. To deal with such a situation,
it is necessary to build effective and efficient facility in database systems
to support continual keyword queries evaluation.
  In this paper, we propose an efficient method for continual keyword queries
answering over relational databases. The proposed method consists of two core
algorithms. The first one computes a set of potential top-$k$ results by
evaluating the ranges of the future relevance score for every query result and
create a light-weight state for each keyword query. The second one uses these
states to maintain the top-$k$ results of keyword queries when the database is
continually growing. Experimental results validate the effectiveness and
efficiency of the proposed method.",Yanwei Xu,"Efficient Continual Top-$k$ Keyword Search in Relational Databases. Keyword search in relational databases has been widely studied in recent
years because it does not require users neither to master a certain structured
query language nor to know the complex underlying data schemas. Most of
existing methods focus on answering snapshot keyword queries in static
databases. In practice, however, databases are updated frequently, and users
may have long-term interests on specific topics. To deal with such a situation,
it is necessary to build effective and efficient facility in database systems
to support continual keyword queries evaluation.
  In this paper, we propose an efficient method for continual keyword queries
answering over relational databases. The proposed method consists of two core
algorithms. The first one computes a set of potential top-$k$ results by
evaluating the ranges of the future relevance score for every query result and
create a light-weight state for each keyword query. The second one uses these
states to maintain the top-$k$ results of keyword queries when the database is
continually growing. Experimental results validate the effectiveness and
efficiency of the proposed method.","Continual Top-$k$ Keyword Search, Relational Databases, Algorithms"
Negative Database for Data Security,"Data Security is a major issue in any web-based application. There have been
approaches to handle intruders in any system, however, these approaches are not
fully trustable; evidently data is not totally protected. Real world databases
have information that needs to be securely stored. The approach of generating
negative database could help solve such problem. A Negative Database can be
defined as a database that contains huge amount of data consisting of
counterfeit data along with the real data. Intruders may be able to get access
to such databases, but, as they try to extract information, they will retrieve
data sets that would include both the actual and the negative data. In this
paper we present our approach towards implementing the concept of negative
database to help prevent data theft from malicious users and provide efficient
data retrieval for all valid users.","Anup Patel, Niveeta Sharma, Magdalini Eirinaki","Negative Database for Data Security. Data Security is a major issue in any web-based application. There have been
approaches to handle intruders in any system, however, these approaches are not
fully trustable; evidently data is not totally protected. Real world databases
have information that needs to be securely stored. The approach of generating
negative database could help solve such problem. A Negative Database can be
defined as a database that contains huge amount of data consisting of
counterfeit data along with the real data. Intruders may be able to get access
to such databases, but, as they try to extract information, they will retrieve
data sets that would include both the actual and the negative data. In this
paper we present our approach towards implementing the concept of negative
database to help prevent data theft from malicious users and provide efficient
data retrieval for all valid users.","Negative Database, Data Security, Web-based Application"
Automatic Partitioning of Database Applications,"Database-backed applications are nearly ubiquitous in our daily lives.
Applications that make many small accesses to the database create two
challenges for developers: increased latency and wasted resources from numerous
network round trips. A well-known technique to improve transactional database
application performance is to convert part of the application into stored
procedures that are executed on the database server. Unfortunately, this
conversion is often difficult. In this paper we describe Pyxis, a system that
takes database-backed applications and automatically partitions their code into
two pieces, one of which is executed on the application server and the other on
the database server. Pyxis profiles the application and server loads,
statically analyzes the code's dependencies, and produces a partitioning that
minimizes the number of control transfers as well as the amount of data sent
during each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is
able to generate partitions with up to 3x reduction in latency and 1.7x
improvement in throughput when compared to a traditional non-partitioned
implementation and has comparable performance to that of a custom stored
procedure implementation.","Alvin Cheung, Owen Arden, Samuel Madden, Andrew C. Myers","Automatic Partitioning of Database Applications. Database-backed applications are nearly ubiquitous in our daily lives.
Applications that make many small accesses to the database create two
challenges for developers: increased latency and wasted resources from numerous
network round trips. A well-known technique to improve transactional database
application performance is to convert part of the application into stored
procedures that are executed on the database server. Unfortunately, this
conversion is often difficult. In this paper we describe Pyxis, a system that
takes database-backed applications and automatically partitions their code into
two pieces, one of which is executed on the application server and the other on
the database server. Pyxis profiles the application and server loads,
statically analyzes the code's dependencies, and produces a partitioning that
minimizes the number of control transfers as well as the amount of data sent
during each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is
able to generate partitions with up to 3x reduction in latency and 1.7x
improvement in throughput when compared to a traditional non-partitioned
implementation and has comparable performance to that of a custom stored
procedure implementation.","Pyxis, Automatic Partitioning, Database Applications, TPC-C"
A Storage Advisor for Hybrid-Store Databases,"With the SAP HANA database, SAP offers a high-performance in-memory
hybrid-store database. Hybrid-store databases---that is, databases supporting
row- and column-oriented data management---are getting more and more prominent.
While the columnar management offers high-performance capabilities for
analyzing large quantities of data, the row-oriented store can handle
transactional point queries as well as inserts and updates more efficiently. To
effectively take advantage of both stores at the same time the novel question
whether to store the given data row- or column-oriented arises. We tackle this
problem with a storage advisor tool that supports database administrators at
this decision. Our proposed storage advisor recommends the optimal store based
on data and query characteristics; its core is a cost model to estimate and
compare query execution times for the different stores. Besides a per-table
decision, our tool also considers to horizontally and vertically partition the
data and manage the partitions on different stores. We evaluated the storage
advisor for the use in the SAP HANA database; we show the recommendation
quality as well as the benefit of having the data in the optimal store with
respect to increased query performance.","Philipp Rösch, Lars Dannecker, Gregor Hackenbroich, Franz Faerber","A Storage Advisor for Hybrid-Store Databases. With the SAP HANA database, SAP offers a high-performance in-memory
hybrid-store database. Hybrid-store databases---that is, databases supporting
row- and column-oriented data management---are getting more and more prominent.
While the columnar management offers high-performance capabilities for
analyzing large quantities of data, the row-oriented store can handle
transactional point queries as well as inserts and updates more efficiently. To
effectively take advantage of both stores at the same time the novel question
whether to store the given data row- or column-oriented arises. We tackle this
problem with a storage advisor tool that supports database administrators at
this decision. Our proposed storage advisor recommends the optimal store based
on data and query characteristics; its core is a cost model to estimate and
compare query execution times for the different stores. Besides a per-table
decision, our tool also considers to horizontally and vertically partition the
data and manage the partitions on different stores. We evaluated the storage
advisor for the use in the SAP HANA database; we show the recommendation
quality as well as the benefit of having the data in the optimal store with
respect to increased query performance.","Hybrid-Store Databases, Storage Advisor, SAP, HANA, Columnar Management, In-memory Database"
Hyper-Graph Based Database Partitioning for Transactional Workloads,"A common approach to scaling transactional databases in practice is
horizontal partitioning, which increases system scalability, high availability
and self-manageability. Usu- ally it is very challenging to choose or design an
optimal partitioning scheme for a given workload and database. In this
technical report, we propose a fine-grained hyper-graph based database
partitioning system for transactional work- loads. The partitioning system
takes a database, a workload, a node cluster and partitioning constraints as
input and out- puts a lookup-table encoding the final database partitioning
decision. The database partitioning problem is modeled as a multi-constraints
hyper-graph partitioning problem. By deriving a min-cut of the hyper-graph, our
system can min- imize the total number of distributed transactions in the
workload, balance the sizes and workload accesses of the partitions and satisfy
all the partition constraints imposed. Our system is highly interactive as it
allows users to im- pose partition constraints, watch visualized partitioning
ef- fects, and provide feedback based on human expertise and indirect domain
knowledge for generating better partition- ing schemes.","Yu cao, Xiaoyan Guo, Stephen Todd","Hyper-Graph Based Database Partitioning for Transactional Workloads. A common approach to scaling transactional databases in practice is
horizontal partitioning, which increases system scalability, high availability
and self-manageability. Usu- ally it is very challenging to choose or design an
optimal partitioning scheme for a given workload and database. In this
technical report, we propose a fine-grained hyper-graph based database
partitioning system for transactional work- loads. The partitioning system
takes a database, a workload, a node cluster and partitioning constraints as
input and out- puts a lookup-table encoding the final database partitioning
decision. The database partitioning problem is modeled as a multi-constraints
hyper-graph partitioning problem. By deriving a min-cut of the hyper-graph, our
system can min- imize the total number of distributed transactions in the
workload, balance the sizes and workload accesses of the partitions and satisfy
all the partition constraints imposed. Our system is highly interactive as it
allows users to im- pose partition constraints, watch visualized partitioning
ef- fects, and provide feedback based on human expertise and indirect domain
knowledge for generating better partition- ing schemes.","Hyper-Graph, Database Partitioning, Transactional Workloads, Scalability, High Availability, Self-Manageability"
Aggregate Estimation Over Dynamic Hidden Web Databases,"Many databases on the web are ""hidden"" behind (i.e., accessible only through)
their restrictive, form-like, search interfaces. Recent studies have shown that
it is possible to estimate aggregate query answers over such hidden web
databases by issuing a small number of carefully designed search queries
through the restrictive web interface. A problem with these existing work,
however, is that they all assume the underlying database to be static, while
most real-world web databases (e.g., Amazon, eBay) are frequently updated. In
this paper, we study the novel problem of estimating/tracking aggregates over
dynamic hidden web databases while adhering to the stringent query-cost
limitation they enforce (e.g., at most 1,000 search queries per day).
Theoretical analysis and extensive real-world experiments demonstrate the
effectiveness of our proposed algorithms and their superiority over baseline
solutions (e.g., the repeated execution of algorithms designed for static web
databases).","Weimo Liu, Saravanan Thirumuruganathan, Nan Zhang, Gautam Das","Aggregate Estimation Over Dynamic Hidden Web Databases. Many databases on the web are ""hidden"" behind (i.e., accessible only through)
their restrictive, form-like, search interfaces. Recent studies have shown that
it is possible to estimate aggregate query answers over such hidden web
databases by issuing a small number of carefully designed search queries
through the restrictive web interface. A problem with these existing work,
however, is that they all assume the underlying database to be static, while
most real-world web databases (e.g., Amazon, eBay) are frequently updated. In
this paper, we study the novel problem of estimating/tracking aggregates over
dynamic hidden web databases while adhering to the stringent query-cost
limitation they enforce (e.g., at most 1,000 search queries per day).
Theoretical analysis and extensive real-world experiments demonstrate the
effectiveness of our proposed algorithms and their superiority over baseline
solutions (e.g., the repeated execution of algorithms designed for static web
databases).","Aggregate Estimation, Dynamic Hidden Web Databases, Amazon, eBay"
"An Approach for Normalizing Fuzzy Relational Databases Based on Join
  Dependency","Fuzziness in databases is used to denote uncertain or incomplete data.
Relational Databases stress on the nature of the data to be certain. This
certainty based data is used as the basis of the normalization approach
designed for traditional relational databases. But real world data may not
always be certain, thereby making it necessary to design an approach for
normalization that deals with fuzzy data. This paper focuses on the approach
for designing the fifth normal form (5NF) based on join dependencies for fuzzy
data. The basis of join dependency for fuzzy relational databases is derived
from the basic relational database concepts. As join dependency implies an
multivalued dependency by symmetry the proof of join dependency based
normalization is stated from the perspective of multivalued dependency based
normalization on fuzzy relational databases.",Deepa S,"An Approach for Normalizing Fuzzy Relational Databases Based on Join
  Dependency. Fuzziness in databases is used to denote uncertain or incomplete data.
Relational Databases stress on the nature of the data to be certain. This
certainty based data is used as the basis of the normalization approach
designed for traditional relational databases. But real world data may not
always be certain, thereby making it necessary to design an approach for
normalization that deals with fuzzy data. This paper focuses on the approach
for designing the fifth normal form (5NF) based on join dependencies for fuzzy
data. The basis of join dependency for fuzzy relational databases is derived
from the basic relational database concepts. As join dependency implies an
multivalued dependency by symmetry the proof of join dependency based
normalization is stated from the perspective of multivalued dependency based
normalization on fuzzy relational databases.","Fuzzy Relational Databases, Join Dependency, Multivalued Dependency"
Query Rewriting and Optimization for Ontological Databases,"Ontological queries are evaluated against a knowledge base consisting of an
extensional database and an ontology (i.e., a set of logical assertions and
constraints which derive new intensional knowledge from the extensional
database), rather than directly on the extensional database. The evaluation and
optimization of such queries is an intriguing new problem for database
research. In this paper, we discuss two important aspects of this problem:
query rewriting and query optimization. Query rewriting consists of the
compilation of an ontological query into an equivalent first-order query
against the underlying extensional database. We present a novel query rewriting
algorithm for rather general types of ontological constraints which is
well-suited for practical implementations. In particular, we show how a
conjunctive query against a knowledge base, expressed using linear and sticky
existential rules, that is, members of the recently introduced Datalog+/-
family of ontology languages, can be compiled into a union of conjunctive
queries (UCQ) against the underlying database. Ontological query optimization,
in this context, attempts to improve this rewriting process so to produce
possibly small and cost-effective UCQ rewritings for an input query.","Georg Gottlob, Giorgio Orsi, Andreas Pieris","Query Rewriting and Optimization for Ontological Databases. Ontological queries are evaluated against a knowledge base consisting of an
extensional database and an ontology (i.e., a set of logical assertions and
constraints which derive new intensional knowledge from the extensional
database), rather than directly on the extensional database. The evaluation and
optimization of such queries is an intriguing new problem for database
research. In this paper, we discuss two important aspects of this problem:
query rewriting and query optimization. Query rewriting consists of the
compilation of an ontological query into an equivalent first-order query
against the underlying extensional database. We present a novel query rewriting
algorithm for rather general types of ontological constraints which is
well-suited for practical implementations. In particular, we show how a
conjunctive query against a knowledge base, expressed using linear and sticky
existential rules, that is, members of the recently introduced Datalog+/-
family of ontology languages, can be compiled into a union of conjunctive
queries (UCQ) against the underlying database. Ontological query optimization,
in this context, attempts to improve this rewriting process so to produce
possibly small and cost-effective UCQ rewritings for an input query.","Query Rewriting, Optimization, Ontological Databases, Datalog+/-"
"The Karlsruhe Astrophysical Database of Nucleosynthesis in Stars Project
  - Status and Prospects","The KADoNiS (Karlsruhe Astrophysical Database of Nucleosynthesis in Stars)
project is an astrophysical online database for cross sections relevant for
nucleosynthesis in the $s$ process and the $\gamma$ process. The $s$-process
database (www.kadonis.org) was started in 2005 and is presently facing its 4th
update (KADoNiS v1.0). The $\gamma$-process database (KADoNiS-p,
www.kadonis.org/pprocess) was recently revised and re-launched in March 2013.
  Both databases are compilations for experimental cross sections with
relevance to heavy ion nucleosynthesis. For the $s$ process recommended
Maxwellian averaged cross sections for $kT$= 5-100~keV are given for more than
360 isotopes between $^{1}$H and $^{210}$Bi. For the $\gamma$-process database
all available experimental data from $(p,\gamma), (p,n), (p,\alpha),
(\alpha,\gamma), (\alpha,n)$, and $(\alpha,p)$ reactions between $^{70}$Ge and
$^{209}$Bi in or close to the respective Gamow window were collected and can be
compared to theoretical predictions. The aim of both databases is a quick and
user-friendly access to the available data in the astrophysically relevant
energy regions.","Iris Dillmann, Tamas Szücs, Zsolt Fülöp, Ralf Plag, Franz Käppeler, Thomas Rauscher","The Karlsruhe Astrophysical Database of Nucleosynthesis in Stars Project
  - Status and Prospects. The KADoNiS (Karlsruhe Astrophysical Database of Nucleosynthesis in Stars)
project is an astrophysical online database for cross sections relevant for
nucleosynthesis in the $s$ process and the $\gamma$ process. The $s$-process
database (www.kadonis.org) was started in 2005 and is presently facing its 4th
update (KADoNiS v1.0). The $\gamma$-process database (KADoNiS-p,
www.kadonis.org/pprocess) was recently revised and re-launched in March 2013.
  Both databases are compilations for experimental cross sections with
relevance to heavy ion nucleosynthesis. For the $s$ process recommended
Maxwellian averaged cross sections for $kT$= 5-100~keV are given for more than
360 isotopes between $^{1}$H and $^{210}$Bi. For the $\gamma$-process database
all available experimental data from $(p,\gamma), (p,n), (p,\alpha),
(\alpha,\gamma), (\alpha,n)$, and $(\alpha,p)$ reactions between $^{70}$Ge and
$^{209}$Bi in or close to the respective Gamow window were collected and can be
compared to theoretical predictions. The aim of both databases is a quick and
user-friendly access to the available data in the astrophysically relevant
energy regions.","KADoNiS, Nucleosynthesis, Heavy Ion"
"NewSQL: Towards Next-Generation Scalable RDBMS for Online Transaction
  Processing (OLTP) for Big Data Management","One of the key advances in resolving the big-data problem has been the
emergence of an alternative database technology. Today, classic RDBMS are
complemented by a rich set of alternative Data Management Systems (DMS)
specially designed to handle the volume, variety, velocity and variability
ofBig Data collections; these DMS include NoSQL, NewSQL and Search-based
systems. NewSQL is a class of modern relational database management systems
(RDBMS) that provide the same scalable performance of NoSQL systems for online
transaction processing (OLTP) read-write workloads while still maintaining the
ACID guarantees of a traditional database system. This paper discusses about
NewSQL data management system; and compares with NoSQL and with traditional
database system. This paper covers architecture, characteristics,
classification of NewSQL databases for online transaction processing (OLTP) for
Big data management. It also provides the list ofpopular NoSQL as well as
NewSQL databases in separate categorized tables. This paper compares SQL based
RDBMS, NoSQL and NewSQL databases with set of metrics; as well as, addressed
some research issues ofNoSQL and NewSQL.",A B M Moniruzzaman,"NewSQL: Towards Next-Generation Scalable RDBMS for Online Transaction
  Processing (OLTP) for Big Data Management. One of the key advances in resolving the big-data problem has been the
emergence of an alternative database technology. Today, classic RDBMS are
complemented by a rich set of alternative Data Management Systems (DMS)
specially designed to handle the volume, variety, velocity and variability
ofBig Data collections; these DMS include NoSQL, NewSQL and Search-based
systems. NewSQL is a class of modern relational database management systems
(RDBMS) that provide the same scalable performance of NoSQL systems for online
transaction processing (OLTP) read-write workloads while still maintaining the
ACID guarantees of a traditional database system. This paper discusses about
NewSQL data management system; and compares with NoSQL and with traditional
database system. This paper covers architecture, characteristics,
classification of NewSQL databases for online transaction processing (OLTP) for
Big data management. It also provides the list ofpopular NoSQL as well as
NewSQL databases in separate categorized tables. This paper compares SQL based
RDBMS, NoSQL and NewSQL databases with set of metrics; as well as, addressed
some research issues ofNoSQL and NewSQL.","NewSQL, RDBMS, NoSQL, NewSQL, Big Data, Online Transaction Processing, OLTP, ACID"
"From Causes for Database Queries to Repairs and Model-Based Diagnosis
  and Back","In this work we establish and investigate connections between causality for
query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. Causality problems are formulated as diagnosis problems, and
the diagnoses provide causes and their responsibilities. The vast body of
research on database repairs can be applied to the newer problem of determining
actual causes for query answers and their responsibilities. These connections,
which are interesting per se, allow us, after a transition -inspired by
consistency-based diagnosis- to computational problems on hitting sets and
vertex covers in hypergraphs, to obtain several new algorithmic and complexity
results for causality in databases.","Babak Salimi, Leopoldo Bertossi","From Causes for Database Queries to Repairs and Model-Based Diagnosis
  and Back. In this work we establish and investigate connections between causality for
query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new problems in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes and the
other way around. Causality problems are formulated as diagnosis problems, and
the diagnoses provide causes and their responsibilities. The vast body of
research on database repairs can be applied to the newer problem of determining
actual causes for query answers and their responsibilities. These connections,
which are interesting per se, allow us, after a transition -inspired by
consistency-based diagnosis- to computational problems on hitting sets and
vertex covers in hypergraphs, to obtain several new algorithmic and complexity
results for causality in databases.","Database Queries, Repairs, Model-Based Diagnosis, Algorithms, Hypergraphs"
An Approach For Transforming of Relational Databases to OWL Ontology,"Rapid growth of documents, web pages, and other types of text content is a
huge challenge for the modern content management systems. One of the problems
in the areas of information storage and retrieval is the lacking of semantic
data. Ontologies can present knowledge in sharable and repeatedly usable manner
and provide an effective way to reduce the data volume overhead by encoding the
structure of a particular domain. Metadata in relational databases can be used
to extract ontology from database in a special domain. According to solve the
problem of sharing and reusing of data, approaches based on transforming
relational database to ontology are proposed. In this paper we propose a method
for automatic ontology construction based on relational database. Mining and
obtaining further components from relational database leads to obtain knowledge
with high semantic power and more expressiveness. Triggers are one of the
database components which could be transformed to the ontology model and
increase the amount of power and expressiveness of knowledge by presenting part
of the knowledge dynamically","Mona Dadjoo, Esmaeil Kheirkhah","An Approach For Transforming of Relational Databases to OWL Ontology. Rapid growth of documents, web pages, and other types of text content is a
huge challenge for the modern content management systems. One of the problems
in the areas of information storage and retrieval is the lacking of semantic
data. Ontologies can present knowledge in sharable and repeatedly usable manner
and provide an effective way to reduce the data volume overhead by encoding the
structure of a particular domain. Metadata in relational databases can be used
to extract ontology from database in a special domain. According to solve the
problem of sharing and reusing of data, approaches based on transforming
relational database to ontology are proposed. In this paper we propose a method
for automatic ontology construction based on relational database. Mining and
obtaining further components from relational database leads to obtain knowledge
with high semantic power and more expressiveness. Triggers are one of the
database components which could be transformed to the ontology model and
increase the amount of power and expressiveness of knowledge by presenting part
of the knowledge dynamically","Relational Databases, OWL Ontology, Metadata, Content Management Systems"
"From Causes for Database Queries to Repairs and Model-Based Diagnosis
  and Back","In this work we establish and investigate connections between causes for
query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new research areas in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes, and the
other way around. Causality problems are formulated as diagnosis problems, and
the diagnoses provide causes and their responsibilities. The vast body of
research on database repairs can be applied to the newer problems of computing
actual causes for query answers and their responsibilities. These connections,
which are interesting per se, allow us, after a transition -inspired by
consistency-based diagnosis- to computational problems on hitting sets and
vertex covers in hypergraphs, to obtain several new algorithmic and complexity
results for database causality.","Leopoldo Bertossi, Babak Salimi","From Causes for Database Queries to Repairs and Model-Based Diagnosis
  and Back. In this work we establish and investigate connections between causes for
query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new research areas in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes, and the
other way around. Causality problems are formulated as diagnosis problems, and
the diagnoses provide causes and their responsibilities. The vast body of
research on database repairs can be applied to the newer problems of computing
actual causes for query answers and their responsibilities. These connections,
which are interesting per se, allow us, after a transition -inspired by
consistency-based diagnosis- to computational problems on hitting sets and
vertex covers in hypergraphs, to obtain several new algorithmic and complexity
results for database causality.","Database Queries, Database Repairs, Model-Based Diagnosis, Algorithms, Complexity"
Document Selection in a Distributed Search Engine Architecture,"Distributed Search Engine Architecture (DSEA) hosts numerous independent
topic-specific search engines and selects a subset of the databases to search
within the architecture. The objective of this approach is to reduce the amount
of space needed to perform a search by querying only a subset of the total data
available. In order to manipulate data across many databases, it is most
efficient to identify a smaller subset of databases that would be most likely
to return the data of specific interest that can then be examined in greater
detail. The selection index has been most commonly used as a method for
choosing the most applicable databases as it captures broad information about
each database and its indexed documents. Employing this type of database allows
the researcher to find information more quickly, not only with less cost, but
it also minimizes the potential for biases. This paper investigates the
effectiveness of different databases selected within the framework and scope of
the distributed search engine architecture. The purpose of the study is to
improve the quality of distributed information retrieval.","Ibrahim AlShourbaji, Samaher Al-Janabi, Ahmed Patel","Document Selection in a Distributed Search Engine Architecture. Distributed Search Engine Architecture (DSEA) hosts numerous independent
topic-specific search engines and selects a subset of the databases to search
within the architecture. The objective of this approach is to reduce the amount
of space needed to perform a search by querying only a subset of the total data
available. In order to manipulate data across many databases, it is most
efficient to identify a smaller subset of databases that would be most likely
to return the data of specific interest that can then be examined in greater
detail. The selection index has been most commonly used as a method for
choosing the most applicable databases as it captures broad information about
each database and its indexed documents. Employing this type of database allows
the researcher to find information more quickly, not only with less cost, but
it also minimizes the potential for biases. This paper investigates the
effectiveness of different databases selected within the framework and scope of
the distributed search engine architecture. The purpose of the study is to
improve the quality of distributed information retrieval.","Document Selection, Distributed Search Engine Architecture, DSEA"
"OpenGeoBase: Information Centric Networking meets Spatial Database
  applications - Extended Version","This paper explores methodologies, advantages and challenges related to the
use of Information Centric Networking (ICN) for realizing distributed spatial
databases. Our findings show that the ICN functionality perfectly fits database
requirements: routing-by-name can be used to dispatch queries and insertions,
in-network caching to accelerate queries, and data-centric security to
implement secure multi-tenancy. We present an ICN-based distributed spatial
database, named OpenGeoBase, and describe its design choices. Thanks to ICN,
OpenGeoBase can quickly and efficiently provide information to database users;
easily operate in a distributed way, deploying and using many database engines
in parallel; secure every piece of content; naturally slice resources, so that
several tenants and users can concurrently and independently use the database.
We also show how OpenGeoBase can support a real world Intelligent Transport
System application, by enabling discovery of geo-referenced public
transportation information.","Andrea Detti, Nicola Blefari Melazzi, Michele Orru, Riccardo Paolillo, Giulio Rossi","OpenGeoBase: Information Centric Networking meets Spatial Database
  applications - Extended Version. This paper explores methodologies, advantages and challenges related to the
use of Information Centric Networking (ICN) for realizing distributed spatial
databases. Our findings show that the ICN functionality perfectly fits database
requirements: routing-by-name can be used to dispatch queries and insertions,
in-network caching to accelerate queries, and data-centric security to
implement secure multi-tenancy. We present an ICN-based distributed spatial
database, named OpenGeoBase, and describe its design choices. Thanks to ICN,
OpenGeoBase can quickly and efficiently provide information to database users;
easily operate in a distributed way, deploying and using many database engines
in parallel; secure every piece of content; naturally slice resources, so that
several tenants and users can concurrently and independently use the database.
We also show how OpenGeoBase can support a real world Intelligent Transport
System application, by enabling discovery of geo-referenced public
transportation information.","OpenGeoBase, Information Centric Networking, Spatial Database"
SNaX - A Database of Supernova X-ray Lightcurves,"We present the Supernova X-ray Database (SNaX), a compilation of the X-ray
data from young supernovae (SNe). The database includes the X-ray flux and
luminosity of young SNe, days to years after outburst. The original goal and
intent were to present a database of Type IIn SNe. After having accomplished
this we are slowly expanding it to include all SNe for which published data are
available. The interface allows one to search for SNe using various criteria,
plot all or selected data-points, and download both the data and the plot. The
plotting facility allows for significant customization. There is also a
facility for the user to submit data that can be directly incorporated into the
database. We include an option to fit the decay of any given SN lightcurve with
a power-law. The database includes a conversion of most datapoints to a common
0.3-8 keV band so that SN lightcurves may be directly compared with each other.
A mailing list has been set up to disseminate information about the database.
We outline the structure and function of the database, describe its various
features and outline the plans for future expansion.","Mathias Ross, Vikram V. Dwarkadas","SNaX - A Database of Supernova X-ray Lightcurves. We present the Supernova X-ray Database (SNaX), a compilation of the X-ray
data from young supernovae (SNe). The database includes the X-ray flux and
luminosity of young SNe, days to years after outburst. The original goal and
intent were to present a database of Type IIn SNe. After having accomplished
this we are slowly expanding it to include all SNe for which published data are
available. The interface allows one to search for SNe using various criteria,
plot all or selected data-points, and download both the data and the plot. The
plotting facility allows for significant customization. There is also a
facility for the user to submit data that can be directly incorporated into the
database. We include an option to fit the decay of any given SN lightcurve with
a power-law. The database includes a conversion of most datapoints to a common
0.3-8 keV band so that SN lightcurves may be directly compared with each other.
A mailing list has been set up to disseminate information about the database.
We outline the structure and function of the database, describe its various
features and outline the plans for future expansion.","SNaX, Supernova, X-ray Database, Type I"
"Synthetic Database for Evaluation of General, Fundamental Biometric
  Principles","We create synthetic biometric databases to study general, fundamental,
biometric principles. First, we check the validity of the synthetic database
design by comparing it to real data in terms of biometric performance. The real
data used for this validity check was from an eye-movement related biometric
database. Next, we employ our database to evaluate the impact of variations of
temporal persistence of features on biometric performance. We index temporal
persistence with the intraclass correlation coefficient (ICC). We find that
variations in temporal persistence are extremely highly correlated with
variations in biometric performance. Finally, we use our synthetic database
strategy to determine how many features are required to achieve particular
levels of performance as the number of subjects in the database increases from
100 to 10,000. An important finding is that the number of features required to
achieve various EER values (2%, 0.3%, 0.15%) is essentially constant in the
database sizes that we studied. We hypothesize that the insights obtained from
our study would be applicable to many biometric modalities where extracted
feature properties resemble the properties of the synthetic features we discuss
in this work.","Lee Friedman, Oleg Komogortsev","Synthetic Database for Evaluation of General, Fundamental Biometric
  Principles. We create synthetic biometric databases to study general, fundamental,
biometric principles. First, we check the validity of the synthetic database
design by comparing it to real data in terms of biometric performance. The real
data used for this validity check was from an eye-movement related biometric
database. Next, we employ our database to evaluate the impact of variations of
temporal persistence of features on biometric performance. We index temporal
persistence with the intraclass correlation coefficient (ICC). We find that
variations in temporal persistence are extremely highly correlated with
variations in biometric performance. Finally, we use our synthetic database
strategy to determine how many features are required to achieve particular
levels of performance as the number of subjects in the database increases from
100 to 10,000. An important finding is that the number of features required to
achieve various EER values (2%, 0.3%, 0.15%) is essentially constant in the
database sizes that we studied. We hypothesize that the insights obtained from
our study would be applicable to many biometric modalities where extracted
feature properties resemble the properties of the synthetic features we discuss
in this work.","Synthetic Database, Biometric Performance, Temporal Persistence"
Finding Theme Communities from Database Networks,"Given a database network where each vertex is associated with a transaction
database, we are interested in finding theme communities. Here, a theme
community is a cohesive subgraph such that a common pattern is frequent in all
transaction databases associated with the vertices in the subgraph. Finding all
theme communities from a database network enjoys many novel applications.
However, it is challenging since even counting the number of all theme
communities in a database network is #P-hard. Inspired by the observation that
a theme community shrinks when the length of the pattern increases, we
investigate several properties of theme communities and develop TCFI, a
scalable algorithm that uses these properties to effectively prune the patterns
that cannot form any theme community. We also design TC-Tree, a scalable
algorithm that decomposes and indexes theme communities efficiently. Retrieving
a ranked list of theme communities from a TC-Tree of hundreds of millions of
theme communities takes less than 1 second. Extensive experiments and a case
study demonstrate the effectiveness and scalability of TCFI and TC-Tree in
discovering and querying meaningful theme communities from large database
networks.","Lingyang Chu, Zhefeng Wang, Jian Pei, Yanyan Zhang, Yu Yang, Enhong Chen","Finding Theme Communities from Database Networks. Given a database network where each vertex is associated with a transaction
database, we are interested in finding theme communities. Here, a theme
community is a cohesive subgraph such that a common pattern is frequent in all
transaction databases associated with the vertices in the subgraph. Finding all
theme communities from a database network enjoys many novel applications.
However, it is challenging since even counting the number of all theme
communities in a database network is #P-hard. Inspired by the observation that
a theme community shrinks when the length of the pattern increases, we
investigate several properties of theme communities and develop TCFI, a
scalable algorithm that uses these properties to effectively prune the patterns
that cannot form any theme community. We also design TC-Tree, a scalable
algorithm that decomposes and indexes theme communities efficiently. Retrieving
a ranked list of theme communities from a TC-Tree of hundreds of millions of
theme communities takes less than 1 second. Extensive experiments and a case
study demonstrate the effectiveness and scalability of TCFI and TC-Tree in
discovering and querying meaningful theme communities from large database
networks.","TCFI, TC-Tree, Database Networks, Database"
Columnar Database Techniques for Creating AI Features,"Recent advances with in-memory columnar database techniques have increased
the performance of analytical queries on very large databases and data
warehouses. At the same time, advances in artificial intelligence (AI)
algorithms have increased the ability to analyze data. We use the term AI to
encompass both Deep Learning (DL or neural network) and Machine Learning (ML
aka Big Data analytics). Our exploration of the AI full stack has led us to a
cross-stack columnar database innovation that efficiently creates features for
AI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to
add to existing columnar database dictionaries in order to increase the
efficiency of featurization by minimizing data movement and data duplication.
We show how various forms of featurization (feature selection, feature
extraction, and feature creation) can be efficiently calculated in a columnar
database. The full stack AI investigation has also led us to propose an
integrated columnar database and AI architecture. This architecture has
information flows and feedback loops to improve the whole analytics cycle
during multiple iterations of extracting data from the data sources,
featurization, and analysis.","Brad Carlile, Akiko Marti, Guy Delamarter","Columnar Database Techniques for Creating AI Features. Recent advances with in-memory columnar database techniques have increased
the performance of analytical queries on very large databases and data
warehouses. At the same time, advances in artificial intelligence (AI)
algorithms have increased the ability to analyze data. We use the term AI to
encompass both Deep Learning (DL or neural network) and Machine Learning (ML
aka Big Data analytics). Our exploration of the AI full stack has led us to a
cross-stack columnar database innovation that efficiently creates features for
AI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to
add to existing columnar database dictionaries in order to increase the
efficiency of featurization by minimizing data movement and data duplication.
We show how various forms of featurization (feature selection, feature
extraction, and feature creation) can be efficiently calculated in a columnar
database. The full stack AI investigation has also led us to propose an
integrated columnar database and AI architecture. This architecture has
information flows and feedback loops to improve the whole analytics cycle
during multiple iterations of extracting data from the data sources,
featurization, and analysis.","Columnar Database, AI, Deep Learning, Neural Network, Machine Learning, Big Data"
A Blockchain Database Application Platform,"A blockchain is a decentralised linked data structure that is characterised
by its inherent resistance to data modification, but it is deficient in search
queries, primarily due to its inferior data formatting. A distributed database
is also a decentralised data structure which features quick query processing
and well-designed data formatting but suffers from data reliability. In this
demonstration, we showcase a blockchain database application platform developed
by integrating the blockchain with the database, i.e. we demonstrate a system
that has the decentralised, distributed and audibility features of the
blockchain and quick query processing and well-designed data structure of the
distributed databases. The system features a tamper-resistant, consistent and
cost-effective multi-active database and an effective and reliable data-level
disaster recovery backup. The system is demonstrated in practice as a
multi-active database along with the data-level disaster recovery backup
feature.","Muhammad Muzammal, Qiang Qu, Bulat Nasrulin, Anders Skovsgaard","A Blockchain Database Application Platform. A blockchain is a decentralised linked data structure that is characterised
by its inherent resistance to data modification, but it is deficient in search
queries, primarily due to its inferior data formatting. A distributed database
is also a decentralised data structure which features quick query processing
and well-designed data formatting but suffers from data reliability. In this
demonstration, we showcase a blockchain database application platform developed
by integrating the blockchain with the database, i.e. we demonstrate a system
that has the decentralised, distributed and audibility features of the
blockchain and quick query processing and well-designed data structure of the
distributed databases. The system features a tamper-resistant, consistent and
cost-effective multi-active database and an effective and reliable data-level
disaster recovery backup. The system is demonstrated in practice as a
multi-active database along with the data-level disaster recovery backup
feature.","Blockchain, Database, Disaster Recovery Backup, Data Reliability"
"Performing energy modelling exercises in a transparent way the issue of
  data quality in power plant databases","In energy modelling, open data and open source code can help enhance
traceability and reproducibility of model exercises which contribute to
facilitate controversial debates and improve policy advice. While the
availability of open power plant databases increased in recent years, they
often differ considerably from each other and their data quality has not been
systematically compared to proprietary sources yet. Here, we introduce the
python-based ""powerplantmatching"" (PPM), an open source toolset for cleaning,
standardizing and combining multiple power plant databases. We apply it once
only with open databases and once with an additional proprietary database in
order to discuss and elaborate the issue of data quality, by analysing
capacities, countries, fuel types, geographic coordinates and commissioning
years for conventional power plants. We find that a derived dataset purely
based on open data is not yet on a par with one in which a proprietary database
has been added to the matching, even though the statistical values for capacity
matched to a large degree with both datasets. When commissioning years are
needed for modelling purposes in the final dataset, the proprietary database
helps crucially to increase the quality of the derived dataset.","Fabian Gotzens, Heidi Heinrichs, Jonas Hörsch, Fabian Hofmann","Performing energy modelling exercises in a transparent way the issue of
  data quality in power plant databases. In energy modelling, open data and open source code can help enhance
traceability and reproducibility of model exercises which contribute to
facilitate controversial debates and improve policy advice. While the
availability of open power plant databases increased in recent years, they
often differ considerably from each other and their data quality has not been
systematically compared to proprietary sources yet. Here, we introduce the
python-based ""powerplantmatching"" (PPM), an open source toolset for cleaning,
standardizing and combining multiple power plant databases. We apply it once
only with open databases and once with an additional proprietary database in
order to discuss and elaborate the issue of data quality, by analysing
capacities, countries, fuel types, geographic coordinates and commissioning
years for conventional power plants. We find that a derived dataset purely
based on open data is not yet on a par with one in which a proprietary database
has been added to the matching, even though the statistical values for capacity
matched to a large degree with both datasets. When commissioning years are
needed for modelling purposes in the final dataset, the proprietary database
helps crucially to increase the quality of the derived dataset.","Energy Modeling, Open Data, Python, Power Plant, PPM, Data Quality"
Emotion recognition with 4kresolution database,"Classifying the human emotion through facial expressions is a big topic in
both the Computer Vision and Deep learning fields. Human emotion can be
classified as one of the basic emotion types like being angry, happy or
dimensional emotion with valence and arousal values. There are a lot of related
challenges in this topic, one of the most famous challenges is called the
'Affect-in-the-wild Challenge'(Aff-Wild Challenge). It is the first challenge
on the estimation of valence and arousal in-the-wild. This project is an
extension of the Aff-wild Challenge. Aff-wild database was created using images
with a mean resolution of 607*359, I and Dimitrios sought to find out the
performance of the model that is trained on a database that contains4K
resolution in-the-wild images. Since there is no existing database to satisfy
the requirement, I built this database from scratch with help from Dimitrios
and trained neural network models with different hyperparameters on this
database. I used network models likeVGG16, AlexNet, ResNet and also some
pre-trained models like Ima-geNet VGG. I compared the results of the different
network models alongside the results from the Aff-wild database to exploit the
optimal model for my database.",Qian Zheng,"Emotion recognition with 4kresolution database. Classifying the human emotion through facial expressions is a big topic in
both the Computer Vision and Deep learning fields. Human emotion can be
classified as one of the basic emotion types like being angry, happy or
dimensional emotion with valence and arousal values. There are a lot of related
challenges in this topic, one of the most famous challenges is called the
'Affect-in-the-wild Challenge'(Aff-Wild Challenge). It is the first challenge
on the estimation of valence and arousal in-the-wild. This project is an
extension of the Aff-wild Challenge. Aff-wild database was created using images
with a mean resolution of 607*359, I and Dimitrios sought to find out the
performance of the model that is trained on a database that contains4K
resolution in-the-wild images. Since there is no existing database to satisfy
the requirement, I built this database from scratch with help from Dimitrios
and trained neural network models with different hyperparameters on this
database. I used network models likeVGG16, AlexNet, ResNet and also some
pre-trained models like Ima-geNet VGG. I compared the results of the different
network models alongside the results from the Aff-wild database to exploit the
optimal model for my database.","Emotion Recognition, Neural Network, VGG16, AlexNet, ResNet, Dimitrios"
Learning Models over Relational Data: A Brief Tutorial,"This tutorial overviews the state of the art in learning models over
relational databases and makes the case for a first-principles approach that
exploits recent developments in database research.
  The input to learning classification and regression models is a training
dataset defined by feature extraction queries over relational databases. The
mainstream approach to learning over relational data is to materialize the
training dataset, export it out of the database, and then learn over it using a
statistical package. This approach can be expensive as it requires the
materialization of the training dataset. An alternative approach is to cast the
machine learning problem as a database problem by transforming the
data-intensive component of the learning task into a batch of aggregates over
the feature extraction query and by computing this batch directly over the
input database.
  The tutorial highlights a variety of techniques developed by the database
theory and systems communities to improve the performance of the learning task.
They rely on structural properties of the relational data and of the feature
extraction query, including algebraic (semi-ring), combinatorial (hypertree
width), statistical (sampling), or geometric (distance) structure. They also
rely on factorized computation, code specialization, query compilation, and
parallelization.","Maximilian Schleich, Dan Olteanu, Mahmoud Abo-Khamis, Hung Q. Ngo, XuanLong Nguyen","Learning Models over Relational Data: A Brief Tutorial. This tutorial overviews the state of the art in learning models over
relational databases and makes the case for a first-principles approach that
exploits recent developments in database research.
  The input to learning classification and regression models is a training
dataset defined by feature extraction queries over relational databases. The
mainstream approach to learning over relational data is to materialize the
training dataset, export it out of the database, and then learn over it using a
statistical package. This approach can be expensive as it requires the
materialization of the training dataset. An alternative approach is to cast the
machine learning problem as a database problem by transforming the
data-intensive component of the learning task into a batch of aggregates over
the feature extraction query and by computing this batch directly over the
input database.
  The tutorial highlights a variety of techniques developed by the database
theory and systems communities to improve the performance of the learning task.
They rely on structural properties of the relational data and of the feature
extraction query, including algebraic (semi-ring), combinatorial (hypertree
width), statistical (sampling), or geometric (distance) structure. They also
rely on factorized computation, code specialization, query compilation, and
parallelization.","Learning Models, Relational Databases, Machine Learning, Regression Models"
On Embeddings in Relational Databases,"We address the problem of learning a distributed representation of entities
in a relational database using a low-dimensional embedding. Low-dimensional
embeddings aim to encapsulate a concise vector representation for an underlying
dataset with minimum loss of information. Embeddings across entities in a
relational database have been less explored due to the intricate data relations
and representation complexity involved. Relational databases are an
inter-weaved collection of relations that not only model relationships between
entities but also record complex domain-specific quantitative and temporal
attributes of data defining complex relationships among entities. Recent
methods for learning an embedding constitute of a naive approach to consider
complete denormalization of the database by materializing the full join of all
tables and representing as a knowledge graph. This popular approach has certain
limitations as it fails to capture the inter-row relationships and additional
semantics encoded in the relational databases. In this paper we demonstrate; a
better methodology for learning representations by exploiting the underlying
semantics of columns in a table while using the relation joins and the latent
inter-row relationships. Empirical results over a real-world database with
evaluations on similarity join and table completion tasks support our
proposition.","Siddhant Arora, Srikanta Bedathur","On Embeddings in Relational Databases. We address the problem of learning a distributed representation of entities
in a relational database using a low-dimensional embedding. Low-dimensional
embeddings aim to encapsulate a concise vector representation for an underlying
dataset with minimum loss of information. Embeddings across entities in a
relational database have been less explored due to the intricate data relations
and representation complexity involved. Relational databases are an
inter-weaved collection of relations that not only model relationships between
entities but also record complex domain-specific quantitative and temporal
attributes of data defining complex relationships among entities. Recent
methods for learning an embedding constitute of a naive approach to consider
complete denormalization of the database by materializing the full join of all
tables and representing as a knowledge graph. This popular approach has certain
limitations as it fails to capture the inter-row relationships and additional
semantics encoded in the relational databases. In this paper we demonstrate; a
better methodology for learning representations by exploiting the underlying
semantics of columns in a table while using the relation joins and the latent
inter-row relationships. Empirical results over a real-world database with
evaluations on similarity join and table completion tasks support our
proposition.","Embeddings, Relational Databases, Low-Dimensional, Relationship Jumps, Latent Inter-row Relationships, Algorithms"
Dynamic Query Evaluation Over Structures with Low Degree,"We consider the evaluation of first-order queries over classes of databases
that have bounded degree and low degree. More precisely, given a query and a
database, we want to efficiently test whether there is a solution, count how
many solutions there are, or be able to enumerate the set of all solutions.
  Bounded and low degree are rather natural notions and both yield efficient
algorithms. For example, Berkholz, Keppeler, and Schweikardt showed in 2017
that over databases of bounded degree, not only any first order query can
efficiently be tested, counted and enumerated, but the data structure used can
be updated when the database itself is updated.
  This paper extends existing results in two directions. First, we show that
over classes of databases with low degree, there is a data structure that
enables us to test, count and enumerate the solutions of first order queries.
This data structure can also be efficiently recomputed when the database is
updated. Secondly, for classes of databases with bounded degree we show that,
without increasing the preprocessing time, we can compute a data structure that
does not depend on the query but only on its quantifier rank. We can therefore
perform a single preprocessing that can later be used for many queries.",Alexandre Vigny,"Dynamic Query Evaluation Over Structures with Low Degree. We consider the evaluation of first-order queries over classes of databases
that have bounded degree and low degree. More precisely, given a query and a
database, we want to efficiently test whether there is a solution, count how
many solutions there are, or be able to enumerate the set of all solutions.
  Bounded and low degree are rather natural notions and both yield efficient
algorithms. For example, Berkholz, Keppeler, and Schweikardt showed in 2017
that over databases of bounded degree, not only any first order query can
efficiently be tested, counted and enumerated, but the data structure used can
be updated when the database itself is updated.
  This paper extends existing results in two directions. First, we show that
over classes of databases with low degree, there is a data structure that
enables us to test, count and enumerate the solutions of first order queries.
This data structure can also be efficiently recomputed when the database is
updated. Secondly, for classes of databases with bounded degree we show that,
without increasing the preprocessing time, we can compute a data structure that
does not depend on the query but only on its quantifier rank. We can therefore
perform a single preprocessing that can later be used for many queries.","Dynamic Query Evaluation, Data Structures, Low Degree, Bounded Degree"
Biometric Masterkeys,"Biometric authentication is used to secure digital or physical access. Such
an authentication system uses a biometric database, where data are sometimes
protected by cancelable transformations. This paper introduces the notion of
biometric masterkeys. A masterkey is a feature vector such that the
corresponding template matches with a significant number of templates stored in
a cancelable biometric database. Such a masterkey is directly researched from a
cancelable biometric database, but we also investigate another scenario in
which the masterkey is fixed before the creation of the cancelable biometric
database, providing additional access rights in the system for the masterkey's
owner. Experimental results on the fingerprint database FVC and the face image
database LFW show the effectiveness and the efficiency of such masterkeys in
both scenarios. In particular, from any given feature vector, we are able to
construct a cancelable database, for which the biometric template matches with
all the templates of the database.","Tanguy Gernot, Patrick Lacharme","Biometric Masterkeys. Biometric authentication is used to secure digital or physical access. Such
an authentication system uses a biometric database, where data are sometimes
protected by cancelable transformations. This paper introduces the notion of
biometric masterkeys. A masterkey is a feature vector such that the
corresponding template matches with a significant number of templates stored in
a cancelable biometric database. Such a masterkey is directly researched from a
cancelable biometric database, but we also investigate another scenario in
which the masterkey is fixed before the creation of the cancelable biometric
database, providing additional access rights in the system for the masterkey's
owner. Experimental results on the fingerprint database FVC and the face image
database LFW show the effectiveness and the efficiency of such masterkeys in
both scenarios. In particular, from any given feature vector, we are able to
construct a cancelable database, for which the biometric template matches with
all the templates of the database.","Biometric Masterkeys, Authentication, Cancellable Databases, FVC, Face Image, LFW"
"DB-BERT: a Database Tuning Tool that ""Reads the Manual""","DB-BERT is a database tuning tool that exploits information gained via
natural language analysis of manuals and other relevant text documents. It uses
text to identify database system parameters to tune as well as recommended
parameter values. DB-BERT applies large, pre-trained language models
(specifically, the BERT model) for text analysis. During an initial training
phase, it fine-tunes model weights in order to translate natural language hints
into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and
prioritize hints to achieve optimal performance for a specific database system
and benchmark. Both phases are iterative and use reinforcement learning to
guide the selection of tuning settings to evaluate (penalizing settings that
the database system rejects while rewarding settings that improve performance).
In our experiments, we leverage hundreds of text documents about database
tuning as input for DB-BERT. We compare DB-BERT against various baselines,
considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run
time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT
finds the best parameter settings among all compared methods. The code of
DB-BERT is available online at https://itrummer.github.io/dbbert/.",Immanuel Trummer,"DB-BERT: a Database Tuning Tool that ""Reads the Manual"". DB-BERT is a database tuning tool that exploits information gained via
natural language analysis of manuals and other relevant text documents. It uses
text to identify database system parameters to tune as well as recommended
parameter values. DB-BERT applies large, pre-trained language models
(specifically, the BERT model) for text analysis. During an initial training
phase, it fine-tunes model weights in order to translate natural language hints
into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and
prioritize hints to achieve optimal performance for a specific database system
and benchmark. Both phases are iterative and use reinforcement learning to
guide the selection of tuning settings to evaluate (penalizing settings that
the database system rejects while rewarding settings that improve performance).
In our experiments, we leverage hundreds of text documents about database
tuning as input for DB-BERT. We compare DB-BERT against various baselines,
considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run
time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT
finds the best parameter settings among all compared methods. The code of
DB-BERT is available online at https://itrummer.github.io/dbbert/.","DB-BERT, Database Tuning Tool, BERT, Natural Language Processing, NLP"
What Can Database Query Processing Do for Instance-Spanning Constraints?,"In the last decade, the term instance-spanning constraint has been introduced
in the process mining field to refer to constraints that span multiple process
instances of one or several processes. Of particular relevance, in this
setting, is checking whether process executions comply with constraints of
interest, which at runtime calls for suitable monitoring techniques. Even
though event data are often stored in some sort of database, there is a lack of
database-oriented approaches to tackle compliance checking and monitoring of
(instance-spanning) constraints. In this paper, we fill this gap by showing how
well-established technology from database query processing can be effectively
used for this purpose. We propose to define an instance-spanning constraint
through an ensemble of four database queries that retrieve the satisfying,
violating, pending-satisfying, and pending-violating cases of the constraint.
In this context, the problem of compliance monitoring then becomes an
application of techniques for incremental view maintenance, which is
well-developed in database query processing. In this paper, we argue for our
approach in detail, and, as a proof of concept, present an experimental
validation using the DBToaster incremental database query engine.","Heba Aamer, Marco Montali, Jan Van den Bussche","What Can Database Query Processing Do for Instance-Spanning Constraints?. In the last decade, the term instance-spanning constraint has been introduced
in the process mining field to refer to constraints that span multiple process
instances of one or several processes. Of particular relevance, in this
setting, is checking whether process executions comply with constraints of
interest, which at runtime calls for suitable monitoring techniques. Even
though event data are often stored in some sort of database, there is a lack of
database-oriented approaches to tackle compliance checking and monitoring of
(instance-spanning) constraints. In this paper, we fill this gap by showing how
well-established technology from database query processing can be effectively
used for this purpose. We propose to define an instance-spanning constraint
through an ensemble of four database queries that retrieve the satisfying,
violating, pending-satisfying, and pending-violating cases of the constraint.
In this context, the problem of compliance monitoring then becomes an
application of techniques for incremental view maintenance, which is
well-developed in database query processing. In this paper, we argue for our
approach in detail, and, as a proof of concept, present an experimental
validation using the DBToaster incremental database query engine.","Database Query Processing, Instance-Spanning Constraints, Compliance Monitoring, DBToaster, Incremental View Maintenance"
Paraconsistent logic and query answering in inconsistent databases,"This paper concerns the paraconsistent logic LPQ$^{\supset,\mathsf{F}}$ and
an application of it in the area of relational database theory. The notions of
a relational database, a query applicable to a relational database, and a
consistent answer to a query with respect to a possibly inconsistent relational
database are considered from the perspective of this logic. This perspective
enables among other things the definition of a consistent answer to a query
with respect to a possibly inconsistent database without resort to database
repairs. In a previous paper, LPQ$^{\supset,\mathsf{F}}$ is presented with a
sequent-style natural deduction proof system. In this paper, a sequent calculus
proof system is presented because it is common to use a sequent calculus proof
system as the basis of proof search procedures and such procedures may form the
core of algorithms for computing consistent answers to queries.",C. A. Middelburg,"Paraconsistent logic and query answering in inconsistent databases. This paper concerns the paraconsistent logic LPQ$^{\supset,\mathsf{F}}$ and
an application of it in the area of relational database theory. The notions of
a relational database, a query applicable to a relational database, and a
consistent answer to a query with respect to a possibly inconsistent relational
database are considered from the perspective of this logic. This perspective
enables among other things the definition of a consistent answer to a query
with respect to a possibly inconsistent database without resort to database
repairs. In a previous paper, LPQ$^{\supset,\mathsf{F}}$ is presented with a
sequent-style natural deduction proof system. In this paper, a sequent calculus
proof system is presented because it is common to use a sequent calculus proof
system as the basis of proof search procedures and such procedures may form the
core of algorithms for computing consistent answers to queries.","Paraconsistent Logic, Relational Database, Sequent Calculus, Natural Decision"
Global Reasoning over Database Structures for Text-to-SQL Parsing,"State-of-the-art semantic parsers rely on auto-regressive decoding, emitting
one symbol at a time. When tested against complex databases that are unobserved
at training time (zero-shot), the parser often struggles to select the correct
set of database constants in the new database, due to the local nature of
decoding. In this work, we propose a semantic parser that globally reasons
about the structure of the output query to make a more contextually-informed
selection of database constants. We use message-passing through a graph neural
network to softly select a subset of database constants for the output query,
conditioned on the question. Moreover, we train a model to rank queries based
on the global alignment of database constants to question words. We apply our
techniques to the current state-of-the-art model for Spider, a zero-shot
semantic parsing dataset with complex databases, increasing accuracy from 39.4%
to 47.4%.","Ben Bogin, Matt Gardner, Jonathan Berant","Global Reasoning over Database Structures for Text-to-SQL Parsing. State-of-the-art semantic parsers rely on auto-regressive decoding, emitting
one symbol at a time. When tested against complex databases that are unobserved
at training time (zero-shot), the parser often struggles to select the correct
set of database constants in the new database, due to the local nature of
decoding. In this work, we propose a semantic parser that globally reasons
about the structure of the output query to make a more contextually-informed
selection of database constants. We use message-passing through a graph neural
network to softly select a subset of database constants for the output query,
conditioned on the question. Moreover, we train a model to rank queries based
on the global alignment of database constants to question words. We apply our
techniques to the current state-of-the-art model for Spider, a zero-shot
semantic parsing dataset with complex databases, increasing accuracy from 39.4%
to 47.4%.","Global Reasoning, Database Structures, Text-to-SQL Parsing, Spider, Graph Neural Network"
NoSQL Databases: Yearning for Disambiguation,"The demanding requirements of the new Big Data intensive era raised the need
for flexible storage systems capable of handling huge volumes of unstructured
data and of tackling the challenges that traditional databases were facing.
NoSQL Databases, in their heterogeneity, are a powerful and diverse set of
databases tailored to specific industrial and business needs. However, the lack
of theoretical background creates a lack of consensus even among experts about
many NoSQL concepts, leading to ambiguity and confusion. In this paper, we
present a survey of NoSQL databases and their classification by data model
type. We also conduct a benchmark in order to compare different NoSQL databases
and distinguish their characteristics. Additionally, we present the major areas
of ambiguity and confusion around NoSQL databases and their related concepts,
and attempt to disambiguate them.","Chaimae Asaad, Karim Baïna, Mounir Ghogho","NoSQL Databases: Yearning for Disambiguation. The demanding requirements of the new Big Data intensive era raised the need
for flexible storage systems capable of handling huge volumes of unstructured
data and of tackling the challenges that traditional databases were facing.
NoSQL Databases, in their heterogeneity, are a powerful and diverse set of
databases tailored to specific industrial and business needs. However, the lack
of theoretical background creates a lack of consensus even among experts about
many NoSQL concepts, leading to ambiguity and confusion. In this paper, we
present a survey of NoSQL databases and their classification by data model
type. We also conduct a benchmark in order to compare different NoSQL databases
and distinguish their characteristics. Additionally, we present the major areas
of ambiguity and confusion around NoSQL databases and their related concepts,
and attempt to disambiguate them.","NoSQL Databases, Big Data, Data Model"
Towards a Natural Language Query Processing System,"Tackling the information retrieval gap between non-technical database
end-users and those with the knowledge of formal query languages has been an
interesting area of data management and analytics research. The use of natural
language interfaces to query information from databases offers the opportunity
to bridge the communication challenges between end-users and systems that use
formal query languages. Previous research efforts mainly focused on developing
structured query interfaces to relational databases. However, the evolution of
unstructured big data such as text, images, and video has exposed the
limitations of traditional structured query interfaces. While the existing web
search tools prove the popularity and usability of natural language query, they
return complete documents and web pages instead of focused query responses and
are not applicable to database systems. This paper reports our study on the
design and development of a natural language query interface to a backend
relational database. The novelty in the study lies in defining a graph database
as a middle layer to store necessary metadata needed to transform a natural
language query into structured query language that can be executed on backend
databases. We implemented and evaluated our approach using a restaurant
dataset. The translation results for some sample queries yielded a 90% accuracy
rate.","Chantal Montgomery, Haruna Isah, Farhana Zulkernine","Towards a Natural Language Query Processing System. Tackling the information retrieval gap between non-technical database
end-users and those with the knowledge of formal query languages has been an
interesting area of data management and analytics research. The use of natural
language interfaces to query information from databases offers the opportunity
to bridge the communication challenges between end-users and systems that use
formal query languages. Previous research efforts mainly focused on developing
structured query interfaces to relational databases. However, the evolution of
unstructured big data such as text, images, and video has exposed the
limitations of traditional structured query interfaces. While the existing web
search tools prove the popularity and usability of natural language query, they
return complete documents and web pages instead of focused query responses and
are not applicable to database systems. This paper reports our study on the
design and development of a natural language query interface to a backend
relational database. The novelty in the study lies in defining a graph database
as a middle layer to store necessary metadata needed to transform a natural
language query into structured query language that can be executed on backend
databases. We implemented and evaluated our approach using a restaurant
dataset. The translation results for some sample queries yielded a 90% accuracy
rate.","Natural Language Processing, NLP, Database, Graph Database"
"BERT Meets Relational DB: Contextual Representations of Relational
  Databases","In this paper, we address the problem of learning low dimension
representation of entities on relational databases consisting of multiple
tables. Embeddings help to capture semantics encoded in the database and can be
used in a variety of settings like auto-completion of tables, fully-neural
query processing of relational joins queries, seamlessly handling missing
values, and more. Current work is restricted to working with just single table,
or using pretrained embeddings over an external corpus making them unsuitable
for use in real-world databases. In this work, we look into ways of using these
attention-based model to learn embeddings for entities in the relational
database. We are inspired by BERT style pretraining methods and are interested
in observing how they can be extended for representation learning on structured
databases. We evaluate our approach of the autocompletion of relational
databases and achieve improvement over standard baselines.","Siddhant Arora, Vinayak Gupta, Garima Gaur, Srikanta Bedathur","BERT Meets Relational DB: Contextual Representations of Relational
  Databases. In this paper, we address the problem of learning low dimension
representation of entities on relational databases consisting of multiple
tables. Embeddings help to capture semantics encoded in the database and can be
used in a variety of settings like auto-completion of tables, fully-neural
query processing of relational joins queries, seamlessly handling missing
values, and more. Current work is restricted to working with just single table,
or using pretrained embeddings over an external corpus making them unsuitable
for use in real-world databases. In this work, we look into ways of using these
attention-based model to learn embeddings for entities in the relational
database. We are inspired by BERT style pretraining methods and are interested
in observing how they can be extended for representation learning on structured
databases. We evaluate our approach of the autocompletion of relational
databases and achieve improvement over standard baselines.","BERT, Relational DB, Contextual Representations, Embeddings, Autocompletion, Fully-neural Query"
Database Reasoning Over Text,"Neural models have shown impressive performance gains in answering queries
from natural language text. However, existing works are unable to support
database queries, such as ""List/Count all female athletes who were born in 20th
century"", which require reasoning over sets of relevant facts with operations
such as join, filtering and aggregation. We show that while state-of-the-art
transformer models perform very well for small databases, they exhibit
limitations in processing noisy data, numerical operations, and queries that
aggregate facts. We propose a modular architecture to answer these
database-style queries over multiple spans from text and aggregating these at
scale. We evaluate the architecture using WikiNLDB, a novel dataset for
exploring such queries. Our architecture scales to databases containing
thousands of facts whereas contemporary models are limited by how many facts
can be encoded. In direct comparison on small databases, our approach increases
overall answer accuracy from 85% to 90%. On larger databases, our approach
retains its accuracy whereas transformer baselines could not encode the
context.","James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, Alon Halevy","Database Reasoning Over Text. Neural models have shown impressive performance gains in answering queries
from natural language text. However, existing works are unable to support
database queries, such as ""List/Count all female athletes who were born in 20th
century"", which require reasoning over sets of relevant facts with operations
such as join, filtering and aggregation. We show that while state-of-the-art
transformer models perform very well for small databases, they exhibit
limitations in processing noisy data, numerical operations, and queries that
aggregate facts. We propose a modular architecture to answer these
database-style queries over multiple spans from text and aggregating these at
scale. We evaluate the architecture using WikiNLDB, a novel dataset for
exploring such queries. Our architecture scales to databases containing
thousands of facts whereas contemporary models are limited by how many facts
can be encoded. In direct comparison on small databases, our approach increases
overall answer accuracy from 85% to 90%. On larger databases, our approach
retains its accuracy whereas transformer baselines could not encode the
context.","Database Reasoning, Neural Models, WikiNLDB"
CIPM: Common Identification Process Model for Database Forensics Field,"Database Forensics (DBF) domain is a branch of digital forensics, concerned
with the identification, collection, reconstruction, analysis, and
documentation of database crimes. Different researchers have introduced several
identification models to handle database crimes. Majority of proposed models
are not specific and are redundant, which makes these models a problem because
of the multidimensional nature and high diversity of database systems.
Accordingly, using the metamodeling approach, the current study is aimed at
proposing a unified identification model applicable to the database forensic
field. The model integrates and harmonizes all exiting identification processes
into a single abstract model, called Common Identification Process Model
(CIPM). The model comprises six phases: 1) notifying an incident, 2) responding
to the incident, 3) identification of the incident source, 4) verification of
the incident, 5) isolation of the database server and 6) provision of an
investigation environment. CIMP was found capable of helping the practitioners
and newcomers to the forensics domain to control database crimes.","Ibrahim Alfadli, Fahad M Ghabban, Omair Ameerbakhsh, Amer Nizar AbuAli, Arafat Al-Dhaqm, Mahmoud Ahmad Al-Khasawneh","CIPM: Common Identification Process Model for Database Forensics Field. Database Forensics (DBF) domain is a branch of digital forensics, concerned
with the identification, collection, reconstruction, analysis, and
documentation of database crimes. Different researchers have introduced several
identification models to handle database crimes. Majority of proposed models
are not specific and are redundant, which makes these models a problem because
of the multidimensional nature and high diversity of database systems.
Accordingly, using the metamodeling approach, the current study is aimed at
proposing a unified identification model applicable to the database forensic
field. The model integrates and harmonizes all exiting identification processes
into a single abstract model, called Common Identification Process Model
(CIPM). The model comprises six phases: 1) notifying an incident, 2) responding
to the incident, 3) identification of the incident source, 4) verification of
the incident, 5) isolation of the database server and 6) provision of an
investigation environment. CIMP was found capable of helping the practitioners
and newcomers to the forensics domain to control database crimes.","CIPM, Database Forensics, CIMP, Digital Forensics"
Quantum-Inspired Keyword Search on Multi-Model Databases,"With the rising applications implemented in different domains, it is
inevitable to require databases to adopt corresponding appropriate data models
to store and exchange data derived from various sources. To handle these data
models in a single platform, the community of databases introduces a
multi-model database. And many vendors are improving their products from
supporting a single data model to being multi-model databases. Although this
brings benefits, spending lots of enthusiasm to master one of the multi-model
query languages for exploring a database is unfriendly to most users.
Therefore, we study using keyword searches as an alternative way to explore and
query multi-model databases. In this paper, we attempt to utilize quantum
physics's probabilistic formalism to bring the problem into vector spaces and
represent events (e.g., words) as subspaces. Then we employ a density matrix to
encapsulate all the information over these subspaces and use density matrices
to measure the divergence between query and candidate answers for finding
top-\textit{k} the most relevant results. In this process, we propose using
pattern mining to identify compounds for improving accuracy and using
dimensionality reduction for reducing complexity. Finally, empirical
experiments demonstrate the performance superiority of our approaches over the
state-of-the-art approaches.","Gongsheng Yuan, Jiaheng Lu, Peifeng Su","Quantum-Inspired Keyword Search on Multi-Model Databases. With the rising applications implemented in different domains, it is
inevitable to require databases to adopt corresponding appropriate data models
to store and exchange data derived from various sources. To handle these data
models in a single platform, the community of databases introduces a
multi-model database. And many vendors are improving their products from
supporting a single data model to being multi-model databases. Although this
brings benefits, spending lots of enthusiasm to master one of the multi-model
query languages for exploring a database is unfriendly to most users.
Therefore, we study using keyword searches as an alternative way to explore and
query multi-model databases. In this paper, we attempt to utilize quantum
physics's probabilistic formalism to bring the problem into vector spaces and
represent events (e.g., words) as subspaces. Then we employ a density matrix to
encapsulate all the information over these subspaces and use density matrices
to measure the divergence between query and candidate answers for finding
top-\textit{k} the most relevant results. In this process, we propose using
pattern mining to identify compounds for improving accuracy and using
dimensionality reduction for reducing complexity. Finally, empirical
experiments demonstrate the performance superiority of our approaches over the
state-of-the-art approaches.","Quantum-Inspired Keyword Search, Multi-Model Databases, Pattern Mining, Density Matrices"
"Demonstrating CAT: Synthesizing Data-Aware Conversational Agents for
  Transactional Databases","Databases for OLTP are often the backbone for applications such as hotel room
or cinema ticket booking applications. However, developing a conversational
agent (i.e., a chatbot-like interface) to allow end-users to interact with an
application using natural language requires both immense amounts of training
data and NLP expertise. This motivates CAT, which can be used to easily create
conversational agents for transactional databases. The main idea is that, for a
given OLTP database, CAT uses weak supervision to synthesize the required
training data to train a state-of-the-art conversational agent, allowing users
to interact with the OLTP database. Furthermore, CAT provides an out-of-the-box
integration of the resulting agent with the database. As a major difference to
existing conversational agents, agents synthesized by CAT are data-aware. This
means that the agent decides which information should be requested from the
user based on the current data distributions in the database, which typically
results in markedly more efficient dialogues compared with non-data-aware
agents. We publish the code for CAT as open source.","Marius Gassen, Benjamin Hättasch, Benjamin Hilprecht, Nadja Geisler, Alexander Fraser, Carsten Binnig","Demonstrating CAT: Synthesizing Data-Aware Conversational Agents for
  Transactional Databases. Databases for OLTP are often the backbone for applications such as hotel room
or cinema ticket booking applications. However, developing a conversational
agent (i.e., a chatbot-like interface) to allow end-users to interact with an
application using natural language requires both immense amounts of training
data and NLP expertise. This motivates CAT, which can be used to easily create
conversational agents for transactional databases. The main idea is that, for a
given OLTP database, CAT uses weak supervision to synthesize the required
training data to train a state-of-the-art conversational agent, allowing users
to interact with the OLTP database. Furthermore, CAT provides an out-of-the-box
integration of the resulting agent with the database. As a major difference to
existing conversational agents, agents synthesized by CAT are data-aware. This
means that the agent decides which information should be requested from the
user based on the current data distributions in the database, which typically
results in markedly more efficient dialogues compared with non-data-aware
agents. We publish the code for CAT as open source.","CAT, Data-Aware, Conversational Agents, OLTP, NLP, Open Source"
"Modeling and Performance Analysis of Single-Server Database Over
  Quasi-static Rayleigh Fading Channel","Cloud database is the key technology in cloud computing. The effective and
efficient service quality of the cloud database is inseparable from
communication technology, just as improving communication quality will reduce
the concurrency phenomenon in the ticketing system. In order to visually
observe the impact of communication on the cloud database, we propose a
Communication-Database (C-D) Model with a single-server database over the
quasi-static Rayleigh fading channel, which consists of three parts: CLIENTS
SOURCE, COMMUNICATION SYSTEM and DATABASE SYSTEM. This paper uses the queuing
model, M/G/1//K, to model the whole system. The C-D Model is analyzed in two
cases: nonlinearity and linearity, which correspond to some instances of SISO
and MIMO. The simulation results of average staying time, average number of
transactions and other performance characteristics are basically consistent
with the theoretical results, which verifies the validity of the C-D Model. The
comparison of these experimental results also proves that poor communication
quality does lead to the reduction in the quality of service.","Mengying Chen, Wannian An, Yang Liu, Chen Dong, Xiaodong Xu, Boxiao Han, Ping Zhang","Modeling and Performance Analysis of Single-Server Database Over
  Quasi-static Rayleigh Fading Channel. Cloud database is the key technology in cloud computing. The effective and
efficient service quality of the cloud database is inseparable from
communication technology, just as improving communication quality will reduce
the concurrency phenomenon in the ticketing system. In order to visually
observe the impact of communication on the cloud database, we propose a
Communication-Database (C-D) Model with a single-server database over the
quasi-static Rayleigh fading channel, which consists of three parts: CLIENTS
SOURCE, COMMUNICATION SYSTEM and DATABASE SYSTEM. This paper uses the queuing
model, M/G/1//K, to model the whole system. The C-D Model is analyzed in two
cases: nonlinearity and linearity, which correspond to some instances of SISO
and MIMO. The simulation results of average staying time, average number of
transactions and other performance characteristics are basically consistent
with the theoretical results, which verifies the validity of the C-D Model. The
comparison of these experimental results also proves that poor communication
quality does lead to the reduction in the quality of service.","Communication-Database, C-D Model, Single-Server Database, Rayleigh Fading Channel, SISO, MIMO, Cloud Computing"
Benchmarking Specialized Databases for High-frequency Data,"This paper presents a benchmarking suite designed for the evaluation and
comparison of time series databases for high-frequency data, with a focus on
financial applications. The proposed suite comprises of four specialized
databases: ClickHouse, InfluxDB, kdb+ and TimescaleDB. The results from the
suite demonstrate that kdb+ has the highest performance amongst the tested
databases, while also highlighting the strengths and weaknesses of each of the
databases. The benchmarking suite was designed to provide an objective measure
of the performance of these databases as well as to compare their capabilities
for different types of data. This provides valuable insights into the
suitability of different time series databases for different use cases and
provides benchmarks that can be used to inform system design decisions.","Fazl Barez, Paul Bilokon, Ruijie Xiong","Benchmarking Specialized Databases for High-frequency Data. This paper presents a benchmarking suite designed for the evaluation and
comparison of time series databases for high-frequency data, with a focus on
financial applications. The proposed suite comprises of four specialized
databases: ClickHouse, InfluxDB, kdb+ and TimescaleDB. The results from the
suite demonstrate that kdb+ has the highest performance amongst the tested
databases, while also highlighting the strengths and weaknesses of each of the
databases. The benchmarking suite was designed to provide an objective measure
of the performance of these databases as well as to compare their capabilities
for different types of data. This provides valuable insights into the
suitability of different time series databases for different use cases and
provides benchmarks that can be used to inform system design decisions.","Benchmarking, Time Series Databases, ClickHouse, InfluxDB, kdb+, TimescaleDB"
Incremental Consistent Updating of Incomplete Databases,"Efficient consistency maintenance of incomplete and dynamic real-life
databases is a quality label for further data analysis. In prior work, we
tackled the generic problem of database updating in the presence of tuple
generating constraints from a theoretical viewpoint. The current paper
considers the usability of our approach by (a) introducing incremental update
routines (instead of the previous from-scratch versions) and (b) removing the
restriction that limits the contents of the database to fit in the main memory.
In doing so, this paper offers new algorithms, proposes queries and data models
inviting discussions on the representation of incompleteness on databases. We
also propose implementations under a graph database model and the traditional
relational database model. Our experiments show that computation times are
similar globally but point to discrepancies in some steps.","Jacques Chabin, Mirian Halfeld Ferrari, Nicolas Hiot, Dominique Laurent","Incremental Consistent Updating of Incomplete Databases. Efficient consistency maintenance of incomplete and dynamic real-life
databases is a quality label for further data analysis. In prior work, we
tackled the generic problem of database updating in the presence of tuple
generating constraints from a theoretical viewpoint. The current paper
considers the usability of our approach by (a) introducing incremental update
routines (instead of the previous from-scratch versions) and (b) removing the
restriction that limits the contents of the database to fit in the main memory.
In doing so, this paper offers new algorithms, proposes queries and data models
inviting discussions on the representation of incompleteness on databases. We
also propose implementations under a graph database model and the traditional
relational database model. Our experiments show that computation times are
similar globally but point to discrepancies in some steps.","Incremental Consistent Updating, Incomplete Databases, Graph Database, Relational Database"
IWEK: An Interpretable What-If Estimator for Database Knobs,"The knobs of modern database management systems have significant impact on
the performance of the systems. With the development of cloud databases, an
estimation service for knobs is urgently needed to improve the performance of
database. Unfortunately, few attentions have been paid to estimate the
performance of certain knob configurations. To fill this gap, we propose IWEK,
an interpretable & transferable what-if estimator for database knobs. To
achieve interpretable estimation, we propose linear estimator based on the
random forest for database knobs for the explicit and trustable evaluation
results. Due to its interpretability, our estimator capture the direct
relationships between knob configuration and its performance, to guarantee the
high availability of database. We design a two-stage transfer algorithm to
leverage historical experiences to efficiently build the knob estimator for new
scenarios. Due to its lightweight design, our method can largely reduce the
overhead of collecting training data and could achieve cold start knob
estimation for new scenarios. Extensive experiments on YCSB and TPCC show that
our method performs well in interpretable and transferable knob estimation with
limited training data. Further, our method could achieve efficient estimator
transfer with only 10 samples in TPCC and YSCB.","Yu Yan, Hongzhi Wang, Jian Geng, Jian Ma, Geng Li, Zixuan Wang, Zhiyu Dai, Tianqing Wang","IWEK: An Interpretable What-If Estimator for Database Knobs. The knobs of modern database management systems have significant impact on
the performance of the systems. With the development of cloud databases, an
estimation service for knobs is urgently needed to improve the performance of
database. Unfortunately, few attentions have been paid to estimate the
performance of certain knob configurations. To fill this gap, we propose IWEK,
an interpretable & transferable what-if estimator for database knobs. To
achieve interpretable estimation, we propose linear estimator based on the
random forest for database knobs for the explicit and trustable evaluation
results. Due to its interpretability, our estimator capture the direct
relationships between knob configuration and its performance, to guarantee the
high availability of database. We design a two-stage transfer algorithm to
leverage historical experiences to efficiently build the knob estimator for new
scenarios. Due to its lightweight design, our method can largely reduce the
overhead of collecting training data and could achieve cold start knob
estimation for new scenarios. Extensive experiments on YCSB and TPCC show that
our method performs well in interpretable and transferable knob estimation with
limited training data. Further, our method could achieve efficient estimator
transfer with only 10 samples in TPCC and YSCB.","IWEK, Database Knobs, Knobs Estimator, YCSB, TPCC"
DBJoules: An Energy Measurement Tool for Database Management Systems,"In the rapidly evolving landscape of modern data-driven technologies,
software relies on large datasets and constant data center operations using
various database systems to support computation-intensive tasks. As energy
consumption in software systems becomes a growing concern, selecting the right
database from energy-efficiency perspective is also critical. To address this,
we introduce \textbf{\textit{DBJoules}}, a tool that measures the energy
consumption of activities in database systems. \textit{DBJoules} supports
energy measurement of CRUD operations for four popular databases. Through
evaluations on two widely-used datasets, we identify disparities of 7\% to 38\%
in the energy consumption of these databases. Hence, the goal is to raise
developer awareness about the effect of running queries in different databases
from an energy consumption perspective, enabling them to select appropriate
database for sustainable usage. The tool's demonstration is available at
\url{https://youtu.be/D1MTZum0jok} and related artifacts at
\url{https://rishalab.github.io/DBJoules/}.","Hemasri Sai Lella, Kurra Manasa, Rajrupa Chattaraj, Sridhar Chimalakonda","DBJoules: An Energy Measurement Tool for Database Management Systems. In the rapidly evolving landscape of modern data-driven technologies,
software relies on large datasets and constant data center operations using
various database systems to support computation-intensive tasks. As energy
consumption in software systems becomes a growing concern, selecting the right
database from energy-efficiency perspective is also critical. To address this,
we introduce \textbf{\textit{DBJoules}}, a tool that measures the energy
consumption of activities in database systems. \textit{DBJoules} supports
energy measurement of CRUD operations for four popular databases. Through
evaluations on two widely-used datasets, we identify disparities of 7\% to 38\%
in the energy consumption of these databases. Hence, the goal is to raise
developer awareness about the effect of running queries in different databases
from an energy consumption perspective, enabling them to select appropriate
database for sustainable usage. The tool's demonstration is available at
\url{https://youtu.be/D1MTZum0jok} and related artifacts at
\url{https://rishalab.github.io/DBJoules/}.","DBJoules, Database Management, Energy Measurement, CRUD"
Consistent Query Answering for Existential Rules with Closed Predicates,"Consistent Query Answering (CQA) is an inconsistency-tolerant approach to
data access in knowledge bases and databases. The goal of CQA is to provide
meaningful (consistent) answers to queries even in the presence of inconsistent
information, e.g. a database whose data conflict with meta-data (typically the
database integrity constraints). The semantics of CQA is based on the notion of
repair, that is, a consistent version of the initial, inconsistent database
that is obtained through minimal modifications. We study CQA in databases with
data dependencies expressed by existential rules. More specifically, we focus
on the broad class of disjunctive embedded dependencies with inequalities
(DEDs), which extend both tuple-generating dependencies and equality-generated
dependencies. We first focus on the case when the database predicates are
closed, i.e. the database is assumed to have complete knowledge about such
predicates, thus no tuple addition is possible to repair the database. In such
a scenario, we provide a detailed analysis of the data complexity of CQA and
associated tasks (repair checking) under different semantics (AR and IAR) and
for different classes of existential rules. In particular, we consider the
classes of acyclic, linear, full, sticky and guarded DEDs, and their
combinations.","Lorenzo Marconi, Riccardo Rosati","Consistent Query Answering for Existential Rules with Closed Predicates. Consistent Query Answering (CQA) is an inconsistency-tolerant approach to
data access in knowledge bases and databases. The goal of CQA is to provide
meaningful (consistent) answers to queries even in the presence of inconsistent
information, e.g. a database whose data conflict with meta-data (typically the
database integrity constraints). The semantics of CQA is based on the notion of
repair, that is, a consistent version of the initial, inconsistent database
that is obtained through minimal modifications. We study CQA in databases with
data dependencies expressed by existential rules. More specifically, we focus
on the broad class of disjunctive embedded dependencies with inequalities
(DEDs), which extend both tuple-generating dependencies and equality-generated
dependencies. We first focus on the case when the database predicates are
closed, i.e. the database is assumed to have complete knowledge about such
predicates, thus no tuple addition is possible to repair the database. In such
a scenario, we provide a detailed analysis of the data complexity of CQA and
associated tasks (repair checking) under different semantics (AR and IAR) and
for different classes of existential rules. In particular, we consider the
classes of acyclic, linear, full, sticky and guarded DEDs, and their
combinations.","Consistent Query Answering, CQA, Existential Rules, Closed Predicates, Database Integrity"
"QCDGE database, Quantum Chemistry Database with Ground- and
  Excited-state Properties of 450 Kilo Molecules","Due to rapid advancements in deep learning techniques, the demand for
large-volume high-quality databases grows significantly in chemical research.
We developed a quantum-chemistry database that includes 443,106 small organic
molecules with sizes up to 10 heavy atoms including carbon (C), nitrogen (N),
oxygen (O), and fluorine (F). Ground-state geometry optimizations and frequency
calculations of all compounds were performed at the B3LYP/6-31G* level with the
BJD3 dispersion correction, while the excited-state single-point calculations
were conducted at the $\omega$B97X-D/6-31G* level. Totally twenty seven
molecular properties, such as geometric, thermodynamic, electronic and
energetic properties, were gathered from these calculations. Meanwhile, we also
established a comprehensive protocol for the construction of a high-volume
quantum-chemistry database. Our QCDGE (Quantum Chemistry Database with Ground-
and Excited-State Properties) database contains a substantial volume of data,
exhibits high chemical diversity, and most importantly includes excited-state
information. This database, along with its construction protocol, is expected
to have a significant impact on the broad applications of machine learning
studies across different fields of chemistry, especially in the area of
excited-state research.","Yifei Zhu, Mengge Li, Chao Xu, Zhenggang Lan","QCDGE database, Quantum Chemistry Database with Ground- and
  Excited-state Properties of 450 Kilo Molecules. Due to rapid advancements in deep learning techniques, the demand for
large-volume high-quality databases grows significantly in chemical research.
We developed a quantum-chemistry database that includes 443,106 small organic
molecules with sizes up to 10 heavy atoms including carbon (C), nitrogen (N),
oxygen (O), and fluorine (F). Ground-state geometry optimizations and frequency
calculations of all compounds were performed at the B3LYP/6-31G* level with the
BJD3 dispersion correction, while the excited-state single-point calculations
were conducted at the $\omega$B97X-D/6-31G* level. Totally twenty seven
molecular properties, such as geometric, thermodynamic, electronic and
energetic properties, were gathered from these calculations. Meanwhile, we also
established a comprehensive protocol for the construction of a high-volume
quantum-chemistry database. Our QCDGE (Quantum Chemistry Database with Ground-
and Excited-State Properties) database contains a substantial volume of data,
exhibits high chemical diversity, and most importantly includes excited-state
information. This database, along with its construction protocol, is expected
to have a significant impact on the broad applications of machine learning
studies across different fields of chemistry, especially in the area of
excited-state research.","QCDGE, Quantum Chemistry Database, Ground- and Excited-State Properties"
"Implementing the Typed Graph Data Model Using Relational Database
  Technology","Recent standardization work for database languages has reflected the growing
use of typed graph models (TGM) in application development. Such data models
are frequently only used early in the design process, and not reflected
directly in underlying physical database. In previous work, we have added
support to a relational database management system (RDBMS) with role-based
structures to ensure that relevant data models are not separately declared in
each application but are an important part of the database implementation. In
this work, we implement this approach for the TGM: the resulting database
implementation is novel in retaining the best features of the graph-based and
relational database technologies.","Malcolm Crowe, Fritz Laux","Implementing the Typed Graph Data Model Using Relational Database
  Technology. Recent standardization work for database languages has reflected the growing
use of typed graph models (TGM) in application development. Such data models
are frequently only used early in the design process, and not reflected
directly in underlying physical database. In previous work, we have added
support to a relational database management system (RDBMS) with role-based
structures to ensure that relevant data models are not separately declared in
each application but are an important part of the database implementation. In
this work, we implement this approach for the TGM: the resulting database
implementation is novel in retaining the best features of the graph-based and
relational database technologies.","Typeed Graph Data Model, TGM, Relational Database, Database Management System, RDBMS, Role-based"
"Dung's Argumentation Framework: Unveiling the Expressive Power with
  Inconsistent Databases","The connection between inconsistent databases and Dung's abstract
argumentation framework has recently drawn growing interest. Specifically, an
inconsistent database, involving certain types of integrity constraints such as
functional and inclusion dependencies, can be viewed as an argumentation
framework in Dung's setting. Nevertheless, no prior work has explored the exact
expressive power of Dung's theory of argumentation when compared to
inconsistent databases and integrity constraints. In this paper, we close this
gap by arguing that an argumentation framework can also be viewed as an
inconsistent database. We first establish a connection between subset-repairs
for databases and extensions for AFs, considering conflict-free, naive,
admissible, and preferred semantics. Further, we define a new family of
attribute-based repairs based on the principle of maximal content preservation.
The effectiveness of these repairs is then highlighted by connecting them to
stable, semi-stable, and stage semantics. Our main contributions include
translating an argumentation framework into a database together with integrity
constraints. Moreover, this translation can be achieved in polynomial time,
which is essential in transferring complexity results between the two
formalisms.","Yasir Mahmood, Markus Hecher, Axel-Cyrille Ngonga Ngomo","Dung's Argumentation Framework: Unveiling the Expressive Power with
  Inconsistent Databases. The connection between inconsistent databases and Dung's abstract
argumentation framework has recently drawn growing interest. Specifically, an
inconsistent database, involving certain types of integrity constraints such as
functional and inclusion dependencies, can be viewed as an argumentation
framework in Dung's setting. Nevertheless, no prior work has explored the exact
expressive power of Dung's theory of argumentation when compared to
inconsistent databases and integrity constraints. In this paper, we close this
gap by arguing that an argumentation framework can also be viewed as an
inconsistent database. We first establish a connection between subset-repairs
for databases and extensions for AFs, considering conflict-free, naive,
admissible, and preferred semantics. Further, we define a new family of
attribute-based repairs based on the principle of maximal content preservation.
The effectiveness of these repairs is then highlighted by connecting them to
stable, semi-stable, and stage semantics. Our main contributions include
translating an argumentation framework into a database together with integrity
constraints. Moreover, this translation can be achieved in polynomial time,
which is essential in transferring complexity results between the two
formalisms.","Dung, Argumentation Framework, Inconsistent Databases, Integrity Constraints, maximal content preservation"
"Dynamic Data Defense: Unveiling the Database in motion Chaos Encryption
  (DaChE) Algorithm -- A Breakthrough in Chaos Theory for Enhanced Database
  Security","Amidst the burgeoning landscape of database architectures, the surge in NoSQL
databases has heralded a transformative era, liberating data storage from
traditional relational constraints and ushering in unprecedented scalability.
As organizations grapple with the escalating security threats posed by database
breaches, a novel theoretical framework emerges at the nexus of chaos theory
and topology: the Database in motion Chaos Encryption (DaChE) Algorithm. This
paradigm-shifting approach challenges the static nature of data storage,
advocating for dynamic data motion to fortify database security. By
incorporating chaos theory, this innovative strategy not only enhances database
defenses against evolving attack vectors but also redefines the boundaries of
data protection, offering a paradigmatic shift in safeguarding critical
information assets. Additionally, it enables parallel processing, facilitating
on-the-fly processing and optimizing the performance of the proposed framework.",Abraham Itzhak Weinberg,"Dynamic Data Defense: Unveiling the Database in motion Chaos Encryption
  (DaChE) Algorithm -- A Breakthrough in Chaos Theory for Enhanced Database
  Security. Amidst the burgeoning landscape of database architectures, the surge in NoSQL
databases has heralded a transformative era, liberating data storage from
traditional relational constraints and ushering in unprecedented scalability.
As organizations grapple with the escalating security threats posed by database
breaches, a novel theoretical framework emerges at the nexus of chaos theory
and topology: the Database in motion Chaos Encryption (DaChE) Algorithm. This
paradigm-shifting approach challenges the static nature of data storage,
advocating for dynamic data motion to fortify database security. By
incorporating chaos theory, this innovative strategy not only enhances database
defenses against evolving attack vectors but also redefines the boundaries of
data protection, offering a paradigmatic shift in safeguarding critical
information assets. Additionally, it enables parallel processing, facilitating
on-the-fly processing and optimizing the performance of the proposed framework.","Dynamic Data Defense, Chaos Encryption, DaChE Algorithm, NoSQL"
Codd's Theorem for Databases over Semirings,"Codd's Theorem, a fundamental result of database theory, asserts that
relational algebra and relational calculus have the same expressive power on
relational databases. We explore Codd's Theorem for databases over semirings
and establish two different versions of this result for such databases: the
first version involves the five basic operations of relational algebra, while
in the second version the division operation is added to the five basic
operations of relational algebra. In both versions, the difference operation of
relations is given semantics using semirings with monus, while on the side of
relational calculus a limited form of negation is used. The reason for
considering these two different versions of Codd's theorem is that, unlike the
case of ordinary relational databases, the division operation need not be
expressible in terms of the five basic operations of relational algebra for
databases over an arbitrary positive semiring; in fact, we show that this
inexpressibility result holds even for bag databases.","Guillermo Badia, Phokion G. Kolaitis, Carles Noguera","Codd's Theorem for Databases over Semirings. Codd's Theorem, a fundamental result of database theory, asserts that
relational algebra and relational calculus have the same expressive power on
relational databases. We explore Codd's Theorem for databases over semirings
and establish two different versions of this result for such databases: the
first version involves the five basic operations of relational algebra, while
in the second version the division operation is added to the five basic
operations of relational algebra. In both versions, the difference operation of
relations is given semantics using semirings with monus, while on the side of
relational calculus a limited form of negation is used. The reason for
considering these two different versions of Codd's theorem is that, unlike the
case of ordinary relational databases, the division operation need not be
expressible in terms of the five basic operations of relational algebra for
databases over an arbitrary positive semiring; in fact, we show that this
inexpressibility result holds even for bag databases.","Codd's Theorem, Database Theory, Semirings, Relational Algebra"
A database of rigorous Maass forms,"We announce a database of rigorously computed Maass forms on congruence
subgroups $\Gamma_0(N)$ and briefly describe the methods of computation.",David Lowry-Duda,"A database of rigorous Maass forms. We announce a database of rigorously computed Maass forms on congruence
subgroups $\Gamma_0(N)$ and briefly describe the methods of computation.","Maass Forms, Congruencesubgroups, Convexity"
Graph Neural Networks for Databases: A Survey,"Graph neural networks (GNNs) are powerful deep learning models for
graph-structured data, demonstrating remarkable success across diverse domains.
Recently, the database (DB) community has increasingly recognized the
potentiality of GNNs, prompting a surge of researches focusing on improving
database systems through GNN-based approaches. However, despite notable
advances, There is a lack of a comprehensive review and understanding of how
GNNs could improve DB systems. Therefore, this survey aims to bridge this gap
by providing a structured and in-depth overview of GNNs for DB systems.
Specifically, we propose a new taxonomy that classifies existing methods into
two key categories: (1) Relational Databases, which includes tasks like
performance prediction, query optimization, and text-to-SQL, and (2) Graph
Databases, addressing challenges like efficient graph query processing and
graph similarity computation. We systematically review key methods in each
category, highlighting their contributions and practical implications. Finally,
we suggest promising avenues for integrating GNNs into Database systems.","Ziming Li, Youhuan Li, Yuyu Luo, Guoliang Li, Chuxu Zhang","Graph Neural Networks for Databases: A Survey. Graph neural networks (GNNs) are powerful deep learning models for
graph-structured data, demonstrating remarkable success across diverse domains.
Recently, the database (DB) community has increasingly recognized the
potentiality of GNNs, prompting a surge of researches focusing on improving
database systems through GNN-based approaches. However, despite notable
advances, There is a lack of a comprehensive review and understanding of how
GNNs could improve DB systems. Therefore, this survey aims to bridge this gap
by providing a structured and in-depth overview of GNNs for DB systems.
Specifically, we propose a new taxonomy that classifies existing methods into
two key categories: (1) Relational Databases, which includes tasks like
performance prediction, query optimization, and text-to-SQL, and (2) Graph
Databases, addressing challenges like efficient graph query processing and
graph similarity computation. We systematically review key methods in each
category, highlighting their contributions and practical implications. Finally,
we suggest promising avenues for integrating GNNs into Database systems.","Graph Neural Networks, Databases, Deep Learning, Performance Prediction, Query Optimization, Text-to-SQL"
Path Database Guidance for Motion Planning,"One approach to using prior experience in robot motion planning is to store
solutions to previously seen problems in a database of paths. Methods that use
such databases are characterized by how they query for a path and how they use
queries given a new problem. In this work we present a new method, Path
Database Guidance (PDG), which innovates on existing work in two ways. First,
we use the database to compute a heuristic for determining which nodes of a
search tree to expand, in contrast to prior work which generally pastes the
(possibly transformed) queried path or uses it to bias a sampling distribution.
We demonstrate that this makes our method more easily composable with other
search methods by dynamically interleaving exploration according to a baseline
algorithm with exploitation of the database guidance. Second, in contrast to
other methods that treat the database as a single fixed prior, our database
(and thus our queried heuristic) updates as we search the implicitly defined
robot configuration space. We experimentally demonstrate the effectiveness of
PDG in a variety of explicitly defined environment distributions in simulation.","Amnon Attali, Praval Telagi, Marco Morales, Nancy M. Amato","Path Database Guidance for Motion Planning. One approach to using prior experience in robot motion planning is to store
solutions to previously seen problems in a database of paths. Methods that use
such databases are characterized by how they query for a path and how they use
queries given a new problem. In this work we present a new method, Path
Database Guidance (PDG), which innovates on existing work in two ways. First,
we use the database to compute a heuristic for determining which nodes of a
search tree to expand, in contrast to prior work which generally pastes the
(possibly transformed) queried path or uses it to bias a sampling distribution.
We demonstrate that this makes our method more easily composable with other
search methods by dynamically interleaving exploration according to a baseline
algorithm with exploitation of the database guidance. Second, in contrast to
other methods that treat the database as a single fixed prior, our database
(and thus our queried heuristic) updates as we search the implicitly defined
robot configuration space. We experimentally demonstrate the effectiveness of
PDG in a variety of explicitly defined environment distributions in simulation.","Path Database Guidance, Motion Planning, Database, Robot Configuration Space, Simulation"
"LogDB: Multivariate Log-based Failure Diagnosis for Distributed
  Databases (Extended from MultiLog)","Distributed databases, as the core infrastructure software for internet
applications, play a critical role in modern cloud services. However, existing
distributed databases frequently experience system failures and performance
degradation, often leading to significant economic losses. Log data, naturally
generated within systems, can effectively reflect internal system states. In
practice, operators often manually inspect logs to monitor system behavior and
diagnose anomalies, a process that is labor-intensive and costly. Although
various log-based failure diagnosis methods have been proposed, they are
generally not tailored for database systems and fail to fully exploit the
internal characteristics and distributed nature of these systems. To address
this gap, we propose LogDB, a log-based failure diagnosis method specifically
designed for distributed databases. LogDB extracts and compresses log features
at each database node and then aggregates these features at the master node to
diagnose cluster-wide anomalies. Experiments conducted on the open-source
distributed database system Apache IoTDB demonstrate that LogDB achieves robust
failure diagnosis performance across different workloads and a variety of
anomaly types.","Lingzhe Zhang, Tong Jia, Mengxi Jia, Ying Li","LogDB: Multivariate Log-based Failure Diagnosis for Distributed
  Databases (Extended from MultiLog). Distributed databases, as the core infrastructure software for internet
applications, play a critical role in modern cloud services. However, existing
distributed databases frequently experience system failures and performance
degradation, often leading to significant economic losses. Log data, naturally
generated within systems, can effectively reflect internal system states. In
practice, operators often manually inspect logs to monitor system behavior and
diagnose anomalies, a process that is labor-intensive and costly. Although
various log-based failure diagnosis methods have been proposed, they are
generally not tailored for database systems and fail to fully exploit the
internal characteristics and distributed nature of these systems. To address
this gap, we propose LogDB, a log-based failure diagnosis method specifically
designed for distributed databases. LogDB extracts and compresses log features
at each database node and then aggregates these features at the master node to
diagnose cluster-wide anomalies. Experiments conducted on the open-source
distributed database system Apache IoTDB demonstrate that LogDB achieves robust
failure diagnosis performance across different workloads and a variety of
anomaly types.","LogDB, Multivariate Log-based Failure Diagnosis, Distributed Databases, Apache IoTDB"
Search-Based Fuzzing For RESTful APIs That Use MongoDB,"In RESTful APIs, interactions with a database are a common and crucial
aspect. When generating whitebox tests, it is essential to consider the
database's state (i.e., the data contained in the database) to achieve higher
code coverage and uncover more hidden faults. This article presents novel
techniques to enhance search-based software test generation for RESTful APIs
interacting with NoSQL databases. Specifically, we target the popular MongoDB
database, by dynamically analyzing (via automated code instrumentation) the
state of the database during the test generation process. Additionally, to
achieve better results, our novel approach allows inserting NoSQL data directly
from test cases. This is particularly beneficial when generating the correct
sequence of events to set the NoSQL database in an appropriate state is
challenging or time-consuming. This method is also advantageous for testing
read-only microservices. Our novel techniques are implemented as an extension
of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.
Experiments conducted on six RESTful APIs demonstrated significant improvements
in code coverage, with increases of up to 18% compared to existing white-box
approaches. To better highlight the improvements of our novel techniques,
comparisons are also carried out with four state-of-the-art black-box fuzzers.","Hernan Ghianni, Man Zhang, Juan P. Galeotti, Andrea Arcuri","Search-Based Fuzzing For RESTful APIs That Use MongoDB. In RESTful APIs, interactions with a database are a common and crucial
aspect. When generating whitebox tests, it is essential to consider the
database's state (i.e., the data contained in the database) to achieve higher
code coverage and uncover more hidden faults. This article presents novel
techniques to enhance search-based software test generation for RESTful APIs
interacting with NoSQL databases. Specifically, we target the popular MongoDB
database, by dynamically analyzing (via automated code instrumentation) the
state of the database during the test generation process. Additionally, to
achieve better results, our novel approach allows inserting NoSQL data directly
from test cases. This is particularly beneficial when generating the correct
sequence of events to set the NoSQL database in an appropriate state is
challenging or time-consuming. This method is also advantageous for testing
read-only microservices. Our novel techniques are implemented as an extension
of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.
Experiments conducted on six RESTful APIs demonstrated significant improvements
in code coverage, with increases of up to 18% compared to existing white-box
approaches. To better highlight the improvements of our novel techniques,
comparisons are also carried out with four state-of-the-art black-box fuzzers.","RESTful APIs, NoSQL, MongoDB, EvoMaster"
Object-oriented tools for advanced applications,"This paper contains a brief discussion of the Application Development
Environment (ADE) that is used to build database applications involving the
graphical user interface (GUI). ADE computing separates the database access and
the user interface. The variety of applications may be generated that
communicate with different and distinct desktop databases. The advanced
techniques allows to involve remote or stored procedures retrieval and call.","Larissa Ismailova, Konstantin Zinchenko","Object-oriented tools for advanced applications. This paper contains a brief discussion of the Application Development
Environment (ADE) that is used to build database applications involving the
graphical user interface (GUI). ADE computing separates the database access and
the user interface. The variety of applications may be generated that
communicate with different and distinct desktop databases. The advanced
techniques allows to involve remote or stored procedures retrieval and call.","Object-Oriented Tools, ADE, GUI, Database Applications"
Improving Web Database Access Using Decision Diagrams,"In some areas of management and commerce, especially in Electronic commerce
(E-commerce), that are accelerated by advances in Web technologies, it is
essential to support the decision making process using formal methods. Among
the problems of E-commerce applications: reducing the time of data access so
that huge databases can be searched quickly; decreasing the cost of database
design ... etc. We present the application of Decision Diagrams design using
Information Theory approach to improve database access speeds. We show that
such utilization provides systematic and visual ways of applying Decision
Making methods to simplify complex Web engineering problems.","Denis V. Popel, Nawar Al-Hakeem","Improving Web Database Access Using Decision Diagrams. In some areas of management and commerce, especially in Electronic commerce
(E-commerce), that are accelerated by advances in Web technologies, it is
essential to support the decision making process using formal methods. Among
the problems of E-commerce applications: reducing the time of data access so
that huge databases can be searched quickly; decreasing the cost of database
design ... etc. We present the application of Decision Diagrams design using
Information Theory approach to improve database access speeds. We show that
such utilization provides systematic and visual ways of applying Decision
Making methods to simplify complex Web engineering problems.","Decision Diagrams, Database Access, E-commerce, Information Theory"
A Web Based Document Database,"We describe a document database, developed for BTeV, which has now been
adopted for use by other collaborations. A single web based database and
archival system is used to maintain public and internal documents as well as
documents for a related collaboration. The database performs archiving,
versioning, access control, and easy remote access and submission. We cover the
technical and security requirements of the database and its implementation.
Usage patterns, improvements in our collaborative style, and missteps along the
way are also discussed.",E. W. Vaandering,"A Web Based Document Database. We describe a document database, developed for BTeV, which has now been
adopted for use by other collaborations. A single web based database and
archival system is used to maintain public and internal documents as well as
documents for a related collaboration. The database performs archiving,
versioning, access control, and easy remote access and submission. We cover the
technical and security requirements of the database and its implementation.
Usage patterns, improvements in our collaborative style, and missteps along the
way are also discussed.","JavaScript, Document Database, BTeV, Collaboration"
An Integrated Enterprise Accelerator Database for the SLC Control System,"Since its inception in the early 1980's, the SLC control system has been
driven by a highly structured memory resident real-time database. While
efficient, its rigid structure and file-based sources makes it difficult to
maintain and extract relevant information. The goal of transforming the sources
for this database into a relational form is to enable it to be part of a
Control System Enterprise Database that is an integrated central repository for
SLC accelerator device and control system data with links to other associated
databases.","T. Lahey, J. Rock, R. Sass, H. Shoaee, K. Underwood","An Integrated Enterprise Accelerator Database for the SLC Control System. Since its inception in the early 1980's, the SLC control system has been
driven by a highly structured memory resident real-time database. While
efficient, its rigid structure and file-based sources makes it difficult to
maintain and extract relevant information. The goal of transforming the sources
for this database into a relational form is to enable it to be part of a
Control System Enterprise Database that is an integrated central repository for
SLC accelerator device and control system data with links to other associated
databases.","SLC Control System, Enterprise Accelerator Database, Real-time Database, Relational Databases"
"Quantum computers can search arbitrarily large databases by a single
  query","This paper shows that a quantum mechanical algorithm that can query
information relating to multiple items of the database, can search a database
in a single query (a query is defined as any question to the database to which
the database has to return a (YES/NO) answer). A classical algorithm will be
limited to the information theoretic bound of at least O(log N) queries (which
it would achieve by using a binary search).",Lov K. Grover,"Quantum computers can search arbitrarily large databases by a single
  query. This paper shows that a quantum mechanical algorithm that can query
information relating to multiple items of the database, can search a database
in a single query (a query is defined as any question to the database to which
the database has to return a (YES/NO) answer). A classical algorithm will be
limited to the information theoretic bound of at least O(log N) queries (which
it would achieve by using a binary search).","Quantum Computers, Quantum Mechanical Algorithm, Database Search"
Fault-Tolerant Partial Replication in Large-Scale Database Systems,"We investigate a decentralised approach to committing transactions in a
replicated database, under partial replication. Previous protocols either
re-execute transactions entirely and/or compute a total order of transactions.
In contrast, ours applies update values, and orders only conflicting
transactions. It results that transactions execute faster, and distributed
databases commit in small committees. Both effects contribute to preserve
scalability as the number of databases and transactions increase. Our algorithm
ensures serializability, and is live and safe in spite of faults.","Pierre Sutra, Marc Shapiro","Fault-Tolerant Partial Replication in Large-Scale Database Systems. We investigate a decentralised approach to committing transactions in a
replicated database, under partial replication. Previous protocols either
re-execute transactions entirely and/or compute a total order of transactions.
In contrast, ours applies update values, and orders only conflicting
transactions. It results that transactions execute faster, and distributed
databases commit in small committees. Both effects contribute to preserve
scalability as the number of databases and transactions increase. Our algorithm
ensures serializability, and is live and safe in spite of faults.","Fault-Tolerant Partial Replication, Large-Scale Database Systems, Distributed Databases, Serialization"
Optimization Approach for Detecting the Critical Data on a Database,"Through purposeful introduction of malicious transactions (tracking
transactions) into randomly select nodes of a (database) graph, soiled and
clean segments are identified. Soiled and clean measures corresponding those
segments are then computed. These measures are used to repose the problem of
critical database elements detection as an optimization problem over the graph.
This method is universally applicable over a large class of graphs (including
directed, weighted, disconnected, cyclic) that occur in several contexts of
databases. A generalization argument is presented which extends the critical
data problem to abstract settings.",Prashanth Alluvada,"Optimization Approach for Detecting the Critical Data on a Database. Through purposeful introduction of malicious transactions (tracking
transactions) into randomly select nodes of a (database) graph, soiled and
clean segments are identified. Soiled and clean measures corresponding those
segments are then computed. These measures are used to repose the problem of
critical database elements detection as an optimization problem over the graph.
This method is universally applicable over a large class of graphs (including
directed, weighted, disconnected, cyclic) that occur in several contexts of
databases. A generalization argument is presented which extends the critical
data problem to abstract settings.","Optimization Approach, Critical Data, Database, Tracking Transactions"
"Techniques for Securing Data Exchange between a Database Server and a
  Client Program","The goal of the presented work is to illustrate a method by which the data
exchange between a standalone computer software and a shared database server
can be protected of unauthorized interceptation of the traffic in Internet
network, a transport network for data managed by those two systems,
interceptation by which an attacker could gain illegetimate access to the
database, threatening this way the data integrity and compromising the
database.",Ovidiu Crista,"Techniques for Securing Data Exchange between a Database Server and a
  Client Program. The goal of the presented work is to illustrate a method by which the data
exchange between a standalone computer software and a shared database server
can be protected of unauthorized interceptation of the traffic in Internet
network, a transport network for data managed by those two systems,
interceptation by which an attacker could gain illegetimate access to the
database, threatening this way the data integrity and compromising the
database.","Data Exchange, Database Server, Client Program, Internet"
Using Grid Files for a Relational Database Management System,"This paper describes our experience with using Grid files as the main storage
organization for a relational database management system. We primarily focus on
the following two aspects. (i) Strategies for implementing grid files
efficiently. (ii) Methods for efficiency evaluating queries posed to a database
organized using grid files.","S. M. Joshi, S. Sanyal, S. Banerjee, S. Srikumar","Using Grid Files for a Relational Database Management System. This paper describes our experience with using Grid files as the main storage
organization for a relational database management system. We primarily focus on
the following two aspects. (i) Strategies for implementing grid files
efficiently. (ii) Methods for efficiency evaluating queries posed to a database
organized using grid files.","Grid Files, Relational Database, Database Management"
"Missing Data Imputation and Corrected Statistics for Large-Scale
  Behavioral Databases","This paper presents a new methodology to solve problems resulting from
missing data in large-scale item performance behavioral databases. Useful
statistics corrected for missing data are described, and a new method of
imputation for missing data is proposed. This methodology is applied to the DLP
database recently published by Keuleers et al. (2010), which allows us to
conclude that this database fulfills the conditions of use of the method
recently proposed by Courrieu et al. (2011) to test item performance models.
Two application programs in Matlab code are provided for the imputation of
missing data in databases, and for the computation of corrected statistics to
test models.","Pierre Courrieu, Arnaud Rey","Missing Data Imputation and Corrected Statistics for Large-Scale
  Behavioral Databases. This paper presents a new methodology to solve problems resulting from
missing data in large-scale item performance behavioral databases. Useful
statistics corrected for missing data are described, and a new method of
imputation for missing data is proposed. This methodology is applied to the DLP
database recently published by Keuleers et al. (2010), which allows us to
conclude that this database fulfills the conditions of use of the method
recently proposed by Courrieu et al. (2011) to test item performance models.
Two application programs in Matlab code are provided for the imputation of
missing data in databases, and for the computation of corrected statistics to
test models.","Missing Data Imputation, Corrected Statistics, Large-Scale Item Performance, Behavioral Databases, Matlab"
Recent Trends and Research Issues in Video Association Mining,"With the ever-growing digital libraries and video databases, it is
increasingly important to understand and mine the knowledge from video database
automatically. Discovering association rules between items in a large video
database plays a considerable role in the video data mining research areas.
Based on the research and development in the past years, application of
association rule mining is growing in different domains such as surveillance,
meetings, broadcast news, sports, archives, movies, medical data, as well as
personal and online media collections. The purpose of this paper is to provide
general framework of mining the association rules from video database. This
article is also represents the research issues in video association mining
followed by the recent trends.","Vijayakumar V, Nedunchezhian R","Recent Trends and Research Issues in Video Association Mining. With the ever-growing digital libraries and video databases, it is
increasingly important to understand and mine the knowledge from video database
automatically. Discovering association rules between items in a large video
database plays a considerable role in the video data mining research areas.
Based on the research and development in the past years, application of
association rule mining is growing in different domains such as surveillance,
meetings, broadcast news, sports, archives, movies, medical data, as well as
personal and online media collections. The purpose of this paper is to provide
general framework of mining the association rules from video database. This
article is also represents the research issues in video association mining
followed by the recent trends.","Video Association Mining, Association Rules, Video Database"
FDB: A Query Engine for Factorised Relational Databases,"Factorised databases are relational databases that use compact factorised
representations at the physical layer to reduce data redundancy and boost query
performance. This paper introduces FDB, an in-memory query engine for
select-project-join queries on factorised databases. Key components of FDB are
novel algorithms for query optimisation and evaluation that exploit the
succinctness brought by data factorisation. Experiments show that for data sets
with many-to-many relationships FDB can outperform relational engines by orders
of magnitude.","Nurzhan Bakibayev, Dan Olteanu, Jakub Závodný","FDB: A Query Engine for Factorised Relational Databases. Factorised databases are relational databases that use compact factorised
representations at the physical layer to reduce data redundancy and boost query
performance. This paper introduces FDB, an in-memory query engine for
select-project-join queries on factorised databases. Key components of FDB are
novel algorithms for query optimisation and evaluation that exploit the
succinctness brought by data factorisation. Experiments show that for data sets
with many-to-many relationships FDB can outperform relational engines by orders
of magnitude.","FDB, Query Engine, Factorised Relational Databases, Data Factorisation"
"Intelligent Database Flexible Querying System by Approximate Query
  Processing","Database flexible querying is an alternative to the classic one for users.
The use of Formal Concepts Analysis (FCA) makes it possible to make approximate
answers that those turned over by a classic DataBase Management System (DBMS).
Some applications do not need exact answers. However, flexible querying can be
expensive in response time. This time is more significant when the flexible
querying require the calculation of aggregate functions (""Sum"", ""Avg"", ""Count"",
""Var"" etc.). In this paper, we propose an approach which tries to solve this
problem by using Approximate Query Processing (AQP).","Oussama Tlili, Minyar Sassi, Habib Ounelli","Intelligent Database Flexible Querying System by Approximate Query
  Processing. Database flexible querying is an alternative to the classic one for users.
The use of Formal Concepts Analysis (FCA) makes it possible to make approximate
answers that those turned over by a classic DataBase Management System (DBMS).
Some applications do not need exact answers. However, flexible querying can be
expensive in response time. This time is more significant when the flexible
querying require the calculation of aggregate functions (""Sum"", ""Avg"", ""Count"",
""Var"" etc.). In this paper, we propose an approach which tries to solve this
problem by using Approximate Query Processing (AQP).","Database Flexible Querying, Approximate Query Processing, Formal Concepts Analysis, FCA"
An Introduction to RNA Databases,"We present an introduction to RNA databases. The history and technology
behind RNA databases is briefly discussed. We examine differing methods of data
collection and curation, and discuss their impact on both the scope and
accuracy of the resulting databases. Finally, we demonstrate these principals
through detailed examination of four leading RNA databases: Noncode, miRBase,
Rfam, and SILVA.","Marc P. Hoeppner, Lars E. Barquist, Paul P. Gardner","An Introduction to RNA Databases. We present an introduction to RNA databases. The history and technology
behind RNA databases is briefly discussed. We examine differing methods of data
collection and curation, and discuss their impact on both the scope and
accuracy of the resulting databases. Finally, we demonstrate these principals
through detailed examination of four leading RNA databases: Noncode, miRBase,
Rfam, and SILVA.","RNA Databases, Noncode, miRBase, Rfam, SILVA"
"Proceedings of the 14th International Symposium on Database Programming
  Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento, Italy","This volume contains the papers presented at the 14th Symposium on Database
Programming Languages (DBPL 2013) held on August 30th, 2013, in Riva del Garda,
co-located with the 39th International Conference on Very Large Databases (VLDB
2013). They cover a wide range of topics including the application of
programming language techniques to further the expressiveness of database
languages, schema management, and the practical use of XPath. To complement
this technical program, DBPL 2013 featured three invited talks by Serge
Abiteboul (Inria), J\'er\^ome Sim\'eon (IBM), and Soren Lassen (Facebook).","Todd J. Green, Alan Schmitt","Proceedings of the 14th International Symposium on Database Programming
  Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento, Italy. This volume contains the papers presented at the 14th Symposium on Database
Programming Languages (DBPL 2013) held on August 30th, 2013, in Riva del Garda,
co-located with the 39th International Conference on Very Large Databases (VLDB
2013). They cover a wide range of topics including the application of
programming language techniques to further the expressiveness of database
languages, schema management, and the practical use of XPath. To complement
this technical program, DBPL 2013 featured three invited talks by Serge
Abiteboul (Inria), J\'er\^ome Sim\'eon (IBM), and Soren Lassen (Facebook).","Database Programming Languages, DBPL 2013, Inria, IBM, Soren Lassen, XPath"
Security Implications of Distributed Database Management System Models,"Security features must be addressed when escalating a distributed database.
The choice between the object oriented and the relational data model, several
factors should be considered. The most important of these factors are single
and multilevel access controls (MAC), protection and integrity maintenance.
While determining which distributed database replica will be more secure for a
particular function, the choice should not be made exclusively on the basis of
available security features. One should also query the effectiveness and
efficiency of the delivery of these characteristics. In this paper, the
security strengths and weaknesses of both database models and the thorough
problems initiate in the distributed environment are conversed.","C. Sunil Kumar, J. Seetha, S. R. Vinotha","Security Implications of Distributed Database Management System Models. Security features must be addressed when escalating a distributed database.
The choice between the object oriented and the relational data model, several
factors should be considered. The most important of these factors are single
and multilevel access controls (MAC), protection and integrity maintenance.
While determining which distributed database replica will be more secure for a
particular function, the choice should not be made exclusively on the basis of
available security features. One should also query the effectiveness and
efficiency of the delivery of these characteristics. In this paper, the
security strengths and weaknesses of both database models and the thorough
problems initiate in the distributed environment are conversed.","Distributed Database, Database Management System, Relational Data Model, Authentication, Multilevel Access Controls, Integrity Maintenance"
Saturation of the morphisms in the database category,"In this paper we present the problem of saturation of a given morphism in the
database category DB, which is the base category for the functiorial semantics
of the database schema mapping systems used in Data Integration theory. This
phenomena appears in the case when we are using the Second-Order
tuple-generating dependencies (SOtgd) with existentially quantified
non-built-in functions, for the database schema mappings. We provide the
algorithm of the saturation for a given morphism, which represents a mapping
between two relational databases, and show that the original morphism in DB can
be equivalently substituted by its more powerful saturated version in any
commutative diagram in DB.",Zoran Majkic,"Saturation of the morphisms in the database category. In this paper we present the problem of saturation of a given morphism in the
database category DB, which is the base category for the functiorial semantics
of the database schema mapping systems used in Data Integration theory. This
phenomena appears in the case when we are using the Second-Order
tuple-generating dependencies (SOtgd) with existentially quantified
non-built-in functions, for the database schema mappings. We provide the
algorithm of the saturation for a given morphism, which represents a mapping
between two relational databases, and show that the original morphism in DB can
be equivalently substituted by its more powerful saturated version in any
commutative diagram in DB.","Saturation, Database Category, Data Integration, SOtgd"
Cover Story Management,"In a multilevel database, cover stories are usually managed using the
ambiguous technique of polyinstantiation. In this paper, we define a new
technique to manage cover stories and propose a formal representation of a
multilevel database containing cover stories. Our model aims to be a generic
model, that is, it can be interpreted for any kind of database (e.g.
relational, object- oriented etc). We then consider the problem of updating a
multilevel database containing cover stories managed with our technique.","Frédéric Cuppens, Alban Gabillon","Cover Story Management. In a multilevel database, cover stories are usually managed using the
ambiguous technique of polyinstantiation. In this paper, we define a new
technique to manage cover stories and propose a formal representation of a
multilevel database containing cover stories. Our model aims to be a generic
model, that is, it can be interpreted for any kind of database (e.g.
relational, object- oriented etc). We then consider the problem of updating a
multilevel database containing cover stories managed with our technique.","Cover Story Management, Multilevel Database, Polyinstantiation, Relational, Object-Oriented"
Improving the Deductive System DES with Persistence by Using SQL DBMS's,"This work presents how persistent predicates have been included in the
in-memory deductive system DES by relying on external SQL database management
systems. We introduce how persistence is supported from a user-point of view
and the possible applications the system opens up, as the deductive expressive
power is projected to relational databases. Also, we describe how it is
possible to intermix computations of the deductive engine and the external
database, explaining its implementation and some optimizations. Finally, a
performance analysis is undertaken, comparing the system with current
relational database systems.",Fernando Sáenz-Pérez,"Improving the Deductive System DES with Persistence by Using SQL DBMS's. This work presents how persistent predicates have been included in the
in-memory deductive system DES by relying on external SQL database management
systems. We introduce how persistence is supported from a user-point of view
and the possible applications the system opens up, as the deductive expressive
power is projected to relational databases. Also, we describe how it is
possible to intermix computations of the deductive engine and the external
database, explaining its implementation and some optimizations. Finally, a
performance analysis is undertaken, comparing the system with current
relational database systems.","Deductive System DES, Persistence, SQL DBMS, Relational Databases"
Design and Implementation of Database Independent Auto Sequence Numbers,"Developers across the world use autonumber or auto sequences field of the
backend databases for developing both the desktop and web based data centric
applications which is easier to use at the development and deployment purpose
but can create a lot of problems under varied situations. This paper examines
how a database independent autonumber could be developed and reused solving all
the problems as well as providing the same degree of easy to use features of
autonumber offered by modern Relational Database Systems.",Kisor Ray,"Design and Implementation of Database Independent Auto Sequence Numbers. Developers across the world use autonumber or auto sequences field of the
backend databases for developing both the desktop and web based data centric
applications which is easier to use at the development and deployment purpose
but can create a lot of problems under varied situations. This paper examines
how a database independent autonumber could be developed and reused solving all
the problems as well as providing the same degree of easy to use features of
autonumber offered by modern Relational Database Systems.","Database Independent Autonumber, Auto Sequence Numbers, Relational Database Systems"
Using MongoDB for Social Networking Website,"Social media is a biggest successful buzzword used in the recent time. Its
success opened various opportunities for the developers. Developing any
application requires storage of large data into databases. Many databases are
available for the developers, Choosing the right one make development easier.
MongoDB is a cross platform document oriented, schema-less database eschewed
the traditional table based relational database structure in favor of JSON like
documents. This article discusses various pros and cons encountered with the
use of the MongoDB so that developers would be helped while choosing it wisely.","Sumitkumar Kanoje, Varsha Powar, Debajyoti Mukhopadhyay","Using MongoDB for Social Networking Website. Social media is a biggest successful buzzword used in the recent time. Its
success opened various opportunities for the developers. Developing any
application requires storage of large data into databases. Many databases are
available for the developers, Choosing the right one make development easier.
MongoDB is a cross platform document oriented, schema-less database eschewed
the traditional table based relational database structure in favor of JSON like
documents. This article discusses various pros and cons encountered with the
use of the MongoDB so that developers would be helped while choosing it wisely.","MongoDB, Social Networking, JSON"
"Application of Information Centric Networking to NoSQL Databases: the
  Spatio-Temporal use case","This paper explores methodologies, advantages and challenges related to the
use of the Information Centric Network technology for developing NoSQL
distributed databases, which are expected to play a central role in the
forthcoming IoT and BigData era. ICN services make possible to simplify the
development of the database software, improve performance, and provide
data-level access control. We use our findings for devising a NoSQL
spatio-temporal database, named OpenGeoBase, and evaluate its performance with
a real data set related to Intelligent Transport System applications.","Andrea Detti, Michele Orru, Riccardo Paolillo, Giulio Rossi, Pierpaolo Loreti, Lorenzo Bracciale, Nicola Blefari Melazzi","Application of Information Centric Networking to NoSQL Databases: the
  Spatio-Temporal use case. This paper explores methodologies, advantages and challenges related to the
use of the Information Centric Network technology for developing NoSQL
distributed databases, which are expected to play a central role in the
forthcoming IoT and BigData era. ICN services make possible to simplify the
development of the database software, improve performance, and provide
data-level access control. We use our findings for devising a NoSQL
spatio-temporal database, named OpenGeoBase, and evaluate its performance with
a real data set related to Intelligent Transport System applications.","Information Centric Networking, NoSQL Databases, OpenGeoBase, Intelligent Transport System, IoT, BigData"
Enforcing Privacy in Cloud Databases,"Outsourcing databases, i.e., resorting to Database-as-a-Service (DBaaS), is
nowadays a popular choice due to the elasticity, availability, scalability and
pay-as-you-go features of cloud computing. However, most data are sensitive to
some extent, and data privacy remains one of the top concerns to DBaaS users,
for obvious legal and competitive reasons.In this paper, we survey the
mechanisms that aim at making databases secure in a cloud environment, and
discuss current pitfalls and related research challenges.","Somayeh Sobati Moghadam, Jérôme Darmont, Gérald Gavin","Enforcing Privacy in Cloud Databases. Outsourcing databases, i.e., resorting to Database-as-a-Service (DBaaS), is
nowadays a popular choice due to the elasticity, availability, scalability and
pay-as-you-go features of cloud computing. However, most data are sensitive to
some extent, and data privacy remains one of the top concerns to DBaaS users,
for obvious legal and competitive reasons.In this paper, we survey the
mechanisms that aim at making databases secure in a cloud environment, and
discuss current pitfalls and related research challenges.","Privacy, Cloud Databases, Database-as-a-Service, Cloud Computing"
"Matrix and Graph Operations for Relationship Inference: An Illustration
  with the Kinship Inference in the China Biographical Database","Biographical databases contain diverse information about individuals. Person
names, birth information, career, friends, family and special achievements are
some possible items in the record for an individual. The relationships between
individuals, such as kinship and friendship, provide invaluable insights about
hidden communities which are not directly recorded in databases. We show that
some simple matrix and graph-based operations are effective for inferring
relationships among individuals, and illustrate the main ideas with the China
Biographical Database (CBDB).","Chao-Lin Liu, Hongsu Wang","Matrix and Graph Operations for Relationship Inference: An Illustration
  with the Kinship Inference in the China Biographical Database. Biographical databases contain diverse information about individuals. Person
names, birth information, career, friends, family and special achievements are
some possible items in the record for an individual. The relationships between
individuals, such as kinship and friendship, provide invaluable insights about
hidden communities which are not directly recorded in databases. We show that
some simple matrix and graph-based operations are effective for inferring
relationships among individuals, and illustrate the main ideas with the China
Biographical Database (CBDB).","Matrix, Graph, Kinship Inference, China Biographical Database, CBDB"
Binary Star Database (BDB): New Developments and Applications,"Binary star DataBase (BDB) is the database of binary/multiple systems of
various observational types. BDB contains data on physical and positional
parameters of 260,000 components of 120,000 stellar systems of multiplicity 2
to more than 20, taken from a large variety of published catalogues and
databases. We describe the new features in organization of the database,
integration of new catalogues and implementation of new possibilities available
to users. The development of the BDB index-catalogue, Identification List of
Binaries (ILB), is discussed. This star catalogue provides cross-referencing
between most popular catalogues of binary stars.","Oleg Malkov, Aleksey Karchevsky, Pavel Kaygorodov, Dana Kovaleva, Nikolay Skvortsov","Binary Star Database (BDB): New Developments and Applications. Binary star DataBase (BDB) is the database of binary/multiple systems of
various observational types. BDB contains data on physical and positional
parameters of 260,000 components of 120,000 stellar systems of multiplicity 2
to more than 20, taken from a large variety of published catalogues and
databases. We describe the new features in organization of the database,
integration of new catalogues and implementation of new possibilities available
to users. The development of the BDB index-catalogue, Identification List of
Binaries (ILB), is discussed. This star catalogue provides cross-referencing
between most popular catalogues of binary stars.","Binary Star Database, BDB, DataBase, Identification List of Binaries, ILB"
RedisGraph GraphBLAS Enabled Graph Database,"RedisGraph is a Redis module developed by Redis Labs to add graph database
functionality to the Redis database. RedisGraph represents connected data as
adjacency matrices. By representing the data as sparse matrices and employing
the power of GraphBLAS (a highly optimized library for sparse matrix
operations), RedisGraph delivers a fast and efficient way to store, manage and
process graphs. Initial benchmarks indicate that RedisGraph is significantly
faster than comparable graph databases.","Pieter Cailliau, Tim Davis, Vijay Gadepally, Jeremy Kepner, Roi Lipman, Jeffrey Lovitz, Keren Ouaknine","RedisGraph GraphBLAS Enabled Graph Database. RedisGraph is a Redis module developed by Redis Labs to add graph database
functionality to the Redis database. RedisGraph represents connected data as
adjacency matrices. By representing the data as sparse matrices and employing
the power of GraphBLAS (a highly optimized library for sparse matrix
operations), RedisGraph delivers a fast and efficient way to store, manage and
process graphs. Initial benchmarks indicate that RedisGraph is significantly
faster than comparable graph databases.","Redis, GraphBLAS, Redis Labs, Graph Database"
Aggregate Queries on Sparse Databases,"We propose an algebraic framework for studying efficient algorithms for query
evaluation, aggregation, enumeration, and maintenance under updates, on sparse
databases. Our framework allows to treat those problems in a unified way, by
considering various semirings, depending on the considered problem. As a
concrete application, we propose a powerful query language extending
first-order logic by aggregation in multiple semirings. We obtain an optimal
algorithm for computing the answers of such queries on sparse databases. More
precisely, given a database from a fixed class with bounded expansion, the
algorithm computes in linear time a data structure which allows to enumerate
the set of answers to the query, with constant delay between two outputs.",Szymon Toruńczyk,"Aggregate Queries on Sparse Databases. We propose an algebraic framework for studying efficient algorithms for query
evaluation, aggregation, enumeration, and maintenance under updates, on sparse
databases. Our framework allows to treat those problems in a unified way, by
considering various semirings, depending on the considered problem. As a
concrete application, we propose a powerful query language extending
first-order logic by aggregation in multiple semirings. We obtain an optimal
algorithm for computing the answers of such queries on sparse databases. More
precisely, given a database from a fixed class with bounded expansion, the
algorithm computes in linear time a data structure which allows to enumerate
the set of answers to the query, with constant delay between two outputs.","Aggregate Queries, Sparse Databases, Algorithms, Algorithm"
"Database Intrusion Detection Systems (DIDs): Insider Threat Detection
  via Behavioural-based Anomaly Detection Systems -- A Brief Survey of Concepts
  and Approaches","One of the data security and privacy concerns is of insider threats, where
legitimate users of the system abuse the access privileges they hold. The
insider threat to data security means that an insider steals or leaks sensitive
personal information. Database Intrusion detection systems, specifically
behavioural-based database intrusion detection systems, have been shown
effective in detecting insider attacks. This paper presents background concepts
on database intrusion detection systems in the context of detecting insider
threats and examines existing approaches in the literature on detecting
malicious accesses by an insider to Database Management Systems (DBMS).","Muhammad Imran Khan, Simon N. Foley, Barry O'Sullivan","Database Intrusion Detection Systems (DIDs): Insider Threat Detection
  via Behavioural-based Anomaly Detection Systems -- A Brief Survey of Concepts
  and Approaches. One of the data security and privacy concerns is of insider threats, where
legitimate users of the system abuse the access privileges they hold. The
insider threat to data security means that an insider steals or leaks sensitive
personal information. Database Intrusion detection systems, specifically
behavioural-based database intrusion detection systems, have been shown
effective in detecting insider attacks. This paper presents background concepts
on database intrusion detection systems in the context of detecting insider
threats and examines existing approaches in the literature on detecting
malicious accesses by an insider to Database Management Systems (DBMS).","Database Intrusion Detection Systems, DIDs, Insider Threat Detection, Behavioural-based Anomaly Detection Systems"
Exploiting Information-centric Networking to Federate Spatial Databases,"This paper explores the methodologies, challenges, and expected advantages
related to the use of the information-centric network (ICN) technology for
federating spatial databases. ICN services allow simplifying the design of
federation procedures, improving their performance, and providing so-called
data-centric security. In this work, we present an architecture that is able to
federate spatial databases and evaluate its performance using a real data set
coming from OpenStreetMap within a heterogeneous federation formed by MongoDB
and CouchBase spatial database systems.","Andrea Detti, Giulio Rossi, Nicola Blefari Melazzi","Exploiting Information-centric Networking to Federate Spatial Databases. This paper explores the methodologies, challenges, and expected advantages
related to the use of the information-centric network (ICN) technology for
federating spatial databases. ICN services allow simplifying the design of
federation procedures, improving their performance, and providing so-called
data-centric security. In this work, we present an architecture that is able to
federate spatial databases and evaluate its performance using a real data set
coming from OpenStreetMap within a heterogeneous federation formed by MongoDB
and CouchBase spatial database systems.","Information-centric Networking, Federate Spatial Databases, OpenStreetMap, MongoDB, CouchBase"
State-of-the-Art on Query & Transaction Processing Acceleration,"The vast amount of processing power and memory bandwidth provided by modern
Graphics Processing Units (GPUs) make them a platform for data-intensive
applications. The database community identified GPUs as effective co-processors
for data processing. In the past years, there were many approaches to make use
of GPUs at different levels of a database system. In this Internal Technical
Report, based on the [1] and some other research papers, we identify possible
research areas at LIP6 for GPU-accelerated database management systems. We
describe some key properties, typical challenges of GPU-aware database
architectures, and identify major open challenges.","Bernd Amann, Youry Khmelevsky, Gaetan Hains","State-of-the-Art on Query & Transaction Processing Acceleration. The vast amount of processing power and memory bandwidth provided by modern
Graphics Processing Units (GPUs) make them a platform for data-intensive
applications. The database community identified GPUs as effective co-processors
for data processing. In the past years, there were many approaches to make use
of GPUs at different levels of a database system. In this Internal Technical
Report, based on the [1] and some other research papers, we identify possible
research areas at LIP6 for GPU-accelerated database management systems. We
describe some key properties, typical challenges of GPU-aware database
architectures, and identify major open challenges.","GPU, Query Processing, Transaction Processing, LIP6, Database Management"
An Empirical Study on the Design and Evolution of NoSQL Database Schemas,"We study how software engineers design and evolve their domain model when
building applications against NoSQL data stores. Specifically, we target Java
projects that use object-NoSQL mappers to interface with schema-free NoSQL data
stores. Given the source code of ten real-world database applications, we
extract the implicit NoSQL database schema. We capture the sizes of the
schemas, and investigate whether the schema is denormalized, as is recommended
practice in data modeling for NoSQL data stores. Further, we analyze the entire
project history, and with it, the evolution history of the NoSQL database
schema. In doing so, we conduct the so far largest empirical study on NoSQL
schema design and evolution.","Stefanie Scherzinger, Sebastian Sidortschuck","An Empirical Study on the Design and Evolution of NoSQL Database Schemas. We study how software engineers design and evolve their domain model when
building applications against NoSQL data stores. Specifically, we target Java
projects that use object-NoSQL mappers to interface with schema-free NoSQL data
stores. Given the source code of ten real-world database applications, we
extract the implicit NoSQL database schema. We capture the sizes of the
schemas, and investigate whether the schema is denormalized, as is recommended
practice in data modeling for NoSQL data stores. Further, we analyze the entire
project history, and with it, the evolution history of the NoSQL database
schema. In doing so, we conduct the so far largest empirical study on NoSQL
schema design and evolution.","NoSQL, Database Schemas, Java"
"Parallel Betweenness Computation in Graph Database for Contingency
  Selection","Parallel betweenness computation algorithms are proposed and implemented in a
graph database for power system contingency selection. Principles of the graph
database and graph computing are investigated for both node and edge
betweenness computation. Experiments on the 118-bus system and a real power
system show that speed-up can be achieved for both node and edge betweenness
computation while the speeding effect on the latter is more remarkable due to
the data retrieving advantages of the graph database on the power network data.","Yongli Zhu, Renchang Dai, Guangyi Liu","Parallel Betweenness Computation in Graph Database for Contingency
  Selection. Parallel betweenness computation algorithms are proposed and implemented in a
graph database for power system contingency selection. Principles of the graph
database and graph computing are investigated for both node and edge
betweenness computation. Experiments on the 118-bus system and a real power
system show that speed-up can be achieved for both node and edge betweenness
computation while the speeding effect on the latter is more remarkable due to
the data retrieving advantages of the graph database on the power network data.","Parallel Betweenness Computation, Graph Database, Contingency Selection, 118-bus system, power"
Crowdsourced Databases and Sui Generis Rights,"In this study we propose a new concept of databases (crowdsourced databases),
adding a new conceptual approach to the debate on legal protection of databases
in Europe. We also summarise the current legal framework and current indexing
and web scraping practices - it would not be prudent to suggest a new theory
without contextualising it in the legal and practical context in which it is
developed.","Gonçalo Simões de Almeida, Gonçalo Faria Abreu","Crowdsourced Databases and Sui Generis Rights. In this study we propose a new concept of databases (crowdsourced databases),
adding a new conceptual approach to the debate on legal protection of databases
in Europe. We also summarise the current legal framework and current indexing
and web scraping practices - it would not be prudent to suggest a new theory
without contextualising it in the legal and practical context in which it is
developed.","Crowdsourced Databases, Sui Generis Rights, Europe"
The World of Graph Databases from An Industry Perspective,"Rapidly growing social networks and other graph data have created a high
demand for graph technologies in the market. A plethora of graph databases,
systems, and solutions have emerged, as a result. On the other hand, graph has
long been a well studied area in the database research community. Despite the
numerous surveys on various graph research topics, there is a lack of survey on
graph technologies from an industry perspective. The purpose of this paper is
to provide the research community with an industrial perspective on the graph
database landscape, so that graph researcher can better understand the industry
trend and the challenges that the industry is facing, and work on solutions to
help address these problems.",Yuanyuan Tian,"The World of Graph Databases from An Industry Perspective. Rapidly growing social networks and other graph data have created a high
demand for graph technologies in the market. A plethora of graph databases,
systems, and solutions have emerged, as a result. On the other hand, graph has
long been a well studied area in the database research community. Despite the
numerous surveys on various graph research topics, there is a lack of survey on
graph technologies from an industry perspective. The purpose of this paper is
to provide the research community with an industrial perspective on the graph
database landscape, so that graph researcher can better understand the industry
trend and the challenges that the industry is facing, and work on solutions to
help address these problems.","Graph Databases, Industry Perspective, Graph Technology"
Detecting Data Type Inconsistencies in a Property Graph Database,"Some property graph databases do not have a fixed schema, which can result in
data type inconsistencies for properties on nodes and relationships, especially
when importing data into a running database. Here we present a tool which can
rapidly produce a detailed report on every property in the graph. When executed
on a large knowledge graph, it allowed us to debug a complex ETL process and
enforce 100% data type consistency.","Joshua R. Porter, Michael N. Young, Aleks Y. M. Ontman","Detecting Data Type Inconsistencies in a Property Graph Database. Some property graph databases do not have a fixed schema, which can result in
data type inconsistencies for properties on nodes and relationships, especially
when importing data into a running database. Here we present a tool which can
rapidly produce a detailed report on every property in the graph. When executed
on a large knowledge graph, it allowed us to debug a complex ETL process and
enforce 100% data type consistency.","Data Type Inconsistencies, Property Graph Database, ETL"
SAP HANA Data Volume Management,"Today information technology is a data-driven environment. The role of data
is to empower business leaders to make decisions based on facts, trends, and
statistical numbers. SAP is no exception. In modern days many companies use
business suites like SAP on HANA S/4 or ERP or SAP Business Warehouse and other
non-SAP applications and run those on HANA databases for faster processing.
While HANA is an extremely powerful in-memory database, growing business data
has an impact on the overall performance and budget of the organization. This
paper presents best practices to reduce the overall data footprint of HANA
databases for three use cases like SAP Business Suite on HANA, SAP Business
Warehouse, and Native HANA database.",Subhadip Kumar,"SAP HANA Data Volume Management. Today information technology is a data-driven environment. The role of data
is to empower business leaders to make decisions based on facts, trends, and
statistical numbers. SAP is no exception. In modern days many companies use
business suites like SAP on HANA S/4 or ERP or SAP Business Warehouse and other
non-SAP applications and run those on HANA databases for faster processing.
While HANA is an extremely powerful in-memory database, growing business data
has an impact on the overall performance and budget of the organization. This
paper presents best practices to reduce the overall data footprint of HANA
databases for three use cases like SAP Business Suite on HANA, SAP Business
Warehouse, and Native HANA database.","SAP HANA, Data Volume Management, HANA S/4, ERP, SAP Business Warehouse"
Attribution-Scores in Data Management and Explainable Machine Learning,"We describe recent research on the use of actual causality in the definition
of responsibility scores as explanations for query answers in databases, and
for outcomes from classification models in machine learning. In the case of
databases, useful connections with database repairs are illustrated and
exploited. Repairs are also used to give a quantitative measure of the
consistency of a database. For classification models, the responsibility score
is properly extended and illustrated. The efficient computation of Shap-score
is also analyzed and discussed. The emphasis is placed on work done by the
author and collaborators.",Leopoldo Bertossi,"Attribution-Scores in Data Management and Explainable Machine Learning. We describe recent research on the use of actual causality in the definition
of responsibility scores as explanations for query answers in databases, and
for outcomes from classification models in machine learning. In the case of
databases, useful connections with database repairs are illustrated and
exploited. Repairs are also used to give a quantitative measure of the
consistency of a database. For classification models, the responsibility score
is properly extended and illustrated. The efficient computation of Shap-score
is also analyzed and discussed. The emphasis is placed on work done by the
author and collaborators.","Attribution-Scores, Data Management, Explainable Machine Learning, Classification Models"
"Adaptive Search Optimization: Dynamic Algorithm Selection and Caching
  for Enhanced Database Performance","Efficient search operations in databases are paramount for timely retrieval
of information various applications. This research introduces a novel approach,
combining dynamicalgorithm1 selection and caching2 strategies, to optimize
search performance. The proposed dynamic search algorithm intelligently
switches between Binary3 and Interpolation 4 Search based on dataset
characteristics, significantly improving efficiency for non-uniformly
distributed data. Additionally, a robust caching mechanism5 stores and
retrieves previous search results, further enhancing computational efficiency6.
Theoretical analysis and extensive experiments demonstrate the effectiveness of
the approach, showcasing its potential to revolutionize database performance7
in scenarios with diverse data distributions. This research contributes
valuable insights and practical solutions to the realm of database
optimization, offering a promising avenue for enhancing search operations in
real-world applications",Hakikat Singh,"Adaptive Search Optimization: Dynamic Algorithm Selection and Caching
  for Enhanced Database Performance. Efficient search operations in databases are paramount for timely retrieval
of information various applications. This research introduces a novel approach,
combining dynamicalgorithm1 selection and caching2 strategies, to optimize
search performance. The proposed dynamic search algorithm intelligently
switches between Binary3 and Interpolation 4 Search based on dataset
characteristics, significantly improving efficiency for non-uniformly
distributed data. Additionally, a robust caching mechanism5 stores and
retrieves previous search results, further enhancing computational efficiency6.
Theoretical analysis and extensive experiments demonstrate the effectiveness of
the approach, showcasing its potential to revolutionize database performance7
in scenarios with diverse data distributions. This research contributes
valuable insights and practical solutions to the realm of database
optimization, offering a promising avenue for enhancing search operations in
real-world applications","Adaptive Search Optimization, Dynamic Algorithm, Caching"
Database Technology Evolution III: Knowledge Graphs and Linked Data,"This paper reviews the changes for database technology represented by the
current development of the draft international standard ISO 39075 (Database
Languages - GQL), which seeks a unified specification for property graphs and
knowledge graphs. This paper examines these current developments as part of our
review of the evolution of database technology, and their relation to the
longer-term goal of supporting the Semantic Web using relational technology.","Malcolm Crowe, Fritz Laux","Database Technology Evolution III: Knowledge Graphs and Linked Data. This paper reviews the changes for database technology represented by the
current development of the draft international standard ISO 39075 (Database
Languages - GQL), which seeks a unified specification for property graphs and
knowledge graphs. This paper examines these current developments as part of our
review of the evolution of database technology, and their relation to the
longer-term goal of supporting the Semantic Web using relational technology.","Database Technology, Knowledge Graphs, Linked Data, GQL, Semantic Web"
